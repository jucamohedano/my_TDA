[
  {
    "pages": [
      {
        "page": 1,
        "text": "arXiv:2403.18293v1 [cs.CV] 27 Mar 2024\n\n                                    Efficient Test-Time Adaptation of Vision-Language Models\n\n                   Adilbek Karmanov1*                 Dayan Guan2*             Shijian Lu1,2†          Abdulmotaleb El Saddik1,3                   Eric Xing1,4\n                    1Mohamed bin Zayed University of Artificial Intelligence                                    2Nanyang Technological University\n                                                     3University of Ottawa                4Carnegie Mellon University\n\n                              Abstract\n\n    Test-time adaptation with pre-trained vision-language\nmodels has attracted increasing attention for tackling dis-\ntribution shifts during the test time. Though prior stud-\nies have achieved very promising performance, they in-\nvolve intensive computation which is severely unaligned\n\nwith test-time adaptation. We design TDA, a training-free\ndynamic adapter that enables effective and efficient test-\ntime adaptation with vision-language models. TDA works\nwith a lightweight key-value cache that maintains a dy-\nnamic queue with few-shot pseudo labels as values and\nthe corresponding test-sample features as keys. Leverag-\ning the key-value cache, TDA allows adapting to test data\ngradually via progressive pseudo label refinement which is\nsuper-efficient without incurring any backpropagation. In\naddition, we introduce negative pseudo labeling that allevi-\nates the adverse impact of pseudo label noises by assigning\npseudo labels to certain negative classes when the model\nis uncertain about its pseudo label predictions. Extensive\nexperiments over two benchmarks demonstrate TDA’s su-\nperior effectiveness and efficiency as compared with the\n\nstate-of-the-art. The code has been released in https:\n//kdiaaa.github.io/tda/.\n\n                    Image                               Selection\n                    Encoder\n Augmented                           CLIP Predictions\nTest Images\n\n                  Cat\n Learnable      Panda                      Text\n  Prompt     +                           Encoder\n                 Dog                                         Prediction\n                 Bird\n                 Class      Backpropagation\n\n                  (a) Test-time Prompt Tuning [9, 35]\n\n                                    Image                    Keys\n                                    Encoder      Update      Values\n Test Image                                             Dynamic Adapter\n\n\n                                 CLIP Prediction\n                   Cat\nHand-crafted      Panda               Text\n   Prompt      +                    Encoder\n                   Dog\n                   Bird                                Adapted Prediction\n                   Class\n               (b) Training-free Dynamic Adapter (Ours)\n\n1. Introduction\n\nRecent advances in vision-language models [19, 31, 43]\nhave opened a new door for integrating human language\ninto various computer vision tasks. Take CLIP [31] as an\nexample. It can enable zero-shot image classification by\nleveraging a shared embedding space that is learnt from\nweb-scale image-text pairs. Within this shared space, im-\nages can be directly recognized by matching their features\nwith the text embeddings of CLIP classes. At the other end,\nCLIP often faces challenges while handling various specific\ndownstream images, especially when the downstream im-\nages have clear domain and distribution shifts as compared\n   *Equal Contribution.\n   †Corresponding Author.\nFigure 1. Comparison of our proposed Training-free Dynamic\nAdapter (TDA) with Test-time Prompt Tuning TPT [35] and its\nenhancement DiffTPT [9]: both TPT and DiffTPT require signif-\nicant computational resources to optimize the learnable prompt\nvia backpropagation; TDA is a dynamic cache that is training-\nfree without any backpropagation, making it efficient for test-time\nadaptation in various real-world scenarios.\n\n\nwith the CLIP training images.\n    Several recent studies [2, 3, 40] introduce a new\nparadigm called test-time adaptation for mitigating the do-\nmain shift. The idea of test-time adaptation is well aligned\nwith real-world scenarios where a model needs to adapt to\nnew environments quickly. Despite its great research and",
        "md": "# Efficient Test-Time Adaptation of Vision-Language Models\n\nAdilbek Karmanov1* Dayan Guan2* Shijian Lu1,2† Abdulmotaleb El Saddik1,3 Eric Xing1,4\n\n1Mohamed bin Zayed University of Artificial Intelligence\n\n2Nanyang Technological University\n\n3University of Ottawa\n\n4Carnegie Mellon University\n\n# Abstract\n\nTest-time adaptation with pre-trained vision-language models has attracted increasing attention for tackling distribution shifts during the test time. Though prior studies have achieved very promising performance, they involve intensive computation which is severely unaligned with test-time adaptation. We design TDA, a training-free dynamic adapter that enables effective and efficient test-time adaptation with vision-language models. TDA works with a lightweight key-value cache that maintains a dynamic queue with few-shot pseudo labels as values and the corresponding test-sample features as keys. Leveraging the key-value cache, TDA allows adapting to test data gradually via progressive pseudo label refinement which is super-efficient without incurring any backpropagation. In addition, we introduce negative pseudo labeling that alleviates the adverse impact of pseudo label noises by assigning pseudo labels to certain negative classes when the model is uncertain about its pseudo label predictions. Extensive experiments over two benchmarks demonstrate TDA’s superior effectiveness and efficiency as compared with the state-of-the-art. The code has been released in https://kdiaaa.github.io/tda/.\n\n# 1. Introduction\n\nRecent advances in vision-language models [19, 31, 43] have opened a new door for integrating human language into various computer vision tasks. Take CLIP [31] as an example. It can enable zero-shot image classification by leveraging a shared embedding space that is learnt from web-scale image-text pairs. Within this shared space, images can be directly recognized by matching their features with the text embeddings of CLIP classes. At the other end, CLIP often faces challenges while handling various specific downstream images, especially when the downstream images have clear domain and distribution shifts as compared with the CLIP training images.\n\nSeveral recent studies [2, 3, 40] introduce a new paradigm called test-time adaptation for mitigating the domain shift. The idea of test-time adaptation is well aligned with real-world scenarios where a model needs to adapt to new environments quickly. Despite its great research and...\n\n# Figure 1\n\nComparison of our proposed Training-free Dynamic Adapter (TDA) with Test-time Prompt Tuning TPT [35] and its enhancement DiffTPT [9]: both TPT and DiffTPT require significant computational resources to optimize the learnable prompt via backpropagation; TDA is a dynamic cache that is training-free without any backpropagation, making it efficient for test-time adaptation in various real-world scenarios.\n\n# (a) Test-time Prompt Tuning\n\nImage Encoder\n\nAugmented Test Images\n\nCLIP Predictions\n\nCat\n\nLearnable Prompt + Text Encoder\n\nDog\n\nBird\n\nPrediction Class\n\nBackpropagation\n\n# (b) Training-free Dynamic Adapter (Ours)\n\nImage Encoder\n\nUpdate\n\nTest Image\n\nDynamic Adapter\n\nCLIP Prediction\n\nCat\n\nHand-crafted Prompt + Text Encoder\n\nDog\n\nBird\n\nAdapted Prediction\n\nClass",
        "images": [
          {
            "name": "img_p0_1.png",
            "height": 493,
            "width": 500,
            "x": 283.2597202581793,
            "y": 215.31323719013614,
            "original_width": 500,
            "original_height": 493
          },
          {
            "name": "img_p0_1.png",
            "height": 493,
            "width": 500,
            "x": 288.27730912733074,
            "y": 179.83520850463776,
            "original_width": 500,
            "original_height": 493
          },
          {
            "name": "img_p0_1.png",
            "height": 493,
            "width": 500,
            "x": 314.3990060235514,
            "y": 220.53013479891897,
            "original_width": 500,
            "original_height": 493
          },
          {
            "name": "img_p0_2.png",
            "height": 199,
            "width": 212,
            "x": 310.3535639378515,
            "y": 216.18156008594002,
            "original_width": 212,
            "original_height": 199
          },
          {
            "name": "img_p0_3.png",
            "height": 493,
            "width": 500,
            "x": 314.00008890296897,
            "y": 364.6860650468526,
            "original_width": 500,
            "original_height": 493
          }
        ],
        "items": [
          {
            "type": "heading",
            "lvl": 1,
            "value": "Efficient Test-Time Adaptation of Vision-Language Models",
            "md": "# Efficient Test-Time Adaptation of Vision-Language Models",
            "bBox": {
              "x": 117,
              "y": 117,
              "w": 360.94,
              "h": 14.35
            }
          },
          {
            "type": "text",
            "value": "Adilbek Karmanov1* Dayan Guan2* Shijian Lu1,2† Abdulmotaleb El Saddik1,3 Eric Xing1,4\n\n1Mohamed bin Zayed University of Artificial Intelligence\n\n2Nanyang Technological University\n\n3University of Ottawa\n\n4Carnegie Mellon University",
            "md": "Adilbek Karmanov1* Dayan Guan2* Shijian Lu1,2† Abdulmotaleb El Saddik1,3 Eric Xing1,4\n\n1Mohamed bin Zayed University of Artificial Intelligence\n\n2Nanyang Technological University\n\n3University of Ottawa\n\n4Carnegie Mellon University",
            "bBox": {
              "x": 61,
              "y": 151,
              "w": 468.46,
              "h": 46.96
            }
          },
          {
            "type": "heading",
            "lvl": 1,
            "value": "Abstract",
            "md": "# Abstract",
            "bBox": {
              "x": 145,
              "y": 227,
              "w": 44.49,
              "h": 11.96
            }
          },
          {
            "type": "text",
            "value": "Test-time adaptation with pre-trained vision-language models has attracted increasing attention for tackling distribution shifts during the test time. Though prior studies have achieved very promising performance, they involve intensive computation which is severely unaligned with test-time adaptation. We design TDA, a training-free dynamic adapter that enables effective and efficient test-time adaptation with vision-language models. TDA works with a lightweight key-value cache that maintains a dynamic queue with few-shot pseudo labels as values and the corresponding test-sample features as keys. Leveraging the key-value cache, TDA allows adapting to test data gradually via progressive pseudo label refinement which is super-efficient without incurring any backpropagation. In addition, we introduce negative pseudo labeling that alleviates the adverse impact of pseudo label noises by assigning pseudo labels to certain negative classes when the model is uncertain about its pseudo label predictions. Extensive experiments over two benchmarks demonstrate TDA’s superior effectiveness and efficiency as compared with the state-of-the-art. The code has been released in https://kdiaaa.github.io/tda/.",
            "md": "Test-time adaptation with pre-trained vision-language models has attracted increasing attention for tackling distribution shifts during the test time. Though prior studies have achieved very promising performance, they involve intensive computation which is severely unaligned with test-time adaptation. We design TDA, a training-free dynamic adapter that enables effective and efficient test-time adaptation with vision-language models. TDA works with a lightweight key-value cache that maintains a dynamic queue with few-shot pseudo labels as values and the corresponding test-sample features as keys. Leveraging the key-value cache, TDA allows adapting to test data gradually via progressive pseudo label refinement which is super-efficient without incurring any backpropagation. In addition, we introduce negative pseudo labeling that alleviates the adverse impact of pseudo label noises by assigning pseudo labels to certain negative classes when the model is uncertain about its pseudo label predictions. Extensive experiments over two benchmarks demonstrate TDA’s superior effectiveness and efficiency as compared with the state-of-the-art. The code has been released in https://kdiaaa.github.io/tda/.",
            "bBox": {
              "x": 50,
              "y": 251,
              "w": 490.31,
              "h": 262.24
            }
          },
          {
            "type": "heading",
            "lvl": 1,
            "value": "1. Introduction",
            "md": "# 1. Introduction",
            "bBox": {
              "x": 50,
              "y": 541,
              "w": 76.84,
              "h": 11.96
            }
          },
          {
            "type": "text",
            "value": "Recent advances in vision-language models [19, 31, 43] have opened a new door for integrating human language into various computer vision tasks. Take CLIP [31] as an example. It can enable zero-shot image classification by leveraging a shared embedding space that is learnt from web-scale image-text pairs. Within this shared space, images can be directly recognized by matching their features with the text embeddings of CLIP classes. At the other end, CLIP often faces challenges while handling various specific downstream images, especially when the downstream images have clear domain and distribution shifts as compared with the CLIP training images.\n\nSeveral recent studies [2, 3, 40] introduce a new paradigm called test-time adaptation for mitigating the domain shift. The idea of test-time adaptation is well aligned with real-world scenarios where a model needs to adapt to new environments quickly. Despite its great research and...",
            "md": "Recent advances in vision-language models [19, 31, 43] have opened a new door for integrating human language into various computer vision tasks. Take CLIP [31] as an example. It can enable zero-shot image classification by leveraging a shared embedding space that is learnt from web-scale image-text pairs. Within this shared space, images can be directly recognized by matching their features with the text embeddings of CLIP classes. At the other end, CLIP often faces challenges while handling various specific downstream images, especially when the downstream images have clear domain and distribution shifts as compared with the CLIP training images.\n\nSeveral recent studies [2, 3, 40] introduce a new paradigm called test-time adaptation for mitigating the domain shift. The idea of test-time adaptation is well aligned with real-world scenarios where a model needs to adapt to new environments quickly. Despite its great research and...",
            "bBox": {
              "x": 50,
              "y": 236,
              "w": 494.35,
              "h": 485.96
            }
          },
          {
            "type": "heading",
            "lvl": 1,
            "value": "Figure 1",
            "md": "# Figure 1",
            "bBox": {
              "x": 0,
              "y": 0,
              "w": 612,
              "h": 792
            }
          },
          {
            "type": "text",
            "value": "Comparison of our proposed Training-free Dynamic Adapter (TDA) with Test-time Prompt Tuning TPT [35] and its enhancement DiffTPT [9]: both TPT and DiffTPT require significant computational resources to optimize the learnable prompt via backpropagation; TDA is a dynamic cache that is training-free without any backpropagation, making it efficient for test-time adaptation in various real-world scenarios.",
            "md": "Comparison of our proposed Training-free Dynamic Adapter (TDA) with Test-time Prompt Tuning TPT [35] and its enhancement DiffTPT [9]: both TPT and DiffTPT require significant computational resources to optimize the learnable prompt via backpropagation; TDA is a dynamic cache that is training-free without any backpropagation, making it efficient for test-time adaptation in various real-world scenarios.",
            "bBox": {
              "x": 308,
              "y": 299,
              "w": 236.6,
              "h": 317.97
            }
          },
          {
            "type": "heading",
            "lvl": 1,
            "value": "(a) Test-time Prompt Tuning",
            "md": "# (a) Test-time Prompt Tuning",
            "bBox": {
              "x": 318,
              "y": 307,
              "w": 26.97,
              "h": 182.14
            }
          },
          {
            "type": "text",
            "value": "Image Encoder\n\nAugmented Test Images\n\nCLIP Predictions\n\nCat\n\nLearnable Prompt + Text Encoder\n\nDog\n\nBird\n\nPrediction Class\n\nBackpropagation",
            "md": "Image Encoder\n\nAugmented Test Images\n\nCLIP Predictions\n\nCat\n\nLearnable Prompt + Text Encoder\n\nDog\n\nBird\n\nPrediction Class\n\nBackpropagation",
            "bBox": {
              "x": 313,
              "y": 236,
              "w": 221.61,
              "h": 277.24
            }
          },
          {
            "type": "heading",
            "lvl": 1,
            "value": "(b) Training-free Dynamic Adapter (Ours)",
            "md": "# (b) Training-free Dynamic Adapter (Ours)",
            "bBox": {
              "x": 359,
              "y": 411,
              "w": 181.31,
              "h": 117.97
            }
          },
          {
            "type": "text",
            "value": "Image Encoder\n\nUpdate\n\nTest Image\n\nDynamic Adapter\n\nCLIP Prediction\n\nCat\n\nHand-crafted Prompt + Text Encoder\n\nDog\n\nBird\n\nAdapted Prediction\n\nClass",
            "md": "Image Encoder\n\nUpdate\n\nTest Image\n\nDynamic Adapter\n\nCLIP Prediction\n\nCat\n\nHand-crafted Prompt + Text Encoder\n\nDog\n\nBird\n\nAdapted Prediction\n\nClass",
            "bBox": {
              "x": 311,
              "y": 236,
              "w": 232.07,
              "h": 277.24
            }
          }
        ],
        "status": "OK",
        "links": [
          {
            "url": "https://kdiaaa.github.io/tda/",
            "text": "https:"
          },
          {
            "url": "https://kdiaaa.github.io/tda/",
            "text": "//kdiaaa.github.io/tda/ ."
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": "3,"
          },
          {
            "text": "40]"
          }
        ],
        "width": 612,
        "height": 792,
        "triggeredAutoMode": false,
        "structuredData": null,
        "noStructuredContent": false,
        "noTextContent": false
      },
      {
        "page": 2,
        "text": "application values, test-time adaptation of vision-language\nmodels has been largely neglected.                Recently, Test-time\nPrompt Tuning, as introduced in TPT [35] and its enhance-\nment DiffTPT [9], attempts to adapt vision-language mod-\nels by learning domain-specific prompts from test data. As\nillustrated in Fig. 1 (a), both TPT and DiffTPT train a learn-\nable prompt for each test sample by feeding its augmented\nviews into the CLIP model for generating predictions and\nminimizing the marginal entropy of confident predictions.\nDespite its decent performance, the prompt optimization in\nboth TPT [51] and DiffTPT [9] is computationally intensive\nwhich hinders its applications in various real-world scenar-\nios.\n    We design a training-free dynamic adapter (TDA) that\nallows efficient and effective test-time adaptation of vision-\nlanguage models without requiring any backpropagation in\ntest time. As Fig. 1 (b) shows, TDA constructs a lightweight\nDynamic Adapter that keeps a dynamic queue with the\npseudo labels of a stream of test samples as values and\nthe corresponding CLIP-extracted features as keys. TDA\nhas two desirable features that make its test-time adapta-\ntion highly applicable in real-world scenarios. First, TDA is\nhighly effective, as it improves the quality of pseudo labels\nvia progressive incorporation of test predictions of lower\nentropy [11, 40]. Second, TDA is very efficient as the key-\nvalue cache is non-parametric and requires no backpropaga-\ntion during testing. Beyond that, TDA cache is lightweight\ndue to the few-shot setup and it can be computed with sim-\nple matrix multiplications [12, 21, 29, 48].\n\n    Note that the performance of TDA depends heavily on\nthe pseudo labels of unlabelled test samples which are often\nnoisy with prediction errors. Inspired by the idea of nega-\ntive learning [22, 23, 33], we introduce negative pseudo la-\nbeling to reduce the impact of noisy estimated labels. Tra-\nditional pseudo labeling methods identify the presence of\nparticular classes in unlabeled data, which may result in\nerroneous pseudo labels being assigned when comparable\nhigh probabilities are observed. In contrast, our designed\nnegative pseudo labeling determines the absence of cer-\ntain classes and can provide more accurate pseudo labels\nas the probabilities of these complementary classes are very\nlow. Concretely, we construct an additional TDA cache that\nstores negative pseudo labels to complement the positive\nTDA cache. By combining positive and negative caches,\nTDA is more tolerant to noisy pseudo labels and can bet-\nter generalize to testing data. Extensive experiments over\ntwo widely adopted test-time adaptation benchmarks show\nthat TDA outperforms the state-of-the-art by large margins\nwhile significantly reducing the testing time from over 12\nhours to 16 minutes on the ImageNet dataset.\n    In summary, the contributions of this work are threefold.\nFirst, we design a training-free dynamic adapter (TDA) that\ncan achieve test-time adaptation of vision-language models\nefficiently and effectively. To the best of our knowledge,\nthis is the first work that investigates the efficiency issue of\ntest-time adaptation of vision-language models. Second, we\nintroduce negative pseudo labeling to alleviate the adverse\nimpact of pseudo label noises which makes TDA more ro-\nbust to pseudo label noises and generalizable to testing data.\nThird, we evaluate TDA extensively over two benchmarks,\nand experiments show that TDA achieves superior accuracy\nand efficiency compared with the state-of-the-art.\n\n2. Related Work\n\nVision-language models [6, 17, 19, 31, 43] have demon-\nstrated significant potential in learning semantic representa-\ntions effectively by undergoing extensive training on image-\ntext data. CLIP [31] stands out among these models for\nits ability to establish links between visual and textual rep-\nresentations, which enables it to achieve impressive zero-\nshot results on various downstream tasks. To enhance the\ntransfer learning capability of the CLIP model in the down-\nstream classification tasks, researchers have proposed inte-\ngrating language prompt learners such as CoOp [51] and\nCoCoOp [50], as well as vision adapters such as CLIP-\nAdapter [10], Tip-Adapter [48], CaFo [49], TaskRes [45]\nand GraphAdapter [25].              Although these methods have\nshown considerable performance improvements, they typ-\nically require a large amount of training data in downstream\ntasks, making them less practical for real-world scenarios.\nOn other hand, this work focuses on a new paradigm named\ntest-time adaptation without accessing the original training\ndata.\n\nTest-time adaptation refers to the process of adapting\nmodels to testing data that may have distributional differ-\nences from the training data. It is particularly beneficial\nfor real-world applications that require models to be de-\nployed in diverse environments, such as autonomous driv-\ning in various weather conditions, medical diagnosis in dif-\nferent hospitals, and etc. Several recent works utilize each\nbatch of testing samples to update partial weights [18, 37,\n38, 44], normalization statistics [34], or a combination of\nboth [40, 46]. To avoid updating models with multiple test-\ning samples, MEMO [47] proposes enforcing the invariant\npredictions from different augmentations of each sample in\nthe testing data stream. TPT [35] tackles the same chal-\nlenge with vision-language models by fine-tuning a learn-\nable prompt with each testing sample. DiffTPT [9] inno-\nvates test-time prompt tuning by leveraging pre-trained dif-\nfusion models to augment the diversity of test data samples\nused in TPT. Although TPT [35] and DiffTPT [9] are effec-\ntive in addressing test-time adaptation of vision-language\nmodels, prompt learning is computationally expensive and\ntime-consuming. This paper aims to mitigate the compu-\ntational efficiency challenges of TPT and DiffTPT through",
        "md": "application values, test-time adaptation of vision-language models has been largely neglected. Recently, Test-time Prompt Tuning, as introduced in TPT [35] and its enhancement DiffTPT [9], attempts to adapt vision-language models by learning domain-specific prompts from test data. As illustrated in Fig. 1 (a), both TPT and DiffTPT train a learnable prompt for each test sample by feeding its augmented views into the CLIP model for generating predictions and minimizing the marginal entropy of confident predictions. Despite its decent performance, the prompt optimization in both TPT [51] and DiffTPT [9] is computationally intensive which hinders its applications in various real-world scenarios.\n\nWe design a training-free dynamic adapter (TDA) that allows efficient and effective test-time adaptation of vision-language models without requiring any backpropagation in test time. As Fig. 1 (b) shows, TDA constructs a lightweight Dynamic Adapter that keeps a dynamic queue with the pseudo labels of a stream of test samples as values and the corresponding CLIP-extracted features as keys. TDA has two desirable features that make its test-time adaptation highly applicable in real-world scenarios. First, TDA is highly effective, as it improves the quality of pseudo labels via progressive incorporation of test predictions of lower entropy [11, 40]. Second, TDA is very efficient as the key-value cache is non-parametric and requires no backpropagation during testing. Beyond that, TDA cache is lightweight due to the few-shot setup and it can be computed with simple matrix multiplications [12, 21, 29, 48].\n\nNote that the performance of TDA depends heavily on the pseudo labels of unlabelled test samples which are often noisy with prediction errors. Inspired by the idea of negative learning [22, 23, 33], we introduce negative pseudo labeling to reduce the impact of noisy estimated labels. Traditional pseudo labeling methods identify the presence of particular classes in unlabeled data, which may result in erroneous pseudo labels being assigned when comparable high probabilities are observed. In contrast, our designed negative pseudo labeling determines the absence of certain classes and can provide more accurate pseudo labels as the probabilities of these complementary classes are very low. Concretely, we construct an additional TDA cache that stores negative pseudo labels to complement the positive TDA cache. By combining positive and negative caches, TDA is more tolerant to noisy pseudo labels and can better generalize to testing data. Extensive experiments over two widely adopted test-time adaptation benchmarks show that TDA outperforms the state-of-the-art by large margins while significantly reducing the testing time from over 12 hours to 16 minutes on the ImageNet dataset.\n\nIn summary, the contributions of this work are threefold. First, we design a training-free dynamic adapter (TDA) that can achieve test-time adaptation of vision-language models efficiently and effectively. To the best of our knowledge, this is the first work that investigates the efficiency issue of test-time adaptation of vision-language models. Second, we introduce negative pseudo labeling to alleviate the adverse impact of pseudo label noises which makes TDA more robust to pseudo label noises and generalizable to testing data. Third, we evaluate TDA extensively over two benchmarks, and experiments show that TDA achieves superior accuracy and efficiency compared with the state-of-the-art.\n\n# 2. Related Work\n\nVision-language models [6, 17, 19, 31, 43] have demonstrated significant potential in learning semantic representations effectively by undergoing extensive training on image-text data. CLIP [31] stands out among these models for its ability to establish links between visual and textual representations, which enables it to achieve impressive zero-shot results on various downstream tasks. To enhance the transfer learning capability of the CLIP model in the downstream classification tasks, researchers have proposed integrating language prompt learners such as CoOp [51] and CoCoOp [50], as well as vision adapters such as CLIP-Adapter [10], Tip-Adapter [48], CaFo [49], TaskRes [45] and GraphAdapter [25]. Although these methods have shown considerable performance improvements, they typically require a large amount of training data in downstream tasks, making them less practical for real-world scenarios. On other hand, this work focuses on a new paradigm named test-time adaptation without accessing the original training data.\n\nTest-time adaptation refers to the process of adapting models to testing data that may have distributional differences from the training data. It is particularly beneficial for real-world applications that require models to be deployed in diverse environments, such as autonomous driving in various weather conditions, medical diagnosis in different hospitals, and etc. Several recent works utilize each batch of testing samples to update partial weights [18, 37, 38, 44], normalization statistics [34], or a combination of both [40, 46]. To avoid updating models with multiple testing samples, MEMO [47] proposes enforcing the invariant predictions from different augmentations of each sample in the testing data stream. TPT [35] tackles the same challenge with vision-language models by fine-tuning a learnable prompt with each testing sample. DiffTPT [9] innovates test-time prompt tuning by leveraging pre-trained diffusion models to augment the diversity of test data samples used in TPT. Although TPT [35] and DiffTPT [9] are effective in addressing test-time adaptation of vision-language models, prompt learning is computationally expensive and time-consuming. This paper aims to mitigate the computational efficiency challenges of TPT and DiffTPT through",
        "images": [],
        "items": [
          {
            "type": "text",
            "value": "application values, test-time adaptation of vision-language models has been largely neglected. Recently, Test-time Prompt Tuning, as introduced in TPT [35] and its enhancement DiffTPT [9], attempts to adapt vision-language models by learning domain-specific prompts from test data. As illustrated in Fig. 1 (a), both TPT and DiffTPT train a learnable prompt for each test sample by feeding its augmented views into the CLIP model for generating predictions and minimizing the marginal entropy of confident predictions. Despite its decent performance, the prompt optimization in both TPT [51] and DiffTPT [9] is computationally intensive which hinders its applications in various real-world scenarios.\n\nWe design a training-free dynamic adapter (TDA) that allows efficient and effective test-time adaptation of vision-language models without requiring any backpropagation in test time. As Fig. 1 (b) shows, TDA constructs a lightweight Dynamic Adapter that keeps a dynamic queue with the pseudo labels of a stream of test samples as values and the corresponding CLIP-extracted features as keys. TDA has two desirable features that make its test-time adaptation highly applicable in real-world scenarios. First, TDA is highly effective, as it improves the quality of pseudo labels via progressive incorporation of test predictions of lower entropy [11, 40]. Second, TDA is very efficient as the key-value cache is non-parametric and requires no backpropagation during testing. Beyond that, TDA cache is lightweight due to the few-shot setup and it can be computed with simple matrix multiplications [12, 21, 29, 48].\n\nNote that the performance of TDA depends heavily on the pseudo labels of unlabelled test samples which are often noisy with prediction errors. Inspired by the idea of negative learning [22, 23, 33], we introduce negative pseudo labeling to reduce the impact of noisy estimated labels. Traditional pseudo labeling methods identify the presence of particular classes in unlabeled data, which may result in erroneous pseudo labels being assigned when comparable high probabilities are observed. In contrast, our designed negative pseudo labeling determines the absence of certain classes and can provide more accurate pseudo labels as the probabilities of these complementary classes are very low. Concretely, we construct an additional TDA cache that stores negative pseudo labels to complement the positive TDA cache. By combining positive and negative caches, TDA is more tolerant to noisy pseudo labels and can better generalize to testing data. Extensive experiments over two widely adopted test-time adaptation benchmarks show that TDA outperforms the state-of-the-art by large margins while significantly reducing the testing time from over 12 hours to 16 minutes on the ImageNet dataset.\n\nIn summary, the contributions of this work are threefold. First, we design a training-free dynamic adapter (TDA) that can achieve test-time adaptation of vision-language models efficiently and effectively. To the best of our knowledge, this is the first work that investigates the efficiency issue of test-time adaptation of vision-language models. Second, we introduce negative pseudo labeling to alleviate the adverse impact of pseudo label noises which makes TDA more robust to pseudo label noises and generalizable to testing data. Third, we evaluate TDA extensively over two benchmarks, and experiments show that TDA achieves superior accuracy and efficiency compared with the state-of-the-art.",
            "md": "application values, test-time adaptation of vision-language models has been largely neglected. Recently, Test-time Prompt Tuning, as introduced in TPT [35] and its enhancement DiffTPT [9], attempts to adapt vision-language models by learning domain-specific prompts from test data. As illustrated in Fig. 1 (a), both TPT and DiffTPT train a learnable prompt for each test sample by feeding its augmented views into the CLIP model for generating predictions and minimizing the marginal entropy of confident predictions. Despite its decent performance, the prompt optimization in both TPT [51] and DiffTPT [9] is computationally intensive which hinders its applications in various real-world scenarios.\n\nWe design a training-free dynamic adapter (TDA) that allows efficient and effective test-time adaptation of vision-language models without requiring any backpropagation in test time. As Fig. 1 (b) shows, TDA constructs a lightweight Dynamic Adapter that keeps a dynamic queue with the pseudo labels of a stream of test samples as values and the corresponding CLIP-extracted features as keys. TDA has two desirable features that make its test-time adaptation highly applicable in real-world scenarios. First, TDA is highly effective, as it improves the quality of pseudo labels via progressive incorporation of test predictions of lower entropy [11, 40]. Second, TDA is very efficient as the key-value cache is non-parametric and requires no backpropagation during testing. Beyond that, TDA cache is lightweight due to the few-shot setup and it can be computed with simple matrix multiplications [12, 21, 29, 48].\n\nNote that the performance of TDA depends heavily on the pseudo labels of unlabelled test samples which are often noisy with prediction errors. Inspired by the idea of negative learning [22, 23, 33], we introduce negative pseudo labeling to reduce the impact of noisy estimated labels. Traditional pseudo labeling methods identify the presence of particular classes in unlabeled data, which may result in erroneous pseudo labels being assigned when comparable high probabilities are observed. In contrast, our designed negative pseudo labeling determines the absence of certain classes and can provide more accurate pseudo labels as the probabilities of these complementary classes are very low. Concretely, we construct an additional TDA cache that stores negative pseudo labels to complement the positive TDA cache. By combining positive and negative caches, TDA is more tolerant to noisy pseudo labels and can better generalize to testing data. Extensive experiments over two widely adopted test-time adaptation benchmarks show that TDA outperforms the state-of-the-art by large margins while significantly reducing the testing time from over 12 hours to 16 minutes on the ImageNet dataset.\n\nIn summary, the contributions of this work are threefold. First, we design a training-free dynamic adapter (TDA) that can achieve test-time adaptation of vision-language models efficiently and effectively. To the best of our knowledge, this is the first work that investigates the efficiency issue of test-time adaptation of vision-language models. Second, we introduce negative pseudo labeling to alleviate the adverse impact of pseudo label noises which makes TDA more robust to pseudo label noises and generalizable to testing data. Third, we evaluate TDA extensively over two benchmarks, and experiments show that TDA achieves superior accuracy and efficiency compared with the state-of-the-art.",
            "bBox": {
              "x": 50,
              "y": 82,
              "w": 494.47,
              "h": 639.96
            }
          },
          {
            "type": "heading",
            "lvl": 1,
            "value": "2. Related Work",
            "md": "# 2. Related Work",
            "bBox": {
              "x": 308,
              "y": 203,
              "w": 83.11,
              "h": 11.96
            }
          },
          {
            "type": "text",
            "value": "Vision-language models [6, 17, 19, 31, 43] have demonstrated significant potential in learning semantic representations effectively by undergoing extensive training on image-text data. CLIP [31] stands out among these models for its ability to establish links between visual and textual representations, which enables it to achieve impressive zero-shot results on various downstream tasks. To enhance the transfer learning capability of the CLIP model in the downstream classification tasks, researchers have proposed integrating language prompt learners such as CoOp [51] and CoCoOp [50], as well as vision adapters such as CLIP-Adapter [10], Tip-Adapter [48], CaFo [49], TaskRes [45] and GraphAdapter [25]. Although these methods have shown considerable performance improvements, they typically require a large amount of training data in downstream tasks, making them less practical for real-world scenarios. On other hand, this work focuses on a new paradigm named test-time adaptation without accessing the original training data.\n\nTest-time adaptation refers to the process of adapting models to testing data that may have distributional differences from the training data. It is particularly beneficial for real-world applications that require models to be deployed in diverse environments, such as autonomous driving in various weather conditions, medical diagnosis in different hospitals, and etc. Several recent works utilize each batch of testing samples to update partial weights [18, 37, 38, 44], normalization statistics [34], or a combination of both [40, 46]. To avoid updating models with multiple testing samples, MEMO [47] proposes enforcing the invariant predictions from different augmentations of each sample in the testing data stream. TPT [35] tackles the same challenge with vision-language models by fine-tuning a learnable prompt with each testing sample. DiffTPT [9] innovates test-time prompt tuning by leveraging pre-trained diffusion models to augment the diversity of test data samples used in TPT. Although TPT [35] and DiffTPT [9] are effective in addressing test-time adaptation of vision-language models, prompt learning is computationally expensive and time-consuming. This paper aims to mitigate the computational efficiency challenges of TPT and DiffTPT through",
            "md": "Vision-language models [6, 17, 19, 31, 43] have demonstrated significant potential in learning semantic representations effectively by undergoing extensive training on image-text data. CLIP [31] stands out among these models for its ability to establish links between visual and textual representations, which enables it to achieve impressive zero-shot results on various downstream tasks. To enhance the transfer learning capability of the CLIP model in the downstream classification tasks, researchers have proposed integrating language prompt learners such as CoOp [51] and CoCoOp [50], as well as vision adapters such as CLIP-Adapter [10], Tip-Adapter [48], CaFo [49], TaskRes [45] and GraphAdapter [25]. Although these methods have shown considerable performance improvements, they typically require a large amount of training data in downstream tasks, making them less practical for real-world scenarios. On other hand, this work focuses on a new paradigm named test-time adaptation without accessing the original training data.\n\nTest-time adaptation refers to the process of adapting models to testing data that may have distributional differences from the training data. It is particularly beneficial for real-world applications that require models to be deployed in diverse environments, such as autonomous driving in various weather conditions, medical diagnosis in different hospitals, and etc. Several recent works utilize each batch of testing samples to update partial weights [18, 37, 38, 44], normalization statistics [34], or a combination of both [40, 46]. To avoid updating models with multiple testing samples, MEMO [47] proposes enforcing the invariant predictions from different augmentations of each sample in the testing data stream. TPT [35] tackles the same challenge with vision-language models by fine-tuning a learnable prompt with each testing sample. DiffTPT [9] innovates test-time prompt tuning by leveraging pre-trained diffusion models to augment the diversity of test data samples used in TPT. Although TPT [35] and DiffTPT [9] are effective in addressing test-time adaptation of vision-language models, prompt learning is computationally expensive and time-consuming. This paper aims to mitigate the computational efficiency challenges of TPT and DiffTPT through",
            "bBox": {
              "x": 50,
              "y": 226,
              "w": 494.62,
              "h": 495.96
            }
          }
        ],
        "status": "OK",
        "links": [
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": "able prompt for each test sample by feeding its augmented"
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": "Dynamic Adapter"
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": "38, 44], normalization statistics [34], or a combination of"
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          }
        ],
        "width": 612,
        "height": 792,
        "triggeredAutoMode": false,
        "structuredData": null,
        "noStructuredContent": false,
        "noTextContent": false
      },
      {
        "page": 3,
        "text": "the introduction of a cache model.\n\nCache model benefits the adaptation techniques by provid-\ning efficient inference and non-parametric processing with-\nout requiring parameter updates. Unbounded Cache [12]\nand PSMM [27] have shown promising results in the text\ngeneration task by storing a large amount of the training\ndataset to capture long-term dependencies, however, this\napproach poses a challenge of memory efficiency during the\ntest-time adaptation. A different technique [20] was pro-\nposed to mitigate this limitation by reducing the size of the\ncache memory through the search find application. Alter-\nnatively, Tip-Adapter [48] solves the cache memory prob-\nlem by only storing few-shot samples per class to create a\ncache model for vision-language models. Inspired by this\nwork, our dynamic adapter use the same architecture for the\ntest-time adaptation setting, where there is no access to the\nsource data, and testing samples can only be accessed one\nby one. To address the lack of access to source data during\ntesting, our adapter collects the most reliable test samples\nand their pseudo labels to form cache model.\n3. Method\n\n3.1. Preliminaries\nCLIP [31] is a vision-language model that performs a\nproxy task of predicting the correct pairings of text and im-\nage. Consider an N -class classification problem where the\nCLIP’s objective in the zero-shot setting is to match images\nwith their most relevant textual descriptions using an image\nencoder Ev and a text encoder Et. To obtain the textual\ndescriptions, N -class names are concatenated with hand-\ncrafted prompts and then mapped into the c-channeled text\nembeddings W c using the text encoder Et.\n\nTPT [35] focuses on test-time adaptation of CLIP. In TPT,\na prompt tuning method is proposed to learn an adaptive\nprompt pc using individual test samples. A set of augmenta-\ntion functions A is used to generate n randomly augmented\nviews ˜test = An(xtest) of a test sample xtest. The ob-x\njective of TPT is to reduce variation in the model’s predic-\ntions across different augmentations f˜test = Ev (˜test) byx\nminimizing the marginal entropy among the outputs of the\naugmented views. Furthermore, TPT also includes confi-\ndence selection to discard noisy augmentations that could\nresult in ambiguous model predictions, as shown in Figure\n1a. This is achieved by filtering out augmented views with\nhigh-entropy predictions as:\n                                  n\n        PTPT( ˜test) = ρni=1f1   X1[H( fi˜pc T) ≤ τ ] fi˜pc T,\n\n\nwhere H is the self-entropy function of the softmax logits\npredictions, fi˜pcT is the class probabilities vector of size\n           N generated from the given i-th augmented view of the\n           test image, and the parameter τ determines that only ρ-\n           percentile of confident samples with entropy values below\n           this threshold can be selected out of n augmented views.\n\n           Tip-Adapter [48] provides a training-free solution that uses\n           a key-value cache model and integrates knowledge from the\n           pre-trained CLIP model with few-shot labeled samples. The\n           cache model is created using a set of k-shot labeled samples\n           xk from N classes and their corresponding ground-truth\n           labels yN . It can be conceptualized as a linear two-layer\n           model, where the first layer contains train image features\n           Ftrain = Ev (xk) and the second layer consists of one-hot\n           vectors Ltrain encoded from the labels yN . Given test im-\n           age features ftest generated from the CLIP’s image encoder\n           Ev , the prediction from the cache model can be calculated\n           as follows:\n                         Pcache(ftest) = A(ftestFT\n                                                          train)Ltrain,        (2)\n           where A(z) = α exp(−β(1 − z)) is an adaptation function\n           within a weighting factor α and a sharpness ratio β. Dur-\n           ing inference, the prediction of Tip-Adapter is computed by\n           combining the pre-trained CLIP model and the cache model\n           as: PTA(ftest) = Pcache(ftest) + ftestWT.c\n           3.2. Training-free Dynamic Adapter\n\n           During testing, pre-trained vision-language models like\n           CLIP may encounter distribution shifts that degrade the\n           classification performance.             To address this issue, ex-\n           isting test-time prompt tuning methods train a learnable\n           prompt by enforcing consistency across different image\n           augmentations during testing. However, it requires a large\n           number of augmentation operations on each test image\n           and computationally-heavy optimization steps to learn the\n           prompt, limiting its applicability in various real-world set-\n           tings.\n               In this paper, our motivation is to design an efficient\n           method for test-time adaptation of the pre-trained vision-\n           language models like CLIP. Inspired by the concept of Tip-\n           Adapter, we propose a training-free dynamic adapter (TDA)\n           to enable efficient and effective test-time adaptation with\n           CLIP. As shown in Figure 2, TDA includes two lightweight\n           key-value caches, where each cache stores a dynamic queue\n           of few-shot test features as keys and the corresponding\n           pseudo labels as values. The first cache is intended for pos-\n           itive learning and it dynamically updates key-value pairs\n           with high-confidence predictions to improve the accuracy.\n           The second cache is designed for negative learning and it\n(1)        aims to address the adverse effects of noisy pseudo labels\n           by introducing negative pseudo labeling to identify class\n           absence rather than presence. By combining the positive\n           cache and negative cache, the proposed TDA can achieve\n           superior performance in terms of both speed and accuracy.",
        "md": "# The Introduction of a Cache Model\n\nCache model benefits the adaptation techniques by providing efficient inference and non-parametric processing without requiring parameter updates. Unbounded Cache [12] and PSMM [27] have shown promising results in the text generation task by storing a large amount of the training dataset to capture long-term dependencies, however, this approach poses a challenge of memory efficiency during the test-time adaptation. A different technique [20] was proposed to mitigate this limitation by reducing the size of the cache memory through the search find application. Alternatively, Tip-Adapter [48] solves the cache memory problem by only storing few-shot samples per class to create a cache model for vision-language models. Inspired by this work, our dynamic adapter uses the same architecture for the test-time adaptation setting, where there is no access to the source data, and testing samples can only be accessed one by one. To address the lack of access to source data during testing, our adapter collects the most reliable test samples and their pseudo labels to form a cache model.\n\n# 3. Method\n\n# 3.1. Preliminaries\n\nCLIP [31] is a vision-language model that performs a proxy task of predicting the correct pairings of text and image. Consider an N-class classification problem where the CLIP’s objective in the zero-shot setting is to match images with their most relevant textual descriptions using an image encoder Ev and a text encoder Et. To obtain the textual descriptions, N-class names are concatenated with handcrafted prompts and then mapped into the c-channeled text embeddings Wc using the text encoder Et.\n\nTPT [35] focuses on test-time adaptation of CLIP. In TPT, a prompt tuning method is proposed to learn an adaptive prompt pc using individual test samples. A set of augmentation functions A is used to generate n randomly augmented views ˜test = An(xtest) of a test sample xtest. The objective of TPT is to reduce variation in the model’s predictions across different augmentations f˜test = Ev (˜test) by minimizing the marginal entropy among the outputs of the augmented views. Furthermore, TPT also includes confidence selection to discard noisy augmentations that could result in ambiguous model predictions, as shown in Figure 1a. This is achieved by filtering out augmented views with high-entropy predictions as:\n\nPTPT( ˜test) = ρni=1f1 X1[H( fi˜pc T) ≤ τ ] fi˜pc T,\n\nwhere H is the self-entropy function of the softmax logits predictions, fi˜pC is the class probabilities vector of size N generated from the given i-th augmented view of the test image, and the parameter τ determines that only ρ-percentile of confident samples with entropy values below this threshold can be selected out of n augmented views.\n\nTip-Adapter [48] provides a training-free solution that uses a key-value cache model and integrates knowledge from the pre-trained CLIP model with few-shot labeled samples. The cache model is created using a set of k-shot labeled samples xk from N classes and their corresponding ground-truth labels yN. It can be conceptualized as a linear two-layer model, where the first layer contains train image features Ftrain = Ev (xk) and the second layer consists of one-hot vectors Ltrain encoded from the labels yN. Given test image features ftest generated from the CLIP’s image encoder Ev, the prediction from the cache model can be calculated as follows:\n\nPcache(ftest) = A(ftestFtrain)Ltrain,\n\nwhere A(z) = α exp(−β(1 − z)) is an adaptation function within a weighting factor α and a sharpness ratio β. During inference, the prediction of Tip-Adapter is computed by combining the pre-trained CLIP model and the cache model as:\n\nPTA(ftest) = Pcache(ftest) + ftestWT.\n\n# 3.2. Training-free Dynamic Adapter\n\nDuring testing, pre-trained vision-language models like CLIP may encounter distribution shifts that degrade the classification performance. To address this issue, existing test-time prompt tuning methods train a learnable prompt by enforcing consistency across different image augmentations during testing. However, it requires a large number of augmentation operations on each test image and computationally-heavy optimization steps to learn the prompt, limiting its applicability in various real-world settings.\n\nIn this paper, our motivation is to design an efficient method for test-time adaptation of the pre-trained vision-language models like CLIP. Inspired by the concept of Tip-Adapter, we propose a training-free dynamic adapter (TDA) to enable efficient and effective test-time adaptation with CLIP. As shown in Figure 2, TDA includes two lightweight key-value caches, where each cache stores a dynamic queue of few-shot test features as keys and the corresponding pseudo labels as values. The first cache is intended for positive learning and it dynamically updates key-value pairs with high-confidence predictions to improve the accuracy. The second cache is designed for negative learning and it aims to address the adverse effects of noisy pseudo labels by introducing negative pseudo labeling to identify class absence rather than presence. By combining the positive cache and negative cache, the proposed TDA can achieve superior performance in terms of both speed and accuracy.",
        "images": [],
        "items": [
          {
            "type": "heading",
            "lvl": 1,
            "value": "The Introduction of a Cache Model",
            "md": "# The Introduction of a Cache Model",
            "bBox": {
              "x": 103,
              "y": 657,
              "w": 59.92,
              "h": 22.96
            }
          },
          {
            "type": "text",
            "value": "Cache model benefits the adaptation techniques by providing efficient inference and non-parametric processing without requiring parameter updates. Unbounded Cache [12] and PSMM [27] have shown promising results in the text generation task by storing a large amount of the training dataset to capture long-term dependencies, however, this approach poses a challenge of memory efficiency during the test-time adaptation. A different technique [20] was proposed to mitigate this limitation by reducing the size of the cache memory through the search find application. Alternatively, Tip-Adapter [48] solves the cache memory problem by only storing few-shot samples per class to create a cache model for vision-language models. Inspired by this work, our dynamic adapter uses the same architecture for the test-time adaptation setting, where there is no access to the source data, and testing samples can only be accessed one by one. To address the lack of access to source data during testing, our adapter collects the most reliable test samples and their pseudo labels to form a cache model.",
            "md": "Cache model benefits the adaptation techniques by providing efficient inference and non-parametric processing without requiring parameter updates. Unbounded Cache [12] and PSMM [27] have shown promising results in the text generation task by storing a large amount of the training dataset to capture long-term dependencies, however, this approach poses a challenge of memory efficiency during the test-time adaptation. A different technique [20] was proposed to mitigate this limitation by reducing the size of the cache memory through the search find application. Alternatively, Tip-Adapter [48] solves the cache memory problem by only storing few-shot samples per class to create a cache model for vision-language models. Inspired by this work, our dynamic adapter uses the same architecture for the test-time adaptation setting, where there is no access to the source data, and testing samples can only be accessed one by one. To address the lack of access to source data during testing, our adapter collects the most reliable test samples and their pseudo labels to form a cache model.",
            "bBox": {
              "x": 50,
              "y": 128,
              "w": 429.87,
              "h": 551.96
            }
          },
          {
            "type": "heading",
            "lvl": 1,
            "value": "3. Method",
            "md": "# 3. Method",
            "bBox": {
              "x": 50,
              "y": 343,
              "w": 51.8,
              "h": 11.96
            }
          },
          {
            "type": "heading",
            "lvl": 1,
            "value": "3.1. Preliminaries",
            "md": "# 3.1. Preliminaries",
            "bBox": {
              "x": 50,
              "y": 362,
              "w": 112.92,
              "h": 310.96
            }
          },
          {
            "type": "text",
            "value": "CLIP [31] is a vision-language model that performs a proxy task of predicting the correct pairings of text and image. Consider an N-class classification problem where the CLIP’s objective in the zero-shot setting is to match images with their most relevant textual descriptions using an image encoder Ev and a text encoder Et. To obtain the textual descriptions, N-class names are concatenated with handcrafted prompts and then mapped into the c-channeled text embeddings Wc using the text encoder Et.\n\nTPT [35] focuses on test-time adaptation of CLIP. In TPT, a prompt tuning method is proposed to learn an adaptive prompt pc using individual test samples. A set of augmentation functions A is used to generate n randomly augmented views ˜test = An(xtest) of a test sample xtest. The objective of TPT is to reduce variation in the model’s predictions across different augmentations f˜test = Ev (˜test) by minimizing the marginal entropy among the outputs of the augmented views. Furthermore, TPT also includes confidence selection to discard noisy augmentations that could result in ambiguous model predictions, as shown in Figure 1a. This is achieved by filtering out augmented views with high-entropy predictions as:\n\nPTPT( ˜test) = ρni=1f1 X1[H( fi˜pc T) ≤ τ ] fi˜pc T,\n\nwhere H is the self-entropy function of the softmax logits predictions, fi˜pC is the class probabilities vector of size N generated from the given i-th augmented view of the test image, and the parameter τ determines that only ρ-percentile of confident samples with entropy values below this threshold can be selected out of n augmented views.\n\nTip-Adapter [48] provides a training-free solution that uses a key-value cache model and integrates knowledge from the pre-trained CLIP model with few-shot labeled samples. The cache model is created using a set of k-shot labeled samples xk from N classes and their corresponding ground-truth labels yN. It can be conceptualized as a linear two-layer model, where the first layer contains train image features Ftrain = Ev (xk) and the second layer consists of one-hot vectors Ltrain encoded from the labels yN. Given test image features ftest generated from the CLIP’s image encoder Ev, the prediction from the cache model can be calculated as follows:\n\nPcache(ftest) = A(ftestFtrain)Ltrain,\n\nwhere A(z) = α exp(−β(1 − z)) is an adaptation function within a weighting factor α and a sharpness ratio β. During inference, the prediction of Tip-Adapter is computed by combining the pre-trained CLIP model and the cache model as:\n\nPTA(ftest) = Pcache(ftest) + ftestWT.",
            "md": "CLIP [31] is a vision-language model that performs a proxy task of predicting the correct pairings of text and image. Consider an N-class classification problem where the CLIP’s objective in the zero-shot setting is to match images with their most relevant textual descriptions using an image encoder Ev and a text encoder Et. To obtain the textual descriptions, N-class names are concatenated with handcrafted prompts and then mapped into the c-channeled text embeddings Wc using the text encoder Et.\n\nTPT [35] focuses on test-time adaptation of CLIP. In TPT, a prompt tuning method is proposed to learn an adaptive prompt pc using individual test samples. A set of augmentation functions A is used to generate n randomly augmented views ˜test = An(xtest) of a test sample xtest. The objective of TPT is to reduce variation in the model’s predictions across different augmentations f˜test = Ev (˜test) by minimizing the marginal entropy among the outputs of the augmented views. Furthermore, TPT also includes confidence selection to discard noisy augmentations that could result in ambiguous model predictions, as shown in Figure 1a. This is achieved by filtering out augmented views with high-entropy predictions as:\n\nPTPT( ˜test) = ρni=1f1 X1[H( fi˜pc T) ≤ τ ] fi˜pc T,\n\nwhere H is the self-entropy function of the softmax logits predictions, fi˜pC is the class probabilities vector of size N generated from the given i-th augmented view of the test image, and the parameter τ determines that only ρ-percentile of confident samples with entropy values below this threshold can be selected out of n augmented views.\n\nTip-Adapter [48] provides a training-free solution that uses a key-value cache model and integrates knowledge from the pre-trained CLIP model with few-shot labeled samples. The cache model is created using a set of k-shot labeled samples xk from N classes and their corresponding ground-truth labels yN. It can be conceptualized as a linear two-layer model, where the first layer contains train image features Ftrain = Ev (xk) and the second layer consists of one-hot vectors Ltrain encoded from the labels yN. Given test image features ftest generated from the CLIP’s image encoder Ev, the prediction from the cache model can be calculated as follows:\n\nPcache(ftest) = A(ftestFtrain)Ltrain,\n\nwhere A(z) = α exp(−β(1 − z)) is an adaptation function within a weighting factor α and a sharpness ratio β. During inference, the prediction of Tip-Adapter is computed by combining the pre-trained CLIP model and the cache model as:\n\nPTA(ftest) = Pcache(ftest) + ftestWT.",
            "bBox": {
              "x": 50,
              "y": 82,
              "w": 494.97,
              "h": 638.97
            }
          },
          {
            "type": "heading",
            "lvl": 1,
            "value": "3.2. Training-free Dynamic Adapter",
            "md": "# 3.2. Training-free Dynamic Adapter",
            "bBox": {
              "x": 103,
              "y": 365,
              "w": 376.87,
              "h": 314.96
            }
          },
          {
            "type": "text",
            "value": "During testing, pre-trained vision-language models like CLIP may encounter distribution shifts that degrade the classification performance. To address this issue, existing test-time prompt tuning methods train a learnable prompt by enforcing consistency across different image augmentations during testing. However, it requires a large number of augmentation operations on each test image and computationally-heavy optimization steps to learn the prompt, limiting its applicability in various real-world settings.\n\nIn this paper, our motivation is to design an efficient method for test-time adaptation of the pre-trained vision-language models like CLIP. Inspired by the concept of Tip-Adapter, we propose a training-free dynamic adapter (TDA) to enable efficient and effective test-time adaptation with CLIP. As shown in Figure 2, TDA includes two lightweight key-value caches, where each cache stores a dynamic queue of few-shot test features as keys and the corresponding pseudo labels as values. The first cache is intended for positive learning and it dynamically updates key-value pairs with high-confidence predictions to improve the accuracy. The second cache is designed for negative learning and it aims to address the adverse effects of noisy pseudo labels by introducing negative pseudo labeling to identify class absence rather than presence. By combining the positive cache and negative cache, the proposed TDA can achieve superior performance in terms of both speed and accuracy.",
            "md": "During testing, pre-trained vision-language models like CLIP may encounter distribution shifts that degrade the classification performance. To address this issue, existing test-time prompt tuning methods train a learnable prompt by enforcing consistency across different image augmentations during testing. However, it requires a large number of augmentation operations on each test image and computationally-heavy optimization steps to learn the prompt, limiting its applicability in various real-world settings.\n\nIn this paper, our motivation is to design an efficient method for test-time adaptation of the pre-trained vision-language models like CLIP. Inspired by the concept of Tip-Adapter, we propose a training-free dynamic adapter (TDA) to enable efficient and effective test-time adaptation with CLIP. As shown in Figure 2, TDA includes two lightweight key-value caches, where each cache stores a dynamic queue of few-shot test features as keys and the corresponding pseudo labels as values. The first cache is intended for positive learning and it dynamically updates key-value pairs with high-confidence predictions to improve the accuracy. The second cache is designed for negative learning and it aims to address the adverse effects of noisy pseudo labels by introducing negative pseudo labeling to identify class absence rather than presence. By combining the positive cache and negative cache, the proposed TDA can achieve superior performance in terms of both speed and accuracy.",
            "bBox": {
              "x": 76,
              "y": 365,
              "w": 468.84,
              "h": 356.96
            }
          }
        ],
        "status": "OK",
        "links": [
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": "1a. This is achieved by filtering out augmented views with"
          },
          {
            "text": ""
          },
          {
            "text": "key-value caches, where each cache stores a dynamic queue"
          }
        ],
        "width": 612,
        "height": 792,
        "triggeredAutoMode": false,
        "structuredData": null,
        "noStructuredContent": false,
        "noTextContent": false
      },
      {
        "page": 4,
        "text": "                  ...                       ...                                                                  TDA\n                                                                    Image\n                                                                   Features                                 Positive Cache\n                    A stream of Test Samples\n                                                                                                           Keys        Values\n                                                                                                                                       Positive Cache Prediction    Adapted Prediction\n\n                                          Cat                                    CLIP Prediction            Negative Cache\n                       Hand-crafted      Panda                       Text                                  Keys        Values\n                          Prompt     +   Dog                     Embeddings                                                           Negative Cache Prediction\n                                         Bird\n                                         Class\n                                                                                          Update of Positive and Negative Caches in TDA\n                                                                                             Keys                                    Positive               Values\n                                                                        Image                                                     Pseudo Label\n                       : CLIP's Image Encoder                         Features                                                      Negative\n                       : CLIP's Text Encoder                                                 Keys                                 Pseudo Label              Values\n                                                                                                          CLIP Prediction\n\n                Figure 2. Overview of the proposed Training-free Dynamic Adapter (TDA). TDA constructs and updates two key-value caches to store\n                the knowledge of a stream of test samples, and uses the two caches to generate positive and negative predictions which are combined with\n                CLIP predictions to produce the final prediction. Specifically, the CLIP predictions are generated by performing the dot product between\n                the image features generated by CLIP’s image encoder Ev and the text embeddings generated by CLIP’s text encoder Et, using the hand-\n                crafted prompt and class names. The two key-value caches are updated by gradually incorporating the test features and their corresponding\n                pseudo labels calculated from CLIP’s predictions, based on prediction entropy and cache capacity.\n\n    Our goal is to conduct test-time adaptation by gather-\ning adequate knowledge from the testing data stream and\nimproving the predictions through the adaptation of the im-\nage features. To accomplish this goal, we create positive\nand negative caches that capture particular characteristics\nof the testing data stream. In the upcoming parts, we will\npresent the process of collecting data for each cache and\ndefine the conditions in which we can utilize cache infor-\nmation to adapt features and enhance model predictions.\n\nPositive Cache.           The positive cache in TDA is a key-\nvalue cache, in which keys and values are represented as\na dynamic queue. It aims to collect high-quality few-shot\npseudo labels Lˆp as positive values and the corresponding\nfeatures Qp as keys. The key-value cache is initially empty\nand then accumulates a sufficient number of key-value pairs\nduring the test-time adaptation. To maintain high-quality\n\npseudo labels, TDA progressively incorporates test predic-\ntions with lower entropy while limiting shot capacity1 in the\npositive cache. Different from a normal queue like a FIFO\nqueue with a fixed size, the dynamic queue in our method\nexpands in size during testing. Besides, the dynamic queue\noperates similarly to a priority queue, using entropy as the\ncriterion for prioritization. Note that, each class has its own\nqueue to maintain the order and correct data structure of\neach class in the cache.\n    Given a pre-trained CLIP model that consists of a text\nencoder Et and an image encoder Ev , Et processes class\n\n    1The shot capacity refers to the maximum number of pairs per class.\nnames with pre-defined prompts to generate c-channeled\ntext embeddings Wc and Ev processes each test image\nxtest to produce image features ftest. To build the posi-\ntive cache, TDA first generates a pseudo label lˆ, a one-hot\nencoded vector of a categorical distribution, for each test\nsample xtest by applying the softmax function to the pre-\ndiction ftestWT. We establish two conditions to ascertainc\nwhether and how to include the pseudo labelˆ and its corre- l\nsponding image features ftest into the positive cache. The\nfirst condition is defined as: if the shot number (the number\nof collected pairs per class) of Lˆp is less than the maximum\nshot capacity (the maximum capacity number of pairs per\nclass) k, TDA will add lˆ and ftest as a new value and a\nnew key to Lˆp and Qp respectively. Meanwhile, the sec-\nond condition is defined as: if the shot number of Lˆp has\nreached the maximum shot capacity k, TDA will replace an\n‘uncertain’ key-value pair {q\nH(ftestWT) < H(qentWc ). Here, H denotes the en-Tent, lp ˆent} with {ftest,ˆ} when l\n               c\ntropy function and the term ‘uncertain’ indicates that the\nentropy of a particular key-value pair is the highest com-\npared to the entropy of all other key-value pairs of the same\nclass in the cache model. By applying these two conditions,\nTDA can gradually integrate test predictions with lower en-\ntropy while controlling the shot capacity, which helps to en-\nsure the collection of high-quality pseudo labels in positive\ncache.\n    During test-time adaptation, the positive cache can\nquickly retrieve relevant information by treating the image\nfeatures ftest generated from a test example as a query and\nsearching the stored key-value pairs {Qp,ˆp} for match- L",
        "md": "# Overview of the proposed Training-free Dynamic Adapter (TDA)\n\nTDA constructs and updates two key-value caches to store the knowledge of a stream of test samples, and uses the two caches to generate positive and negative predictions which are combined with CLIP predictions to produce the final prediction. Specifically, the CLIP predictions are generated by performing the dot product between the image features generated by CLIP’s image encoder Ev and the text embeddings generated by CLIP’s text encoder Et, using the hand-crafted prompt and class names. The two key-value caches are updated by gradually incorporating the test features and their corresponding pseudo labels calculated from CLIP’s predictions, based on prediction entropy and cache capacity.\n\n# Our goal\n\nOur goal is to conduct test-time adaptation by gathering adequate knowledge from the testing data stream and improving the predictions through the adaptation of the image features. To accomplish this goal, we create positive and negative caches that capture particular characteristics of the testing data stream. In the upcoming parts, we will present the process of collecting data for each cache and define the conditions in which we can utilize cache information to adapt features and enhance model predictions.\n\n# Positive Cache\n\nThe positive cache in TDA is a key-value cache, in which keys and values are represented as a dynamic queue. It aims to collect high-quality few-shot pseudo labels Lˆp as positive values and the corresponding features Qp as keys. The key-value cache is initially empty and then accumulates a sufficient number of key-value pairs during the test-time adaptation. To maintain high-quality pseudo labels, TDA progressively incorporates test predictions with lower entropy while limiting shot capacity1 in the positive cache. Different from a normal queue like a FIFO queue with a fixed size, the dynamic queue in our method expands in size during testing. Besides, the dynamic queue operates similarly to a priority queue, using entropy as the criterion for prioritization. Note that, each class has its own queue to maintain the order and correct data structure of each class in the cache.\n\nGiven a pre-trained CLIP model that consists of a text encoder Et and an image encoder Ev, Et processes class names with pre-defined prompts to generate c-channeled text embeddings Wc and Ev processes each test image xtest to produce image features ftest. To build the positive cache, TDA first generates a pseudo label lˆ, a one-hot encoded vector of a categorical distribution, for each test sample xtest by applying the softmax function to the prediction ftestWT. We establish two conditions to ascertain whether and how to include the pseudo label lˆ and its corresponding image features ftest into the positive cache. The first condition is defined as: if the shot number (the number of collected pairs per class) of Lˆp is less than the maximum shot capacity (the maximum capacity number of pairs per class) k, TDA will add lˆ and ftest as a new value and a new key to Lˆp and Qp respectively. Meanwhile, the second condition is defined as: if the shot number of Lˆp has reached the maximum shot capacity k, TDA will replace an ‘uncertain’ key-value pair {qent, lˆ} with {ftest, lˆ} when H(ftestWT) < H(qentWc). Here, H denotes the entropy function and the term ‘uncertain’ indicates that the entropy of a particular key-value pair is the highest compared to the entropy of all other key-value pairs of the same class in the cache model. By applying these two conditions, TDA can gradually integrate test predictions with lower entropy while controlling the shot capacity, which helps to ensure the collection of high-quality pseudo labels in positive cache.\n\nDuring test-time adaptation, the positive cache can quickly retrieve relevant information by treating the image features ftest generated from a test example as a query and searching the stored key-value pairs {Qp, lˆ} for matches.\n\n1 The shot capacity refers to the maximum number of pairs per class.",
        "images": [
          {
            "name": "img_p3_1.png",
            "height": 493,
            "width": 500,
            "x": 85.9792042647443,
            "y": 75.26116619017692,
            "original_width": 500,
            "original_height": 493
          },
          {
            "name": "img_p3_2.png",
            "height": 639,
            "width": 640,
            "x": 61.82078172468608,
            "y": 75.2611664583313,
            "original_width": 640,
            "original_height": 639
          },
          {
            "name": "img_p3_3.png",
            "height": 549,
            "width": 732,
            "x": 106.59000430720573,
            "y": 74.82698481597102,
            "original_width": 732,
            "original_height": 549
          }
        ],
        "items": [
          {
            "type": "heading",
            "lvl": 1,
            "value": "Overview of the proposed Training-free Dynamic Adapter (TDA)",
            "md": "# Overview of the proposed Training-free Dynamic Adapter (TDA)",
            "bBox": {
              "x": 338,
              "y": 83,
              "w": 37.88,
              "h": 491.97
            }
          },
          {
            "type": "text",
            "value": "TDA constructs and updates two key-value caches to store the knowledge of a stream of test samples, and uses the two caches to generate positive and negative predictions which are combined with CLIP predictions to produce the final prediction. Specifically, the CLIP predictions are generated by performing the dot product between the image features generated by CLIP’s image encoder Ev and the text embeddings generated by CLIP’s text encoder Et, using the hand-crafted prompt and class names. The two key-value caches are updated by gradually incorporating the test features and their corresponding pseudo labels calculated from CLIP’s predictions, based on prediction entropy and cache capacity.",
            "md": "TDA constructs and updates two key-value caches to store the knowledge of a stream of test samples, and uses the two caches to generate positive and negative predictions which are combined with CLIP predictions to produce the final prediction. Specifically, the CLIP predictions are generated by performing the dot product between the image features generated by CLIP’s image encoder Ev and the text embeddings generated by CLIP’s text encoder Et, using the hand-crafted prompt and class names. The two key-value caches are updated by gradually incorporating the test features and their corresponding pseudo labels calculated from CLIP’s predictions, based on prediction entropy and cache capacity.",
            "bBox": {
              "x": 50,
              "y": 83,
              "w": 495,
              "h": 637.96
            }
          },
          {
            "type": "heading",
            "lvl": 1,
            "value": "Our goal",
            "md": "# Our goal",
            "bBox": {
              "x": 0,
              "y": 0,
              "w": 612,
              "h": 792
            }
          },
          {
            "type": "text",
            "value": "Our goal is to conduct test-time adaptation by gathering adequate knowledge from the testing data stream and improving the predictions through the adaptation of the image features. To accomplish this goal, we create positive and negative caches that capture particular characteristics of the testing data stream. In the upcoming parts, we will present the process of collecting data for each cache and define the conditions in which we can utilize cache information to adapt features and enhance model predictions.",
            "md": "Our goal is to conduct test-time adaptation by gathering adequate knowledge from the testing data stream and improving the predictions through the adaptation of the image features. To accomplish this goal, we create positive and negative caches that capture particular characteristics of the testing data stream. In the upcoming parts, we will present the process of collecting data for each cache and define the conditions in which we can utilize cache information to adapt features and enhance model predictions.",
            "bBox": {
              "x": 50,
              "y": 91,
              "w": 373.07,
              "h": 373.96
            }
          },
          {
            "type": "heading",
            "lvl": 1,
            "value": "Positive Cache",
            "md": "# Positive Cache",
            "bBox": {
              "x": 322,
              "y": 102,
              "w": 99.95,
              "h": 118.05
            }
          },
          {
            "type": "text",
            "value": "The positive cache in TDA is a key-value cache, in which keys and values are represented as a dynamic queue. It aims to collect high-quality few-shot pseudo labels Lˆp as positive values and the corresponding features Qp as keys. The key-value cache is initially empty and then accumulates a sufficient number of key-value pairs during the test-time adaptation. To maintain high-quality pseudo labels, TDA progressively incorporates test predictions with lower entropy while limiting shot capacity1 in the positive cache. Different from a normal queue like a FIFO queue with a fixed size, the dynamic queue in our method expands in size during testing. Besides, the dynamic queue operates similarly to a priority queue, using entropy as the criterion for prioritization. Note that, each class has its own queue to maintain the order and correct data structure of each class in the cache.\n\nGiven a pre-trained CLIP model that consists of a text encoder Et and an image encoder Ev, Et processes class names with pre-defined prompts to generate c-channeled text embeddings Wc and Ev processes each test image xtest to produce image features ftest. To build the positive cache, TDA first generates a pseudo label lˆ, a one-hot encoded vector of a categorical distribution, for each test sample xtest by applying the softmax function to the prediction ftestWT. We establish two conditions to ascertain whether and how to include the pseudo label lˆ and its corresponding image features ftest into the positive cache. The first condition is defined as: if the shot number (the number of collected pairs per class) of Lˆp is less than the maximum shot capacity (the maximum capacity number of pairs per class) k, TDA will add lˆ and ftest as a new value and a new key to Lˆp and Qp respectively. Meanwhile, the second condition is defined as: if the shot number of Lˆp has reached the maximum shot capacity k, TDA will replace an ‘uncertain’ key-value pair {qent, lˆ} with {ftest, lˆ} when H(ftestWT) < H(qentWc). Here, H denotes the entropy function and the term ‘uncertain’ indicates that the entropy of a particular key-value pair is the highest compared to the entropy of all other key-value pairs of the same class in the cache model. By applying these two conditions, TDA can gradually integrate test predictions with lower entropy while controlling the shot capacity, which helps to ensure the collection of high-quality pseudo labels in positive cache.\n\nDuring test-time adaptation, the positive cache can quickly retrieve relevant information by treating the image features ftest generated from a test example as a query and searching the stored key-value pairs {Qp, lˆ} for matches.\n\n1 The shot capacity refers to the maximum number of pairs per class.",
            "md": "The positive cache in TDA is a key-value cache, in which keys and values are represented as a dynamic queue. It aims to collect high-quality few-shot pseudo labels Lˆp as positive values and the corresponding features Qp as keys. The key-value cache is initially empty and then accumulates a sufficient number of key-value pairs during the test-time adaptation. To maintain high-quality pseudo labels, TDA progressively incorporates test predictions with lower entropy while limiting shot capacity1 in the positive cache. Different from a normal queue like a FIFO queue with a fixed size, the dynamic queue in our method expands in size during testing. Besides, the dynamic queue operates similarly to a priority queue, using entropy as the criterion for prioritization. Note that, each class has its own queue to maintain the order and correct data structure of each class in the cache.\n\nGiven a pre-trained CLIP model that consists of a text encoder Et and an image encoder Ev, Et processes class names with pre-defined prompts to generate c-channeled text embeddings Wc and Ev processes each test image xtest to produce image features ftest. To build the positive cache, TDA first generates a pseudo label lˆ, a one-hot encoded vector of a categorical distribution, for each test sample xtest by applying the softmax function to the prediction ftestWT. We establish two conditions to ascertain whether and how to include the pseudo label lˆ and its corresponding image features ftest into the positive cache. The first condition is defined as: if the shot number (the number of collected pairs per class) of Lˆp is less than the maximum shot capacity (the maximum capacity number of pairs per class) k, TDA will add lˆ and ftest as a new value and a new key to Lˆp and Qp respectively. Meanwhile, the second condition is defined as: if the shot number of Lˆp has reached the maximum shot capacity k, TDA will replace an ‘uncertain’ key-value pair {qent, lˆ} with {ftest, lˆ} when H(ftestWT) < H(qentWc). Here, H denotes the entropy function and the term ‘uncertain’ indicates that the entropy of a particular key-value pair is the highest compared to the entropy of all other key-value pairs of the same class in the cache model. By applying these two conditions, TDA can gradually integrate test predictions with lower entropy while controlling the shot capacity, which helps to ensure the collection of high-quality pseudo labels in positive cache.\n\nDuring test-time adaptation, the positive cache can quickly retrieve relevant information by treating the image features ftest generated from a test example as a query and searching the stored key-value pairs {Qp, lˆ} for matches.\n\n1 The shot capacity refers to the maximum number of pairs per class.",
            "bBox": {
              "x": 50,
              "y": 83,
              "w": 495.08,
              "h": 637.96
            }
          }
        ],
        "status": "OK",
        "links": [
          {
            "text": "1 positive cache. Different from a normal queue like a FIFO"
          }
        ],
        "width": 612,
        "height": 792,
        "triggeredAutoMode": false,
        "structuredData": null,
        "noStructuredContent": false,
        "noTextContent": false
      },
      {
        "page": 5,
        "text": "ing information. The adapted predictions using the positive\ncache can then be obtained as:\n                   Ppos(ftest) = A(ftestQT)Lp,ˆ\n                                                 p\nwhere A is the adaptation function defined in Tip-Adapter.\n\nNegative Cache.           Similar to the positive cache in our\nTDA, the negative cache is also a dynamic queue structure\nwith negative keys and negative values denoted as Qn and\nˆn, respectively. It aims to gather CLIP-generated image\nL\nfeatures to Qn and the corresponding negative pseudo la-\nbels to Lˆn. Unlike the pseudo labels in the positive cache,\nthe negative pseudo labels are obtained by applying nega-\ntive mask on the class probabilities as:\n                      Lˆn = −1[pl < P (Qn)],\n\nwhere higher probabilities than pl are selected as negative\npseudo labels from uncertain predictions and the uncer-\ntainty is measured by the entropy of predictions. Here, pl\nrepresents a threshold in negative pseudo labeling, and Lˆn\ndenotes a negative pseudo label, which is a vector whose el-\nements larger than pl have a value -1 and otherwise 0. Dif-\nferent from existing negative learning methods [22, 23, 33]\nthat select negative labels from all noisy labels, TDA selects\nnegative pseudo labels from uncertain predictions to avoid\nbias to the data with certain predictions.\n    When constructing the negative cache, the testing feature\nftest will be included in negative cache if it satisfies the\ncondition γ(ftest): the entropy of the prediction is in the\nspecified interval between τl and τh:\n                γ(ftest) : τl < H(ftestWT) < τh.c\nThis condition is designed to mitigate the risk of prediction\nerrors due to high entropy or be biased to certain predictions\n(characterized by very low entropy), by incorporating test\nsamples that exhibit a moderate degree of prediction uncer-\ntainty. Once the γ(ftest) check is completed, the remain-\ning steps for collecting uncertain samples in the negative\ncache follow the same two conditions designed in the pos-\nitive cache. Similar to the positive cache, TDA also limits\nthe shot capacity˜ in the negative cache. k\n    During test-time adaption, the testing features ftest can\nbe quickly adapted to target domains by retrieving the\nknowledge from {Qn,ˆn} in the negative cache and the L\nadapted prediction can be obtained as:\n                 Pneg(ftest) = −A(ftestQT)Ln,n        ˆ\n\nwhere A is the adaptation function defined in Tip-Adapter.\nThe predictions of TDA can be formulated by combining\nthe negative cache, the positive cache and the pre-trained\nCLIP model together as follows:\n  PTDA(ftest) = ftestWT + Ppos(ftest) + Pneg(ftest). (7)c\n\n           3.3. Relationship with TPT and Tip-Adapter\n\n           Both TPT and TDA are designed to handle the challenge of\n(3)        adapting models to test data that have distributional discrep-\n           ancies from the training data. TPT trains a learnable prompt\n           pc in Eq. (1) for each test sample with a large number\n           of augmentations and such training process requires back-\n           propagation and is computationally intensive. In contrast,\n           our TDA is training-free as both positive cache {Qp, Lˆp}\n\n           and negative cache {Qn, Lˆn} are non-parametric, which is\n           super-efficient without incurring any backpropagation.\n               Our TDA employs a cache model that shares similarities\n           with the Tip-Adapter, where features and labels are stored\n           as key-value pairs in a memory cache. One notable dis-\n(4)        tinction between the Tip-Adapter and TDA lies in the type\n           of cache used. Specifically, the Tip-Adapter relies on a\n           static cache {Ftrain, Ltrain} because it is designed for a su-\n           pervised adaptation setting, where the ground-truth labels\n           Ltrain are predetermined and readily available. In contrast,\n           our TDA introduces a new dynamic cache {Qp, Lˆp} de-\n           signed for a test-time adaptation setting, where the pseudo\n           labels Lˆp are generated on-the-fly from a stream of test\n           samples. Furthermore, TDA incorporates a novel negative\n           cache model {Qn,ˆn} that enhances testing predictions by L\n           utilizing indirect knowledge that a test image does not be-\n           long to certain negative classes. By combining positive and\n           negative caches, our proposed TDA is more robust to noisy\n           pseudo labels and can generalize well to testing data.\n\n\n(5)        4. Experiments\n           4.1. Experimental Setup\n\n           Benchmarks.          We conducted main experiments on two\n           benchmarks: out-of-distribution (OOD) benchmark and\n           cross-domain benchmark, both applied in the previous\n           work [35] that adapts vision-language models in test time.\n           The OOD benchmark serves as a measure of the ro-\n           bustness of our approach by involving assessment on 4\n           out-of-distribution datasets derived from ImageNet [5]:\n           ImageNet-A [16], ImageNet-V2 [32], ImageNet-R [15],\n           and ImageNet-S [41]. This benchmark is specifically de-\n           signed to evaluate a model’s capacity to generalize to new\n           and unseen data. The cross-domain benchmark, on the\n(6)        other hand, is involved to evaluate the model’s perfor-\n           mance across 10 diverse image classification datasets, each\n           from a distinct domain with different classes: Aircraft [26],\n           Caltech101 [8], Cars [24], DTD [4], EuroSAT [14],\n           Flower102 [28], Food101 [1], Pets [30], SUN397 [42],\n           and UCF101 [36]. This benchmark provides a comprehen-\n           sive evaluation of the model’s adaptability during test time\n           across various class spaces.",
        "md": "# 3.2 Negative Cache\n\nSimilar to the positive cache in our TDA, the negative cache is also a dynamic queue structure with negative keys and negative values denoted as Qn and ˆn, respectively. It aims to gather CLIP-generated image features to Qn and the corresponding negative pseudo labels to Lˆn. Unlike the pseudo labels in the positive cache, the negative pseudo labels are obtained by applying negative mask on the class probabilities as:\n\nLˆn = −1[pl < P (Qn)],\n\nwhere higher probabilities than pl are selected as negative pseudo labels from uncertain predictions and the uncertainty is measured by the entropy of predictions. Here, pl represents a threshold in negative pseudo labeling, and Lˆn denotes a negative pseudo label, which is a vector whose elements larger than pl have a value -1 and otherwise 0. Different from existing negative learning methods [22, 23, 33] that select negative labels from all noisy labels, TDA selects negative pseudo labels from uncertain predictions to avoid bias to the data with certain predictions.\n\nWhen constructing the negative cache, the testing feature ftest will be included in negative cache if it satisfies the condition γ(ftest): the entropy of the prediction is in the specified interval between τl and τh:\n\nγ(ftest) : τl < H(ftestWT) < τh.c\n\nThis condition is designed to mitigate the risk of prediction errors due to high entropy or be biased to certain predictions (characterized by very low entropy), by incorporating test samples that exhibit a moderate degree of prediction uncertainty. Once the γ(ftest) check is completed, the remaining steps for collecting uncertain samples in the negative cache follow the same two conditions designed in the positive cache. Similar to the positive cache, TDA also limits the shot capacity ˜ in the negative cache.\n\nDuring test-time adaption, the testing features ftest can be quickly adapted to target domains by retrieving the knowledge from {Qn,ˆn} in the negative cache and the adapted prediction can be obtained as:\n\nPneg(ftest) = −A(ftestQT)Ln,n ˆ\n\nwhere A is the adaptation function defined in Tip-Adapter. The predictions of TDA can be formulated by combining the negative cache, the positive cache and the pre-trained CLIP model together as follows:\n\nPTDA(ftest) = ftestWT + Ppos(ftest) + Pneg(ftest). (7)c\n\n# 3.3 Relationship with TPT and Tip-Adapter\n\nBoth TPT and TDA are designed to handle the challenge of adapting models to test data that have distributional discrepancies from the training data. TPT trains a learnable prompt pc in Eq. (1) for each test sample with a large number of augmentations and such training process requires back-propagation and is computationally intensive. In contrast, our TDA is training-free as both positive cache {Qp, Lˆp} and negative cache {Qn, Lˆn} are non-parametric, which is super-efficient without incurring any backpropagation.\n\nOur TDA employs a cache model that shares similarities with the Tip-Adapter, where features and labels are stored as key-value pairs in a memory cache. One notable distinction between the Tip-Adapter and TDA lies in the type of cache used. Specifically, the Tip-Adapter relies on a static cache {Ftrain, Ltrain} because it is designed for a supervised adaptation setting, where the ground-truth labels Ltrain are predetermined and readily available. In contrast, our TDA introduces a new dynamic cache {Qp, Lˆp} designed for a test-time adaptation setting, where the pseudo labels Lˆp are generated on-the-fly from a stream of test samples. Furthermore, TDA incorporates a novel negative cache model {Qn,ˆn} that enhances testing predictions by utilizing indirect knowledge that a test image does not belong to certain negative classes. By combining positive and negative caches, our proposed TDA is more robust to noisy pseudo labels and can generalize well to testing data.\n\n# 4. Experiments\n\n# 4.1 Experimental Setup\n\nBenchmarks. We conducted main experiments on two benchmarks: out-of-distribution (OOD) benchmark and cross-domain benchmark, both applied in the previous work [35] that adapts vision-language models in test time. The OOD benchmark serves as a measure of the robustness of our approach by involving assessment on 4 out-of-distribution datasets derived from ImageNet [5]: ImageNet-A [16], ImageNet-V2 [32], ImageNet-R [15], and ImageNet-S [41]. This benchmark is specifically designed to evaluate a model’s capacity to generalize to new and unseen data. The cross-domain benchmark, on the other hand, is involved to evaluate the model’s performance across 10 diverse image classification datasets, each from a distinct domain with different classes: Aircraft [26], Caltech101 [8], Cars [24], DTD [4], EuroSAT [14], Flower102 [28], Food101 [1], Pets [30], SUN397 [42], and UCF101 [36]. This benchmark provides a comprehensive evaluation of the model’s adaptability during test time across various class spaces.",
        "images": [],
        "items": [
          {
            "type": "heading",
            "lvl": 1,
            "value": "3.2 Negative Cache",
            "md": "# 3.2 Negative Cache",
            "bBox": {
              "x": 148,
              "y": 705,
              "w": 3.56,
              "h": 6.97
            }
          },
          {
            "type": "text",
            "value": "Similar to the positive cache in our TDA, the negative cache is also a dynamic queue structure with negative keys and negative values denoted as Qn and ˆn, respectively. It aims to gather CLIP-generated image features to Qn and the corresponding negative pseudo labels to Lˆn. Unlike the pseudo labels in the positive cache, the negative pseudo labels are obtained by applying negative mask on the class probabilities as:\n\nLˆn = −1[pl < P (Qn)],\n\nwhere higher probabilities than pl are selected as negative pseudo labels from uncertain predictions and the uncertainty is measured by the entropy of predictions. Here, pl represents a threshold in negative pseudo labeling, and Lˆn denotes a negative pseudo label, which is a vector whose elements larger than pl have a value -1 and otherwise 0. Different from existing negative learning methods [22, 23, 33] that select negative labels from all noisy labels, TDA selects negative pseudo labels from uncertain predictions to avoid bias to the data with certain predictions.\n\nWhen constructing the negative cache, the testing feature ftest will be included in negative cache if it satisfies the condition γ(ftest): the entropy of the prediction is in the specified interval between τl and τh:\n\nγ(ftest) : τl < H(ftestWT) < τh.c\n\nThis condition is designed to mitigate the risk of prediction errors due to high entropy or be biased to certain predictions (characterized by very low entropy), by incorporating test samples that exhibit a moderate degree of prediction uncertainty. Once the γ(ftest) check is completed, the remaining steps for collecting uncertain samples in the negative cache follow the same two conditions designed in the positive cache. Similar to the positive cache, TDA also limits the shot capacity ˜ in the negative cache.\n\nDuring test-time adaption, the testing features ftest can be quickly adapted to target domains by retrieving the knowledge from {Qn,ˆn} in the negative cache and the adapted prediction can be obtained as:\n\nPneg(ftest) = −A(ftestQT)Ln,n ˆ\n\nwhere A is the adaptation function defined in Tip-Adapter. The predictions of TDA can be formulated by combining the negative cache, the positive cache and the pre-trained CLIP model together as follows:\n\nPTDA(ftest) = ftestWT + Ppos(ftest) + Pneg(ftest). (7)c",
            "md": "Similar to the positive cache in our TDA, the negative cache is also a dynamic queue structure with negative keys and negative values denoted as Qn and ˆn, respectively. It aims to gather CLIP-generated image features to Qn and the corresponding negative pseudo labels to Lˆn. Unlike the pseudo labels in the positive cache, the negative pseudo labels are obtained by applying negative mask on the class probabilities as:\n\nLˆn = −1[pl < P (Qn)],\n\nwhere higher probabilities than pl are selected as negative pseudo labels from uncertain predictions and the uncertainty is measured by the entropy of predictions. Here, pl represents a threshold in negative pseudo labeling, and Lˆn denotes a negative pseudo label, which is a vector whose elements larger than pl have a value -1 and otherwise 0. Different from existing negative learning methods [22, 23, 33] that select negative labels from all noisy labels, TDA selects negative pseudo labels from uncertain predictions to avoid bias to the data with certain predictions.\n\nWhen constructing the negative cache, the testing feature ftest will be included in negative cache if it satisfies the condition γ(ftest): the entropy of the prediction is in the specified interval between τl and τh:\n\nγ(ftest) : τl < H(ftestWT) < τh.c\n\nThis condition is designed to mitigate the risk of prediction errors due to high entropy or be biased to certain predictions (characterized by very low entropy), by incorporating test samples that exhibit a moderate degree of prediction uncertainty. Once the γ(ftest) check is completed, the remaining steps for collecting uncertain samples in the negative cache follow the same two conditions designed in the positive cache. Similar to the positive cache, TDA also limits the shot capacity ˜ in the negative cache.\n\nDuring test-time adaption, the testing features ftest can be quickly adapted to target domains by retrieving the knowledge from {Qn,ˆn} in the negative cache and the adapted prediction can be obtained as:\n\nPneg(ftest) = −A(ftestQT)Ln,n ˆ\n\nwhere A is the adaptation function defined in Tip-Adapter. The predictions of TDA can be formulated by combining the negative cache, the positive cache and the pre-trained CLIP model together as follows:\n\nPTDA(ftest) = ftestWT + Ppos(ftest) + Pneg(ftest). (7)c",
            "bBox": {
              "x": 50,
              "y": 110,
              "w": 340.89,
              "h": 601.97
            }
          },
          {
            "type": "heading",
            "lvl": 1,
            "value": "3.3 Relationship with TPT and Tip-Adapter",
            "md": "# 3.3 Relationship with TPT and Tip-Adapter",
            "bBox": {
              "x": 50,
              "y": 115,
              "w": 159.75,
              "h": 153.96
            }
          },
          {
            "type": "text",
            "value": "Both TPT and TDA are designed to handle the challenge of adapting models to test data that have distributional discrepancies from the training data. TPT trains a learnable prompt pc in Eq. (1) for each test sample with a large number of augmentations and such training process requires back-propagation and is computationally intensive. In contrast, our TDA is training-free as both positive cache {Qp, Lˆp} and negative cache {Qn, Lˆn} are non-parametric, which is super-efficient without incurring any backpropagation.\n\nOur TDA employs a cache model that shares similarities with the Tip-Adapter, where features and labels are stored as key-value pairs in a memory cache. One notable distinction between the Tip-Adapter and TDA lies in the type of cache used. Specifically, the Tip-Adapter relies on a static cache {Ftrain, Ltrain} because it is designed for a supervised adaptation setting, where the ground-truth labels Ltrain are predetermined and readily available. In contrast, our TDA introduces a new dynamic cache {Qp, Lˆp} designed for a test-time adaptation setting, where the pseudo labels Lˆp are generated on-the-fly from a stream of test samples. Furthermore, TDA incorporates a novel negative cache model {Qn,ˆn} that enhances testing predictions by utilizing indirect knowledge that a test image does not belong to certain negative classes. By combining positive and negative caches, our proposed TDA is more robust to noisy pseudo labels and can generalize well to testing data.",
            "md": "Both TPT and TDA are designed to handle the challenge of adapting models to test data that have distributional discrepancies from the training data. TPT trains a learnable prompt pc in Eq. (1) for each test sample with a large number of augmentations and such training process requires back-propagation and is computationally intensive. In contrast, our TDA is training-free as both positive cache {Qp, Lˆp} and negative cache {Qn, Lˆn} are non-parametric, which is super-efficient without incurring any backpropagation.\n\nOur TDA employs a cache model that shares similarities with the Tip-Adapter, where features and labels are stored as key-value pairs in a memory cache. One notable distinction between the Tip-Adapter and TDA lies in the type of cache used. Specifically, the Tip-Adapter relies on a static cache {Ftrain, Ltrain} because it is designed for a supervised adaptation setting, where the ground-truth labels Ltrain are predetermined and readily available. In contrast, our TDA introduces a new dynamic cache {Qp, Lˆp} designed for a test-time adaptation setting, where the pseudo labels Lˆp are generated on-the-fly from a stream of test samples. Furthermore, TDA incorporates a novel negative cache model {Qn,ˆn} that enhances testing predictions by utilizing indirect knowledge that a test image does not belong to certain negative classes. By combining positive and negative caches, our proposed TDA is more robust to noisy pseudo labels and can generalize well to testing data.",
            "bBox": {
              "x": 50,
              "y": 105,
              "w": 494.98,
              "h": 606.97
            }
          },
          {
            "type": "heading",
            "lvl": 1,
            "value": "4. Experiments",
            "md": "# 4. Experiments",
            "bBox": {
              "x": 308,
              "y": 449,
              "w": 77.04,
              "h": 11.96
            }
          },
          {
            "type": "heading",
            "lvl": 1,
            "value": "4.1 Experimental Setup",
            "md": "# 4.1 Experimental Setup",
            "bBox": {
              "x": 50,
              "y": 193,
              "w": 74.89,
              "h": 75.96
            }
          },
          {
            "type": "text",
            "value": "Benchmarks. We conducted main experiments on two benchmarks: out-of-distribution (OOD) benchmark and cross-domain benchmark, both applied in the previous work [35] that adapts vision-language models in test time. The OOD benchmark serves as a measure of the robustness of our approach by involving assessment on 4 out-of-distribution datasets derived from ImageNet [5]: ImageNet-A [16], ImageNet-V2 [32], ImageNet-R [15], and ImageNet-S [41]. This benchmark is specifically designed to evaluate a model’s capacity to generalize to new and unseen data. The cross-domain benchmark, on the other hand, is involved to evaluate the model’s performance across 10 diverse image classification datasets, each from a distinct domain with different classes: Aircraft [26], Caltech101 [8], Cars [24], DTD [4], EuroSAT [14], Flower102 [28], Food101 [1], Pets [30], SUN397 [42], and UCF101 [36]. This benchmark provides a comprehensive evaluation of the model’s adaptability during test time across various class spaces.",
            "md": "Benchmarks. We conducted main experiments on two benchmarks: out-of-distribution (OOD) benchmark and cross-domain benchmark, both applied in the previous work [35] that adapts vision-language models in test time. The OOD benchmark serves as a measure of the robustness of our approach by involving assessment on 4 out-of-distribution datasets derived from ImageNet [5]: ImageNet-A [16], ImageNet-V2 [32], ImageNet-R [15], and ImageNet-S [41]. This benchmark is specifically designed to evaluate a model’s capacity to generalize to new and unseen data. The cross-domain benchmark, on the other hand, is involved to evaluate the model’s performance across 10 diverse image classification datasets, each from a distinct domain with different classes: Aircraft [26], Caltech101 [8], Cars [24], DTD [4], EuroSAT [14], Flower102 [28], Food101 [1], Pets [30], SUN397 [42], and UCF101 [36]. This benchmark provides a comprehensive evaluation of the model’s adaptability during test time across various class spaces.",
            "bBox": {
              "x": 50,
              "y": 193,
              "w": 495.11,
              "h": 528.96
            }
          }
        ],
        "status": "OK",
        "links": [
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": "of augmentations and such training process requires back-"
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          }
        ],
        "width": 612,
        "height": 792,
        "triggeredAutoMode": false,
        "structuredData": null,
        "noStructuredContent": false,
        "noTextContent": false
      },
      {
        "page": 6,
        "text": "                  Method                    ImageNet        ImageNet-A          ImageNet-V2          ImageNet-R          ImageNet-S         Average   OOD Average\n                  CLIP-ResNet-50              59.81             23.24                52.91               60.72               35.48            46.43         43.09\n                  CoOp                        63.33             23.06                55.40               56.60               34.67            46.61         42.43\n                  CoCoOp                      62.81             23.32                55.72               57.74               34.48            46.81         42.82\n                  Tip-Adapter                 62.03             23.13                53.97               60.35               35.74            47.04         43.30\n                  TPT                         60.74             26.67                54.70               59.11               35.09            47.26         43.89\n                  DiffTPT                     60.80             31.06                55.80               58.80               37.10            48.71         45.69\n                  TDA (Ours)                  61.35             30.29                55.54               62.58               38.12            49.58         46.63\n                  CLIP-ViT-B/16               68.34             49.89                61.88               77.65               48.24            61.20         59.42\n                  CoOp                        71.51             49.71                64.20               75.21               47.99            61.72         59.28\n                  CoCoOp                      71.02             50.63                64.07               76.18               48.75            62.13         59.91\n                  Tip-Adapter                 70.75             51.04                63.41               77.76               48.88            62.37         60.27\n                  TPT                         68.98             54.77                63.45               77.06               47.94            62.44         60.81\n                  DiffTPT                     70.30             55.68                65.10               75.00               46.80            62.28         60.52\n                  TDA (Ours)                  69.51             60.11                64.67               80.24               50.54            65.01         63.89\n\n                Table 1. Results on the OOD Benchmark. Our TDA is compared with several state-of-the-art methods designed for vision-language\n                models: the baseline method CLIP, three train-time adaptation methods (i.e., CoOp, CoCoOp, and Tip-Adapter), and two test-time adap-\n                tation methods (i.e., TPT and DiffTPT). All the compared methods are built upon CLIP-ResNet-50 or CLIP-ViT-B/16 baselines. The two\n                evaluation metrics Average and OOD Average are calculated by taking the mean accuracy across all five datasets and four OOD datasets\n                excluding ImageNet. The results of CLIP, CoOp, CoCoOp, and TPT are obtained from the TPT paper, the results of DiffTPT are obtained\n                from the DiffTPT paper, while the results of Tip-Adapter are reproduced using the official codes.\n\n   Method                   Testing Time         Accuracy         Gain\n   CLIP-ResNet-50                12min              59.81           0\n   TPT                        12h 50min             60.74        +0.93\n   DiffTPT                    34h 45min             60.80        +0.99\n   TDA (Ours)                    16min              61.35        +1.54\nTable 2. Comparisons of our TDA with CLIP-ResNet-50, TPT,\nand DiffTPT in terms of efficiency (Testing Time) and effective-\nness (Accuracy). The final column shows the accuracy gain rel-\native to the baseline CLIP. Note that the testing time of DiffTPT\ndoes not include the duration required for the image generation\nprocess with pre-trained diffusion models, which is an additional\ntime-consuming factor during the testing phase.\n\n\nImplementation details.             All the models in our experi-\nments are built upon the pre-trained CLIP model [31] that\nconsists of an image encoder and a text encoder. The im-\nage encoder can be either ResNet [13] or Vision Trans-\nformer [7], while the text encoder is Transformer [39]. Test-\ntime adaptation is set for single-image scenarios, using a\nbatch size of 1. We conduct a search for all our hyper-\nparameters using a single ImageNet validation set. The\nthreshold pl for negative pseudo-labeling in Eq 4 is set as\n0.03. The upper and lower thresholds [τ, τh] for testingl\nfeature selection in Eq 5 are set as [0.2, 0.5]. Once searched,\nthese hyperparameters are fixed and evaluated across vari-\nous new datasets. To avoid incurring backpropagation when\nusing learnable prompt, we follow [31] to use hand-crafted\nprompts. We use top-1 accuracy (%), a standard classifica-\ntion criterion, as our evaluation metric. All the experiments\nare conducted using a single NVIDIA Quadro RTX 6000\nGPU.\n4.2. Comparisons with State-of-the-art\nIn this section, we compare our proposed TDA with sev-\neral state-of-the-art methods, including CLIP [31], three\ntrain-time adaptation methods, i.e., CoOp [51], CoCoOp\n[50], and Tip-Adapter [48], as well as two existing test-\ntime adaptation methods TPT [35] and DiffTPT [9], all of\nwhich are designed for vision-language models. Specif-\nically, CLIP is evaluated using an ensemble of 80 hand-\ncrafted prompts as in [31]. All train-time adaptation meth-\nods are trained on the ImageNet train set with 16 shots per\nclass and tested on other datasets as in [35]. CoOp utilizes\na learnable context module of size 4 that is fine-tuned for\nspecific downstream tasks, while CoCoOp is the general-\nized version of CoOp with the input-conditional token for\nimage features. Tip-Adapter employs the optimal hyperpa-\nrameters obtained from the ImageNet validation set during\nevaluation. We would like to note that Tip-Adapter is un-\nable to handle new classes during testing, limiting its imple-\nmentation to OOD benchmark evaluation where the training\nclasses encompass all the testing classes. Different from\ntrain-time adaptation methods, test-time adaptation meth-\nods (i.e., TPT, DiffTPT, and our TDA) do not utilize the\nImageNet train set. Instead, they are fine-tuned with target\ndatasets using a stream of unlabeled test samples. Follow-",
        "md": "# Table 1. Results on the OOD Benchmark.\n\n|Method|ImageNet|ImageNet-A|ImageNet-V2|ImageNet-R|ImageNet-S|Average|OOD Average|\n|---|---|---|---|---|---|---|---|\n|CLIP-ResNet-50|59.81|23.24|52.91|60.72|35.48|46.43|43.09|\n|CoOp|63.33|23.06|55.40|56.60|34.67|46.61|42.43|\n|CoCoOp|62.81|23.32|55.72|57.74|34.48|46.81|42.82|\n|Tip-Adapter|62.03|23.13|53.97|60.35|35.74|47.04|43.30|\n|TPT|60.74|26.67|54.70|59.11|35.09|47.26|43.89|\n|DiffTPT|60.80|31.06|55.80|58.80|37.10|48.71|45.69|\n|TDA (Ours)|61.35|30.29|55.54|62.58|38.12|49.58|46.63|\n|CLIP-ViT-B/16|68.34|49.89|61.88|77.65|48.24|61.20|59.42|\n|CoOp|71.51|49.71|64.20|75.21|47.99|61.72|59.28|\n|CoCoOp|71.02|50.63|64.07|76.18|48.75|62.13|59.91|\n|Tip-Adapter|70.75|51.04|63.41|77.76|48.88|62.37|60.27|\n|TPT|68.98|54.77|63.45|77.06|47.94|62.44|60.81|\n|DiffTPT|70.30|55.68|65.10|75.00|46.80|62.28|60.52|\n|TDA (Ours)|69.51|60.11|64.67|80.24|50.54|65.01|63.89|\n\nOur TDA is compared with several state-of-the-art methods designed for vision-language models: the baseline method CLIP, three train-time adaptation methods (i.e., CoOp, CoCoOp, and Tip-Adapter), and two test-time adaptation methods (i.e., TPT and DiffTPT). All the compared methods are built upon CLIP-ResNet-50 or CLIP-ViT-B/16 baselines. The two evaluation metrics Average and OOD Average are calculated by taking the mean accuracy across all five datasets and four OOD datasets excluding ImageNet. The results of CLIP, CoOp, CoCoOp, and TPT are obtained from the TPT paper, the results of DiffTPT are obtained from the DiffTPT paper, while the results of Tip-Adapter are reproduced using the official codes.\n\n# Table 2. Comparisons of our TDA with CLIP-ResNet-50, TPT, and DiffTPT in terms of efficiency (Testing Time) and effectiveness (Accuracy).\n\n|Method|Testing Time|Accuracy|Gain|\n|---|---|---|---|\n|CLIP-ResNet-50|12min|59.81|0|\n|TPT|12h 50min|60.74|+0.93|\n|DiffTPT|34h 45min|60.80|+0.99|\n|TDA (Ours)|16min|61.35|+1.54|\n\nThe final column shows the accuracy gain relative to the baseline CLIP. Note that the testing time of DiffTPT does not include the duration required for the image generation process with pre-trained diffusion models, which is an additional time-consuming factor during the testing phase.\n\n# Implementation details.\n\nAll the models in our experiments are built upon the pre-trained CLIP model [31] that consists of an image encoder and a text encoder. The image encoder can be either ResNet [13] or Vision Transformer [7], while the text encoder is Transformer [39]. Test-time adaptation is set for single-image scenarios, using a batch size of 1. We conduct a search for all our hyperparameters using a single ImageNet validation set. The threshold pl for negative pseudo-labeling in Eq 4 is set as 0.03. The upper and lower thresholds [τ, τh] for testing feature selection in Eq 5 are set as [0.2, 0.5]. Once searched, these hyperparameters are fixed and evaluated across various new datasets. To avoid incurring backpropagation when using learnable prompt, we follow [31] to use hand-crafted prompts. We use top-1 accuracy (%), a standard classification criterion, as our evaluation metric. All the experiments are conducted using a single NVIDIA Quadro RTX 6000 GPU.\n\n# 4.2. Comparisons with State-of-the-art\n\nIn this section, we compare our proposed TDA with several state-of-the-art methods, including CLIP [31], three train-time adaptation methods, i.e., CoOp [51], CoCoOp [50], and Tip-Adapter [48], as well as two existing test-time adaptation methods TPT [35] and DiffTPT [9], all of which are designed for vision-language models. Specifically, CLIP is evaluated using an ensemble of 80 hand-crafted prompts as in [31]. All train-time adaptation methods are trained on the ImageNet train set with 16 shots per class and tested on other datasets as in [35]. CoOp utilizes a learnable context module of size 4 that is fine-tuned for specific downstream tasks, while CoCoOp is the generalized version of CoOp with the input-conditional token for image features. Tip-Adapter employs the optimal hyperparameters obtained from the ImageNet validation set during evaluation. We would like to note that Tip-Adapter is unable to handle new classes during testing, limiting its implementation to OOD benchmark evaluation where the training classes encompass all the testing classes. Different from train-time adaptation methods, test-time adaptation methods (i.e., TPT, DiffTPT, and our TDA) do not utilize the ImageNet train set. Instead, they are fine-tuned with target datasets using a stream of unlabeled test samples.",
        "images": [],
        "items": [
          {
            "type": "heading",
            "lvl": 1,
            "value": "Table 1. Results on the OOD Benchmark.",
            "md": "# Table 1. Results on the OOD Benchmark.",
            "bBox": {
              "x": 218,
              "y": 665,
              "w": 2.52,
              "h": 6.97
            }
          },
          {
            "type": "table",
            "rows": [
              [
                "Method",
                "ImageNet",
                "ImageNet-A",
                "ImageNet-V2",
                "ImageNet-R",
                "ImageNet-S",
                "Average",
                "OOD Average"
              ],
              [
                "CLIP-ResNet-50",
                "59.81",
                "23.24",
                "52.91",
                "60.72",
                "35.48",
                "46.43",
                "43.09"
              ],
              [
                "CoOp",
                "63.33",
                "23.06",
                "55.40",
                "56.60",
                "34.67",
                "46.61",
                "42.43"
              ],
              [
                "CoCoOp",
                "62.81",
                "23.32",
                "55.72",
                "57.74",
                "34.48",
                "46.81",
                "42.82"
              ],
              [
                "Tip-Adapter",
                "62.03",
                "23.13",
                "53.97",
                "60.35",
                "35.74",
                "47.04",
                "43.30"
              ],
              [
                "TPT",
                "60.74",
                "26.67",
                "54.70",
                "59.11",
                "35.09",
                "47.26",
                "43.89"
              ],
              [
                "DiffTPT",
                "60.80",
                "31.06",
                "55.80",
                "58.80",
                "37.10",
                "48.71",
                "45.69"
              ],
              [
                "TDA (Ours)",
                "61.35",
                "30.29",
                "55.54",
                "62.58",
                "38.12",
                "49.58",
                "46.63"
              ],
              [
                "CLIP-ViT-B/16",
                "68.34",
                "49.89",
                "61.88",
                "77.65",
                "48.24",
                "61.20",
                "59.42"
              ],
              [
                "CoOp",
                "71.51",
                "49.71",
                "64.20",
                "75.21",
                "47.99",
                "61.72",
                "59.28"
              ],
              [
                "CoCoOp",
                "71.02",
                "50.63",
                "64.07",
                "76.18",
                "48.75",
                "62.13",
                "59.91"
              ],
              [
                "Tip-Adapter",
                "70.75",
                "51.04",
                "63.41",
                "77.76",
                "48.88",
                "62.37",
                "60.27"
              ],
              [
                "TPT",
                "68.98",
                "54.77",
                "63.45",
                "77.06",
                "47.94",
                "62.44",
                "60.81"
              ],
              [
                "DiffTPT",
                "70.30",
                "55.68",
                "65.10",
                "75.00",
                "46.80",
                "62.28",
                "60.52"
              ],
              [
                "TDA (Ours)",
                "69.51",
                "60.11",
                "64.67",
                "80.24",
                "50.54",
                "65.01",
                "63.89"
              ]
            ],
            "md": "|Method|ImageNet|ImageNet-A|ImageNet-V2|ImageNet-R|ImageNet-S|Average|OOD Average|\n|---|---|---|---|---|---|---|---|\n|CLIP-ResNet-50|59.81|23.24|52.91|60.72|35.48|46.43|43.09|\n|CoOp|63.33|23.06|55.40|56.60|34.67|46.61|42.43|\n|CoCoOp|62.81|23.32|55.72|57.74|34.48|46.81|42.82|\n|Tip-Adapter|62.03|23.13|53.97|60.35|35.74|47.04|43.30|\n|TPT|60.74|26.67|54.70|59.11|35.09|47.26|43.89|\n|DiffTPT|60.80|31.06|55.80|58.80|37.10|48.71|45.69|\n|TDA (Ours)|61.35|30.29|55.54|62.58|38.12|49.58|46.63|\n|CLIP-ViT-B/16|68.34|49.89|61.88|77.65|48.24|61.20|59.42|\n|CoOp|71.51|49.71|64.20|75.21|47.99|61.72|59.28|\n|CoCoOp|71.02|50.63|64.07|76.18|48.75|62.13|59.91|\n|Tip-Adapter|70.75|51.04|63.41|77.76|48.88|62.37|60.27|\n|TPT|68.98|54.77|63.45|77.06|47.94|62.44|60.81|\n|DiffTPT|70.30|55.68|65.10|75.00|46.80|62.28|60.52|\n|TDA (Ours)|69.51|60.11|64.67|80.24|50.54|65.01|63.89|",
            "isPerfectTable": true,
            "csv": "\"Method\",\"ImageNet\",\"ImageNet-A\",\"ImageNet-V2\",\"ImageNet-R\",\"ImageNet-S\",\"Average\",\"OOD Average\"\n\"CLIP-ResNet-50\",\"59.81\",\"23.24\",\"52.91\",\"60.72\",\"35.48\",\"46.43\",\"43.09\"\n\"CoOp\",\"63.33\",\"23.06\",\"55.40\",\"56.60\",\"34.67\",\"46.61\",\"42.43\"\n\"CoCoOp\",\"62.81\",\"23.32\",\"55.72\",\"57.74\",\"34.48\",\"46.81\",\"42.82\"\n\"Tip-Adapter\",\"62.03\",\"23.13\",\"53.97\",\"60.35\",\"35.74\",\"47.04\",\"43.30\"\n\"TPT\",\"60.74\",\"26.67\",\"54.70\",\"59.11\",\"35.09\",\"47.26\",\"43.89\"\n\"DiffTPT\",\"60.80\",\"31.06\",\"55.80\",\"58.80\",\"37.10\",\"48.71\",\"45.69\"\n\"TDA (Ours)\",\"61.35\",\"30.29\",\"55.54\",\"62.58\",\"38.12\",\"49.58\",\"46.63\"\n\"CLIP-ViT-B/16\",\"68.34\",\"49.89\",\"61.88\",\"77.65\",\"48.24\",\"61.20\",\"59.42\"\n\"CoOp\",\"71.51\",\"49.71\",\"64.20\",\"75.21\",\"47.99\",\"61.72\",\"59.28\"\n\"CoCoOp\",\"71.02\",\"50.63\",\"64.07\",\"76.18\",\"48.75\",\"62.13\",\"59.91\"\n\"Tip-Adapter\",\"70.75\",\"51.04\",\"63.41\",\"77.76\",\"48.88\",\"62.37\",\"60.27\"\n\"TPT\",\"68.98\",\"54.77\",\"63.45\",\"77.06\",\"47.94\",\"62.44\",\"60.81\"\n\"DiffTPT\",\"70.30\",\"55.68\",\"65.10\",\"75.00\",\"46.80\",\"62.28\",\"60.52\"\n\"TDA (Ours)\",\"69.51\",\"60.11\",\"64.67\",\"80.24\",\"50.54\",\"65.01\",\"63.89\"",
            "bBox": {
              "x": 58,
              "y": 84,
              "w": 478.45,
              "h": 587.97
            }
          },
          {
            "type": "text",
            "value": "Our TDA is compared with several state-of-the-art methods designed for vision-language models: the baseline method CLIP, three train-time adaptation methods (i.e., CoOp, CoCoOp, and Tip-Adapter), and two test-time adaptation methods (i.e., TPT and DiffTPT). All the compared methods are built upon CLIP-ResNet-50 or CLIP-ViT-B/16 baselines. The two evaluation metrics Average and OOD Average are calculated by taking the mean accuracy across all five datasets and four OOD datasets excluding ImageNet. The results of CLIP, CoOp, CoCoOp, and TPT are obtained from the TPT paper, the results of DiffTPT are obtained from the DiffTPT paper, while the results of Tip-Adapter are reproduced using the official codes.",
            "md": "Our TDA is compared with several state-of-the-art methods designed for vision-language models: the baseline method CLIP, three train-time adaptation methods (i.e., CoOp, CoCoOp, and Tip-Adapter), and two test-time adaptation methods (i.e., TPT and DiffTPT). All the compared methods are built upon CLIP-ResNet-50 or CLIP-ViT-B/16 baselines. The two evaluation metrics Average and OOD Average are calculated by taking the mean accuracy across all five datasets and four OOD datasets excluding ImageNet. The results of CLIP, CoOp, CoCoOp, and TPT are obtained from the TPT paper, the results of DiffTPT are obtained from the DiffTPT paper, while the results of Tip-Adapter are reproduced using the official codes.",
            "bBox": {
              "x": 50,
              "y": 84,
              "w": 495,
              "h": 587.97
            }
          },
          {
            "type": "heading",
            "lvl": 1,
            "value": "Table 2. Comparisons of our TDA with CLIP-ResNet-50, TPT, and DiffTPT in terms of efficiency (Testing Time) and effectiveness (Accuracy).",
            "md": "# Table 2. Comparisons of our TDA with CLIP-ResNet-50, TPT, and DiffTPT in terms of efficiency (Testing Time) and effectiveness (Accuracy).",
            "bBox": {
              "x": 50,
              "y": 99,
              "w": 235.87,
              "h": 572.97
            }
          },
          {
            "type": "table",
            "rows": [
              [
                "Method",
                "Testing Time",
                "Accuracy",
                "Gain"
              ],
              [
                "CLIP-ResNet-50",
                "12min",
                "59.81",
                "0"
              ],
              [
                "TPT",
                "12h 50min",
                "60.74",
                "+0.93"
              ],
              [
                "DiffTPT",
                "34h 45min",
                "60.80",
                "+0.99"
              ],
              [
                "TDA (Ours)",
                "16min",
                "61.35",
                "+1.54"
              ]
            ],
            "md": "|Method|Testing Time|Accuracy|Gain|\n|---|---|---|---|\n|CLIP-ResNet-50|12min|59.81|0|\n|TPT|12h 50min|60.74|+0.93|\n|DiffTPT|34h 45min|60.80|+0.99|\n|TDA (Ours)|16min|61.35|+1.54|",
            "isPerfectTable": true,
            "csv": "\"Method\",\"Testing Time\",\"Accuracy\",\"Gain\"\n\"CLIP-ResNet-50\",\"12min\",\"59.81\",\"0\"\n\"TPT\",\"12h 50min\",\"60.74\",\"+0.93\"\n\"DiffTPT\",\"34h 45min\",\"60.80\",\"+0.99\"\n\"TDA (Ours)\",\"16min\",\"61.35\",\"+1.54\"",
            "bBox": {
              "x": 58,
              "y": 84,
              "w": 218.11,
              "h": 587.97
            }
          },
          {
            "type": "text",
            "value": "The final column shows the accuracy gain relative to the baseline CLIP. Note that the testing time of DiffTPT does not include the duration required for the image generation process with pre-trained diffusion models, which is an additional time-consuming factor during the testing phase.",
            "md": "The final column shows the accuracy gain relative to the baseline CLIP. Note that the testing time of DiffTPT does not include the duration required for the image generation process with pre-trained diffusion models, which is an additional time-consuming factor during the testing phase.",
            "bBox": {
              "x": 50,
              "y": 152,
              "w": 236.25,
              "h": 519.97
            }
          },
          {
            "type": "heading",
            "lvl": 1,
            "value": "Implementation details.",
            "md": "# Implementation details.",
            "bBox": {
              "x": 50,
              "y": 556,
              "w": 170.52,
              "h": 115.97
            }
          },
          {
            "type": "text",
            "value": "All the models in our experiments are built upon the pre-trained CLIP model [31] that consists of an image encoder and a text encoder. The image encoder can be either ResNet [13] or Vision Transformer [7], while the text encoder is Transformer [39]. Test-time adaptation is set for single-image scenarios, using a batch size of 1. We conduct a search for all our hyperparameters using a single ImageNet validation set. The threshold pl for negative pseudo-labeling in Eq 4 is set as 0.03. The upper and lower thresholds [τ, τh] for testing feature selection in Eq 5 are set as [0.2, 0.5]. Once searched, these hyperparameters are fixed and evaluated across various new datasets. To avoid incurring backpropagation when using learnable prompt, we follow [31] to use hand-crafted prompts. We use top-1 accuracy (%), a standard classification criterion, as our evaluation metric. All the experiments are conducted using a single NVIDIA Quadro RTX 6000 GPU.",
            "md": "All the models in our experiments are built upon the pre-trained CLIP model [31] that consists of an image encoder and a text encoder. The image encoder can be either ResNet [13] or Vision Transformer [7], while the text encoder is Transformer [39]. Test-time adaptation is set for single-image scenarios, using a batch size of 1. We conduct a search for all our hyperparameters using a single ImageNet validation set. The threshold pl for negative pseudo-labeling in Eq 4 is set as 0.03. The upper and lower thresholds [τ, τh] for testing feature selection in Eq 5 are set as [0.2, 0.5]. Once searched, these hyperparameters are fixed and evaluated across various new datasets. To avoid incurring backpropagation when using learnable prompt, we follow [31] to use hand-crafted prompts. We use top-1 accuracy (%), a standard classification criterion, as our evaluation metric. All the experiments are conducted using a single NVIDIA Quadro RTX 6000 GPU.",
            "bBox": {
              "x": 50,
              "y": 84,
              "w": 494.25,
              "h": 637.96
            }
          },
          {
            "type": "heading",
            "lvl": 1,
            "value": "4.2. Comparisons with State-of-the-art",
            "md": "# 4.2. Comparisons with State-of-the-art",
            "bBox": {
              "x": 308,
              "y": 431,
              "w": 179.88,
              "h": 10.96
            }
          },
          {
            "type": "text",
            "value": "In this section, we compare our proposed TDA with several state-of-the-art methods, including CLIP [31], three train-time adaptation methods, i.e., CoOp [51], CoCoOp [50], and Tip-Adapter [48], as well as two existing test-time adaptation methods TPT [35] and DiffTPT [9], all of which are designed for vision-language models. Specifically, CLIP is evaluated using an ensemble of 80 hand-crafted prompts as in [31]. All train-time adaptation methods are trained on the ImageNet train set with 16 shots per class and tested on other datasets as in [35]. CoOp utilizes a learnable context module of size 4 that is fine-tuned for specific downstream tasks, while CoCoOp is the generalized version of CoOp with the input-conditional token for image features. Tip-Adapter employs the optimal hyperparameters obtained from the ImageNet validation set during evaluation. We would like to note that Tip-Adapter is unable to handle new classes during testing, limiting its implementation to OOD benchmark evaluation where the training classes encompass all the testing classes. Different from train-time adaptation methods, test-time adaptation methods (i.e., TPT, DiffTPT, and our TDA) do not utilize the ImageNet train set. Instead, they are fine-tuned with target datasets using a stream of unlabeled test samples.",
            "md": "In this section, we compare our proposed TDA with several state-of-the-art methods, including CLIP [31], three train-time adaptation methods, i.e., CoOp [51], CoCoOp [50], and Tip-Adapter [48], as well as two existing test-time adaptation methods TPT [35] and DiffTPT [9], all of which are designed for vision-language models. Specifically, CLIP is evaluated using an ensemble of 80 hand-crafted prompts as in [31]. All train-time adaptation methods are trained on the ImageNet train set with 16 shots per class and tested on other datasets as in [35]. CoOp utilizes a learnable context module of size 4 that is fine-tuned for specific downstream tasks, while CoCoOp is the generalized version of CoOp with the input-conditional token for image features. Tip-Adapter employs the optimal hyperparameters obtained from the ImageNet validation set during evaluation. We would like to note that Tip-Adapter is unable to handle new classes during testing, limiting its implementation to OOD benchmark evaluation where the training classes encompass all the testing classes. Different from train-time adaptation methods, test-time adaptation methods (i.e., TPT, DiffTPT, and our TDA) do not utilize the ImageNet train set. Instead, they are fine-tuned with target datasets using a stream of unlabeled test samples.",
            "bBox": {
              "x": 58,
              "y": 84,
              "w": 486.61,
              "h": 625.96
            }
          }
        ],
        "status": "OK",
        "links": [
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          }
        ],
        "width": 612,
        "height": 792,
        "triggeredAutoMode": false,
        "structuredData": null,
        "noStructuredContent": false,
        "noTextContent": false
      },
      {
        "page": 7,
        "text": "                  Method                Aircraft    Caltech101      Cars      DTD      EuroSAT      Flower102      Food101       Pets     SUN397       UCF101   Average\n                  CLIP-ResNet-50         16.11         87.26        55.89    40.37       25.79         62.77         74.82       82.97      60.85        59.48   56.63\n                  CoOp                   15.12         86.53        55.32    37.29       26.20         61.55         75.59       87.00      58.15        59.05   56.18\n                  CoCoOp                 14.61         87.38        56.22    38.53       28.73         65.57         76.20       88.39      59.61        57.10   57.23\n                  TPT                    17.58         87.02        58.46    40.84       28.33         62.69         74.88       84.49      61.46        60.82   57.66\n                  DiffTPT                17.60         86.89        60.71    40.72       41.04         63.53         79.21       83.40      62.72        62.67   59.85\n                  TDA (Ours)             17.61         89.70        57.78    43.74       42.11         68.74         77.75       86.18      62.53        64.18   61.03\n                  CLIP-ViT-B/16          23.22         93.55        66.11    45.04       50.42         66.99         82.86       86.92      65.63        65.16   64.59\n                  CoOp                   18.47         93.70        64.51    41.92       46.39         68.71         85.30       89.14      64.15        66.55   63.88\n                  CoCoOp                 22.29         93.79        64.90    45.45       39.23         70.85         83.97       90.46      66.89        68.44   64.63\n                  TPT                    24.78         94.16        66.87    47.75       42.44         68.98         84.67       87.79      65.50        68.04   65.10\n                  DiffTPT                25.60         92.49        67.01    47.00       43.13         70.10         87.23       88.22      65.74        62.67   65.47\n                  TDA (Ours)             23.91         94.24        67.28    47.40       58.00         71.42         86.14       88.63      67.62        70.66   67.53\n\n                Table 3. Results on the Cross-Domain Benchmark. Our TDA is compared with several state-of-the-art methods designed for vision-\n                language models: the baseline method CLIP, two train-time adaptation methods (i.e., CoOp and CoCoOp), and two test-time adaptation\n                methods (i.e., TPT and DiffTPT). Note that Tip-Adapter is unable to be evaluated on the Cross-Domain Benchmark as it cannot handle\n                new classes during testing. The evaluation metric Average is calculated by taking the mean accuracy across all ten datasets. The results of\n                CLIP, CoOp, CoCoOp, and TPT are obtained from the TPT paper, while the results of DiffTPT are obtained from the DiffTPT paper.\n\ning TPT and DiffTPT, we compare TDA with the state-of-\nthe-art over two public benchmarks: OOD benchmark and\ncross-domain benchmark.\n\nResults on the OOD Benchmark.                  We first compare TDA\nwith state-of-the-art methods over the OOD benchmark.\nTable 1 presents the experimental results, highlighting the\nsuperior performance of the proposed TDA compared to\nboth TPT and DiffTPT across various OOD datasets de-\nrived from ImageNet. Specifically, TDA outperforms TPT\non both ResNet-50 and ViT-B/16 architectures, improving\nOOD accuracy by 2.74% and 3.08% on average, respec-\ntively. Furthermore, compared to DiffTPT, TDA exhibits an\naverage accuracy improvement of 0.94% and 3.37% in the\nOOD benchmark for ResNet-50 and ViT-B/16, respectively.\nThese results validate the effectiveness of TDA in enhanc-\ning test-time adaptation performance on various OOD test\ndatasets.\n    In order to provide a more comprehensive evaluation\nof our proposed method’s efficiency and effectiveness, we\ncompared it with the baseline CLIP-ResNet-50 and two ex-\nisting test-time adaptation methods (i.e., TPT and DiffTPT).\nThis comparison encompasses both testing time and test-\ning accuracy, and the corresponding results are shown in\nTable 2. This evaluation is performed on the ImageNet\nvalidation dataset, which consists of 50,000 images, us-\ning a single NVIDIA Quadro RTX 6000 GPU. When com-\npared to the baseline CLIP-ResNet-50, the proposed TDA\ndemonstrates a significant improvement in testing accuracy\n(+1.54%), with only a minimal sacrifice in testing efficiency\n(requiring an additional 4min). In comparison to TPT and\nDiffTPT, the proposed TDA demonstrates not only superior\ntesting accuracy but also significantly improved efficiency.\nIt reduces the testing time dramatically from 12h 50min by\nTPT and even more from 34h 45min by DiffTPT, down to\njust 16 minutes. Without including the image generation\ntime, DiffTPT consumes clearly more test time than TPT\nas it involves a time-consuming multi-step prompt updat-\ning process whereas TPT requires a single step only. The\nexperimental results strongly validate the effectiveness and\nefficiency of our proposed method, establishing its suitabil-\nity for real-world applications.\n    We then compare TDA with state-of-the-art methods\nover cross-domain benchmark. The results, presented in\nTable 3, demonstrate that TDA not only surpasses the per-\nformance of the TPT method but also shows a significant\nadvantage over its improvement method DiffTPT. Specifi-\ncally, when utilizing CLIP-ResNet-50 and CLIP-ViT-B/16\nas the backbone, TDA achieves an improvement in aver-\nage accuracy over TPT by 3.37% and 2.43%, respectively.\nThese improvements, along with a 1.18% and 2.06% gain\nover DiffTPT for the respective backbones, further ver-\nify the effectiveness of TDA in adapting to diverse class\ndatasets during test time. This attribute holds significant\nvalue for vision-language models such as CLIP, as it en-\nables them to classify arbitrary classes in image classifica-\ntion without the need for additional training.\n4.3. Ablation Studies\n\nIn this section, we perform ablation studies to examine\nthe effectiveness of our designs. All the ablation studies\nare conducted over the ImageNet dataset, where TDA can\nachieve an accuracy of 61.35% under default settings. TDA\nconsists of a Positive Cache and a Negative Cache, which",
        "md": "# Results on the Cross-Domain Benchmark\n\n|Method|Aircraft|Caltech101|Cars|DTD|EuroSAT|Flower102|Food101|Pets|SUN397|UCF101|Average|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|CLIP-ResNet-50|16.11|87.26|55.89|40.37|25.79|62.77|74.82|82.97|60.85|59.48|56.63|\n|CoOp|15.12|86.53|55.32|37.29|26.20|61.55|75.59|87.00|58.15|59.05|56.18|\n|CoCoOp|14.61|87.38|56.22|38.53|28.73|65.57|76.20|88.39|59.61|57.10|57.23|\n|TPT|17.58|87.02|58.46|40.84|28.33|62.69|74.88|84.49|61.46|60.82|57.66|\n|DiffTPT|17.60|86.89|60.71|40.72|41.04|63.53|79.21|83.40|62.72|62.67|59.85|\n|TDA (Ours)|17.61|89.70|57.78|43.74|42.11|68.74|77.75|86.18|62.53|64.18|61.03|\n|CLIP-ViT-B/16|23.22|93.55|66.11|45.04|50.42|66.99|82.86|86.92|65.63|65.16|64.59|\n|CoOp|18.47|93.70|64.51|41.92|46.39|68.71|85.30|89.14|64.15|66.55|63.88|\n|CoCoOp|22.29|93.79|64.90|45.45|39.23|70.85|83.97|90.46|66.89|68.44|64.63|\n|TPT|24.78|94.16|66.87|47.75|42.44|68.98|84.67|87.79|65.50|68.04|65.10|\n|DiffTPT|25.60|92.49|67.01|47.00|43.13|70.10|87.23|88.22|65.74|62.67|65.47|\n|TDA (Ours)|23.91|94.24|67.28|47.40|58.00|71.42|86.14|88.63|67.62|70.66|67.53|\n\nTable 3. Results on the Cross-Domain Benchmark. Our TDA is compared with several state-of-the-art methods designed for vision-language models: the baseline method CLIP, two train-time adaptation methods (i.e., CoOp and CoCoOp), and two test-time adaptation methods (i.e., TPT and DiffTPT). Note that Tip-Adapter is unable to be evaluated on the Cross-Domain Benchmark as it cannot handle new classes during testing. The evaluation metric Average is calculated by taking the mean accuracy across all ten datasets. The results of CLIP, CoOp, CoCoOp, and TPT are obtained from the TPT paper, while the results of DiffTPT are obtained from the DiffTPT paper.\n\n# Results on the OOD Benchmark\n\nWe first compare TDA with state-of-the-art methods over the OOD benchmark. Table 1 presents the experimental results, highlighting the superior performance of the proposed TDA compared to both TPT and DiffTPT across various OOD datasets derived from ImageNet. Specifically, TDA outperforms TPT on both ResNet-50 and ViT-B/16 architectures, improving OOD accuracy by 2.74% and 3.08% on average, respectively. Furthermore, compared to DiffTPT, TDA exhibits an average accuracy improvement of 0.94% and 3.37% in the OOD benchmark for ResNet-50 and ViT-B/16, respectively. These results validate the effectiveness of TDA in enhancing test-time adaptation performance on various OOD test datasets.\n\nIn order to provide a more comprehensive evaluation of our proposed method’s efficiency and effectiveness, we compared it with the baseline CLIP-ResNet-50 and two existing test-time adaptation methods (i.e., TPT and DiffTPT). This comparison encompasses both testing time and testing accuracy, and the corresponding results are shown in Table 2. This evaluation is performed on the ImageNet validation dataset, which consists of 50,000 images, using a single NVIDIA Quadro RTX 6000 GPU. When compared to the baseline CLIP-ResNet-50, the proposed TDA demonstrates a significant improvement in testing accuracy (+1.54%), with only a minimal sacrifice in testing efficiency (requiring an additional 4min). In comparison to TPT and DiffTPT, the proposed TDA demonstrates not only superior testing accuracy but also significantly improved efficiency. It reduces the testing time dramatically from 12h 50min by TPT and even more from 34h 45min by DiffTPT, down to just 16 minutes. Without including the image generation time, DiffTPT consumes clearly more test time than TPT as it involves a time-consuming multi-step prompt updating process whereas TPT requires a single step only. The experimental results strongly validate the effectiveness and efficiency of our proposed method, establishing its suitability for real-world applications.\n\nWe then compare TDA with state-of-the-art methods over cross-domain benchmark. The results, presented in Table 3, demonstrate that TDA not only surpasses the performance of the TPT method but also shows a significant advantage over its improvement method DiffTPT. Specifically, when utilizing CLIP-ResNet-50 and CLIP-ViT-B/16 as the backbone, TDA achieves an improvement in average accuracy over TPT by 3.37% and 2.43%, respectively. These improvements, along with a 1.18% and 2.06% gain over DiffTPT for the respective backbones, further verify the effectiveness of TDA in adapting to diverse class datasets during test time. This attribute holds significant value for vision-language models such as CLIP, as it enables them to classify arbitrary classes in image classification without the need for additional training.\n\n# Ablation Studies\n\nIn this section, we perform ablation studies to examine the effectiveness of our designs. All the ablation studies are conducted over the ImageNet dataset, where TDA can achieve an accuracy of 61.35% under default settings. TDA consists of a Positive Cache and a Negative Cache, which...",
        "images": [],
        "items": [
          {
            "type": "heading",
            "lvl": 1,
            "value": "Results on the Cross-Domain Benchmark",
            "md": "# Results on the Cross-Domain Benchmark",
            "bBox": {
              "x": 0,
              "y": 0,
              "w": 612,
              "h": 792
            }
          },
          {
            "type": "table",
            "rows": [
              [
                "Method",
                "Aircraft",
                "Caltech101",
                "Cars",
                "DTD",
                "EuroSAT",
                "Flower102",
                "Food101",
                "Pets",
                "SUN397",
                "UCF101",
                "Average"
              ],
              [
                "CLIP-ResNet-50",
                "16.11",
                "87.26",
                "55.89",
                "40.37",
                "25.79",
                "62.77",
                "74.82",
                "82.97",
                "60.85",
                "59.48",
                "56.63"
              ],
              [
                "CoOp",
                "15.12",
                "86.53",
                "55.32",
                "37.29",
                "26.20",
                "61.55",
                "75.59",
                "87.00",
                "58.15",
                "59.05",
                "56.18"
              ],
              [
                "CoCoOp",
                "14.61",
                "87.38",
                "56.22",
                "38.53",
                "28.73",
                "65.57",
                "76.20",
                "88.39",
                "59.61",
                "57.10",
                "57.23"
              ],
              [
                "TPT",
                "17.58",
                "87.02",
                "58.46",
                "40.84",
                "28.33",
                "62.69",
                "74.88",
                "84.49",
                "61.46",
                "60.82",
                "57.66"
              ],
              [
                "DiffTPT",
                "17.60",
                "86.89",
                "60.71",
                "40.72",
                "41.04",
                "63.53",
                "79.21",
                "83.40",
                "62.72",
                "62.67",
                "59.85"
              ],
              [
                "TDA (Ours)",
                "17.61",
                "89.70",
                "57.78",
                "43.74",
                "42.11",
                "68.74",
                "77.75",
                "86.18",
                "62.53",
                "64.18",
                "61.03"
              ],
              [
                "CLIP-ViT-B/16",
                "23.22",
                "93.55",
                "66.11",
                "45.04",
                "50.42",
                "66.99",
                "82.86",
                "86.92",
                "65.63",
                "65.16",
                "64.59"
              ],
              [
                "CoOp",
                "18.47",
                "93.70",
                "64.51",
                "41.92",
                "46.39",
                "68.71",
                "85.30",
                "89.14",
                "64.15",
                "66.55",
                "63.88"
              ],
              [
                "CoCoOp",
                "22.29",
                "93.79",
                "64.90",
                "45.45",
                "39.23",
                "70.85",
                "83.97",
                "90.46",
                "66.89",
                "68.44",
                "64.63"
              ],
              [
                "TPT",
                "24.78",
                "94.16",
                "66.87",
                "47.75",
                "42.44",
                "68.98",
                "84.67",
                "87.79",
                "65.50",
                "68.04",
                "65.10"
              ],
              [
                "DiffTPT",
                "25.60",
                "92.49",
                "67.01",
                "47.00",
                "43.13",
                "70.10",
                "87.23",
                "88.22",
                "65.74",
                "62.67",
                "65.47"
              ],
              [
                "TDA (Ours)",
                "23.91",
                "94.24",
                "67.28",
                "47.40",
                "58.00",
                "71.42",
                "86.14",
                "88.63",
                "67.62",
                "70.66",
                "67.53"
              ]
            ],
            "md": "|Method|Aircraft|Caltech101|Cars|DTD|EuroSAT|Flower102|Food101|Pets|SUN397|UCF101|Average|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|CLIP-ResNet-50|16.11|87.26|55.89|40.37|25.79|62.77|74.82|82.97|60.85|59.48|56.63|\n|CoOp|15.12|86.53|55.32|37.29|26.20|61.55|75.59|87.00|58.15|59.05|56.18|\n|CoCoOp|14.61|87.38|56.22|38.53|28.73|65.57|76.20|88.39|59.61|57.10|57.23|\n|TPT|17.58|87.02|58.46|40.84|28.33|62.69|74.88|84.49|61.46|60.82|57.66|\n|DiffTPT|17.60|86.89|60.71|40.72|41.04|63.53|79.21|83.40|62.72|62.67|59.85|\n|TDA (Ours)|17.61|89.70|57.78|43.74|42.11|68.74|77.75|86.18|62.53|64.18|61.03|\n|CLIP-ViT-B/16|23.22|93.55|66.11|45.04|50.42|66.99|82.86|86.92|65.63|65.16|64.59|\n|CoOp|18.47|93.70|64.51|41.92|46.39|68.71|85.30|89.14|64.15|66.55|63.88|\n|CoCoOp|22.29|93.79|64.90|45.45|39.23|70.85|83.97|90.46|66.89|68.44|64.63|\n|TPT|24.78|94.16|66.87|47.75|42.44|68.98|84.67|87.79|65.50|68.04|65.10|\n|DiffTPT|25.60|92.49|67.01|47.00|43.13|70.10|87.23|88.22|65.74|62.67|65.47|\n|TDA (Ours)|23.91|94.24|67.28|47.40|58.00|71.42|86.14|88.63|67.62|70.66|67.53|",
            "isPerfectTable": true,
            "csv": "\"Method\",\"Aircraft\",\"Caltech101\",\"Cars\",\"DTD\",\"EuroSAT\",\"Flower102\",\"Food101\",\"Pets\",\"SUN397\",\"UCF101\",\"Average\"\n\"CLIP-ResNet-50\",\"16.11\",\"87.26\",\"55.89\",\"40.37\",\"25.79\",\"62.77\",\"74.82\",\"82.97\",\"60.85\",\"59.48\",\"56.63\"\n\"CoOp\",\"15.12\",\"86.53\",\"55.32\",\"37.29\",\"26.20\",\"61.55\",\"75.59\",\"87.00\",\"58.15\",\"59.05\",\"56.18\"\n\"CoCoOp\",\"14.61\",\"87.38\",\"56.22\",\"38.53\",\"28.73\",\"65.57\",\"76.20\",\"88.39\",\"59.61\",\"57.10\",\"57.23\"\n\"TPT\",\"17.58\",\"87.02\",\"58.46\",\"40.84\",\"28.33\",\"62.69\",\"74.88\",\"84.49\",\"61.46\",\"60.82\",\"57.66\"\n\"DiffTPT\",\"17.60\",\"86.89\",\"60.71\",\"40.72\",\"41.04\",\"63.53\",\"79.21\",\"83.40\",\"62.72\",\"62.67\",\"59.85\"\n\"TDA (Ours)\",\"17.61\",\"89.70\",\"57.78\",\"43.74\",\"42.11\",\"68.74\",\"77.75\",\"86.18\",\"62.53\",\"64.18\",\"61.03\"\n\"CLIP-ViT-B/16\",\"23.22\",\"93.55\",\"66.11\",\"45.04\",\"50.42\",\"66.99\",\"82.86\",\"86.92\",\"65.63\",\"65.16\",\"64.59\"\n\"CoOp\",\"18.47\",\"93.70\",\"64.51\",\"41.92\",\"46.39\",\"68.71\",\"85.30\",\"89.14\",\"64.15\",\"66.55\",\"63.88\"\n\"CoCoOp\",\"22.29\",\"93.79\",\"64.90\",\"45.45\",\"39.23\",\"70.85\",\"83.97\",\"90.46\",\"66.89\",\"68.44\",\"64.63\"\n\"TPT\",\"24.78\",\"94.16\",\"66.87\",\"47.75\",\"42.44\",\"68.98\",\"84.67\",\"87.79\",\"65.50\",\"68.04\",\"65.10\"\n\"DiffTPT\",\"25.60\",\"92.49\",\"67.01\",\"47.00\",\"43.13\",\"70.10\",\"87.23\",\"88.22\",\"65.74\",\"62.67\",\"65.47\"\n\"TDA (Ours)\",\"23.91\",\"94.24\",\"67.28\",\"47.40\",\"58.00\",\"71.42\",\"86.14\",\"88.63\",\"67.62\",\"70.66\",\"67.53\"",
            "bBox": {
              "x": 57,
              "y": 82,
              "w": 480.17,
              "h": 153.54
            }
          },
          {
            "type": "text",
            "value": "Table 3. Results on the Cross-Domain Benchmark. Our TDA is compared with several state-of-the-art methods designed for vision-language models: the baseline method CLIP, two train-time adaptation methods (i.e., CoOp and CoCoOp), and two test-time adaptation methods (i.e., TPT and DiffTPT). Note that Tip-Adapter is unable to be evaluated on the Cross-Domain Benchmark as it cannot handle new classes during testing. The evaluation metric Average is calculated by taking the mean accuracy across all ten datasets. The results of CLIP, CoOp, CoCoOp, and TPT are obtained from the TPT paper, while the results of DiffTPT are obtained from the DiffTPT paper.",
            "md": "Table 3. Results on the Cross-Domain Benchmark. Our TDA is compared with several state-of-the-art methods designed for vision-language models: the baseline method CLIP, two train-time adaptation methods (i.e., CoOp and CoCoOp), and two test-time adaptation methods (i.e., TPT and DiffTPT). Note that Tip-Adapter is unable to be evaluated on the Cross-Domain Benchmark as it cannot handle new classes during testing. The evaluation metric Average is calculated by taking the mean accuracy across all ten datasets. The results of CLIP, CoOp, CoCoOp, and TPT are obtained from the TPT paper, while the results of DiffTPT are obtained from the DiffTPT paper.",
            "bBox": {
              "x": 50,
              "y": 82,
              "w": 495.03,
              "h": 471.96
            }
          },
          {
            "type": "heading",
            "lvl": 1,
            "value": "Results on the OOD Benchmark",
            "md": "# Results on the OOD Benchmark",
            "bBox": {
              "x": 0,
              "y": 0,
              "w": 612,
              "h": 792
            }
          },
          {
            "type": "text",
            "value": "We first compare TDA with state-of-the-art methods over the OOD benchmark. Table 1 presents the experimental results, highlighting the superior performance of the proposed TDA compared to both TPT and DiffTPT across various OOD datasets derived from ImageNet. Specifically, TDA outperforms TPT on both ResNet-50 and ViT-B/16 architectures, improving OOD accuracy by 2.74% and 3.08% on average, respectively. Furthermore, compared to DiffTPT, TDA exhibits an average accuracy improvement of 0.94% and 3.37% in the OOD benchmark for ResNet-50 and ViT-B/16, respectively. These results validate the effectiveness of TDA in enhancing test-time adaptation performance on various OOD test datasets.\n\nIn order to provide a more comprehensive evaluation of our proposed method’s efficiency and effectiveness, we compared it with the baseline CLIP-ResNet-50 and two existing test-time adaptation methods (i.e., TPT and DiffTPT). This comparison encompasses both testing time and testing accuracy, and the corresponding results are shown in Table 2. This evaluation is performed on the ImageNet validation dataset, which consists of 50,000 images, using a single NVIDIA Quadro RTX 6000 GPU. When compared to the baseline CLIP-ResNet-50, the proposed TDA demonstrates a significant improvement in testing accuracy (+1.54%), with only a minimal sacrifice in testing efficiency (requiring an additional 4min). In comparison to TPT and DiffTPT, the proposed TDA demonstrates not only superior testing accuracy but also significantly improved efficiency. It reduces the testing time dramatically from 12h 50min by TPT and even more from 34h 45min by DiffTPT, down to just 16 minutes. Without including the image generation time, DiffTPT consumes clearly more test time than TPT as it involves a time-consuming multi-step prompt updating process whereas TPT requires a single step only. The experimental results strongly validate the effectiveness and efficiency of our proposed method, establishing its suitability for real-world applications.\n\nWe then compare TDA with state-of-the-art methods over cross-domain benchmark. The results, presented in Table 3, demonstrate that TDA not only surpasses the performance of the TPT method but also shows a significant advantage over its improvement method DiffTPT. Specifically, when utilizing CLIP-ResNet-50 and CLIP-ViT-B/16 as the backbone, TDA achieves an improvement in average accuracy over TPT by 3.37% and 2.43%, respectively. These improvements, along with a 1.18% and 2.06% gain over DiffTPT for the respective backbones, further verify the effectiveness of TDA in adapting to diverse class datasets during test time. This attribute holds significant value for vision-language models such as CLIP, as it enables them to classify arbitrary classes in image classification without the need for additional training.",
            "md": "We first compare TDA with state-of-the-art methods over the OOD benchmark. Table 1 presents the experimental results, highlighting the superior performance of the proposed TDA compared to both TPT and DiffTPT across various OOD datasets derived from ImageNet. Specifically, TDA outperforms TPT on both ResNet-50 and ViT-B/16 architectures, improving OOD accuracy by 2.74% and 3.08% on average, respectively. Furthermore, compared to DiffTPT, TDA exhibits an average accuracy improvement of 0.94% and 3.37% in the OOD benchmark for ResNet-50 and ViT-B/16, respectively. These results validate the effectiveness of TDA in enhancing test-time adaptation performance on various OOD test datasets.\n\nIn order to provide a more comprehensive evaluation of our proposed method’s efficiency and effectiveness, we compared it with the baseline CLIP-ResNet-50 and two existing test-time adaptation methods (i.e., TPT and DiffTPT). This comparison encompasses both testing time and testing accuracy, and the corresponding results are shown in Table 2. This evaluation is performed on the ImageNet validation dataset, which consists of 50,000 images, using a single NVIDIA Quadro RTX 6000 GPU. When compared to the baseline CLIP-ResNet-50, the proposed TDA demonstrates a significant improvement in testing accuracy (+1.54%), with only a minimal sacrifice in testing efficiency (requiring an additional 4min). In comparison to TPT and DiffTPT, the proposed TDA demonstrates not only superior testing accuracy but also significantly improved efficiency. It reduces the testing time dramatically from 12h 50min by TPT and even more from 34h 45min by DiffTPT, down to just 16 minutes. Without including the image generation time, DiffTPT consumes clearly more test time than TPT as it involves a time-consuming multi-step prompt updating process whereas TPT requires a single step only. The experimental results strongly validate the effectiveness and efficiency of our proposed method, establishing its suitability for real-world applications.\n\nWe then compare TDA with state-of-the-art methods over cross-domain benchmark. The results, presented in Table 3, demonstrate that TDA not only surpasses the performance of the TPT method but also shows a significant advantage over its improvement method DiffTPT. Specifically, when utilizing CLIP-ResNet-50 and CLIP-ViT-B/16 as the backbone, TDA achieves an improvement in average accuracy over TPT by 3.37% and 2.43%, respectively. These improvements, along with a 1.18% and 2.06% gain over DiffTPT for the respective backbones, further verify the effectiveness of TDA in adapting to diverse class datasets during test time. This attribute holds significant value for vision-language models such as CLIP, as it enables them to classify arbitrary classes in image classification without the need for additional training.",
            "bBox": {
              "x": 50,
              "y": 82,
              "w": 494.57,
              "h": 638.96
            }
          },
          {
            "type": "heading",
            "lvl": 1,
            "value": "Ablation Studies",
            "md": "# Ablation Studies",
            "bBox": {
              "x": 0,
              "y": 0,
              "w": 612,
              "h": 792
            }
          },
          {
            "type": "text",
            "value": "In this section, we perform ablation studies to examine the effectiveness of our designs. All the ablation studies are conducted over the ImageNet dataset, where TDA can achieve an accuracy of 61.35% under default settings. TDA consists of a Positive Cache and a Negative Cache, which...",
            "md": "In this section, we perform ablation studies to examine the effectiveness of our designs. All the ablation studies are conducted over the ImageNet dataset, where TDA can achieve an accuracy of 61.35% under default settings. TDA consists of a Positive Cache and a Negative Cache, which...",
            "bBox": {
              "x": 308,
              "y": 664,
              "w": 236.28,
              "h": 56.96
            }
          }
        ],
        "status": "OK",
        "links": [
          {
            "text": "superior performance of the proposed TDA compared to"
          },
          {
            "text": "validation dataset, which consists of 50,000 images, us-"
          },
          {
            "text": "formance of the TPT method but also shows a significant"
          }
        ],
        "width": 612,
        "height": 792,
        "triggeredAutoMode": false,
        "structuredData": null,
        "noStructuredContent": false,
        "noTextContent": false
      },
      {
        "page": 8,
        "text": "Accuracy (%)\n\n                                                                               61.5                                                                                                              ImageNet                                                                                                                    ImageNet\n                                                                                                                                                                                                                        61.35                  61.0\n\n                                                                               61.0\n                                                                                                                                                                                          60.82                  60.83                         60.5\n\n                                                                               60.5                                                                                                                                                            60.0\n\n                                                                               60.0                                                                                                                                                            59.5\n                                                                                                                         59.81\n                                                                                                                                                                                                                                                                                                  Positive Cache\n                                                                                                                                                                                                                                               59.0                                               Negative Cache\n                                                                               59.5                                      CLIP                                                       Positive               Negative       TDA                                                          1                          2         3          4  5  6\n                                                                                                               ResNet-50                                                                Cache                   Cache                                                                                                Shot capacity\n                                                              Figure 3. Ablation studies on two cache designs in TDA: Posi-                                                                                                    Figure 4. Parameter studies on the Shot Capacity in Positive Cache\n                                                              tive Cache and Negative Cache. All the models are built upon the                                                                                                 and Negative Cache.\n                                                              baseline model CLIP-ResNet-50.\n\n\ndesigned dynamic adapter, respectively. We first assess the\nefficacy of the two cache designs in TDA. As shown in\nFigure 3, both Positive Cache and Negative Cache signifi-\ncantly surpass the baseline model CLIP, demonstrating that\ntest-time adaptation can be improved by introducing a dy-\nnamic adapter with either positive pseudo labeling or neg-\native pseudo labeling. Besides, the two cache designs in\nTDA can complement each other as TDA (i.e., the combi-\nnation of the two designs) clearly outperforms either Posi-\ntive Cache or Negative Cache on the challenging ImageNet\ndataset. Moreover, we extend our ablation studies to the\ncross-domain benchmark, and results show that the posi-\ntive cache achieved 60.38% accuracy, the negative cache\n60.11%, while their combination yielded a 61.03% accu-\nracy, thereby highlighting the significance of each type of\ncache in enhancing TDA’s performance.\n               We proceed to perform parameter studies on the shot ca-\npacity, which refers to the maximum number of key-value\npairs per class, in both Positive Cache and Negative Cache\nmodels. These studies aims to find the optimal balance be-\ntween the diversity and accuracy of the key-value pairs. Fig-\nure 4 shows that the performance of both cache models is\nsignificantly affected when the shot capacity is either too\nlow or too high. We find that the shot capacity is set as 3 for\nthe Positive Cache and 2 for the Negative Cache yields the\nbest performance. This is because an appropriate shot ca-\npacity ensures both high-quality pseudo labels (paired val-\nues) and diverse image features (paired keys) in the cache\nmodels. The negative cache is sensitive to shot numbers\ndue to its role in storing probabilities for various negative\npseudo labels, each representing a class to be excluded from\nthe model. Contrary to the intuition that a larger negative\ncache might be beneficial, a larger negative cache leads toAccuracy (%)perform positive and negative pseudo labeling within ourmore high-entropy, noisier pseudo labels, as highlighted in\nself-training studies like [11], thereby lowering the perfor-\nmance. Conversely, the positive cache, which stores sin-\ngle and high-confidence prediction, is less affected by vari-\nations in shot capacity, thereby maintaining consistent accu-\nracy across different shot capacities. To facilitate practical\napplications, the shot capacity settings are fixed and directly\napplied to new datasets without the need for additional pa-\nrameter adjustments.\n\n5. Conclusion\nIn this work, we have presented TDA, a dynamic adapter\nfor efficient and effective test-time adaptation of vision-\nlanguage models. The proposed method employs a key-\nvalue cache, which maintains a dynamic queue with test-\nsample features as keys and corresponding few-shot pseudo\nlabels as values, allowing for gradual adaptation to test data\nthrough progressive pseudo label improvement. Moreover,\nTDA introduces a negative cache to mitigate the undesirable\neffects of noisy pseudo labels by assigning negative pseudo\nlabels to certain classes when the model is uncertain about\nits predictions. The results of extensive experiments over\ntwo benchmarks demonstrate that TDA outperforms state-\nof-the-art test-time adaptation methods while significantly\nreducing testing time. This work contributes to the research\nand application values of test-time adaptation and presents a\npromising solution to the efficiency issue of test-time adap-\ntation of vision-language models.\n\nAcknowledgement\n\nThis study is supported under the Mohamed bin Zayed Uni-\nversity of Artificial Intelligence.",
        "md": "# Accuracy (%)\n\n| | |CLIP|Positive Cache|Negative Cache|TDA| | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| | | | | | |1|2|3|4|5|6|\n|61.5|61.35|61.0|60.82|60.83|60.5| | | | | | |\n| |60.0| | | |59.5|59.81|59.0| | | | |\n\nFigure 3. Ablation studies on two cache designs in TDA: Positive Cache and Negative Cache. All the models are built upon the baseline model CLIP-ResNet-50.\n\nFigure 4. Parameter studies on the Shot Capacity in Positive Cache and Negative Cache.\n\nWe first assess the efficacy of the two cache designs in TDA. As shown in Figure 3, both Positive Cache and Negative Cache significantly surpass the baseline model CLIP, demonstrating that test-time adaptation can be improved by introducing a dynamic adapter with either positive pseudo labeling or negative pseudo labeling. Besides, the two cache designs in TDA can complement each other as TDA (i.e., the combination of the two designs) clearly outperforms either Positive Cache or Negative Cache on the challenging ImageNet dataset. Moreover, we extend our ablation studies to the cross-domain benchmark, and results show that the positive cache achieved 60.38% accuracy, the negative cache 60.11%, while their combination yielded a 61.03% accuracy, thereby highlighting the significance of each type of cache in enhancing TDA’s performance.\n\nWe proceed to perform parameter studies on the shot capacity, which refers to the maximum number of key-value pairs per class, in both Positive Cache and Negative Cache models. These studies aim to find the optimal balance between the diversity and accuracy of the key-value pairs. Figure 4 shows that the performance of both cache models is significantly affected when the shot capacity is either too low or too high. We find that the shot capacity is set as 3 for the Positive Cache and 2 for the Negative Cache yields the best performance. This is because an appropriate shot capacity ensures both high-quality pseudo labels (paired values) and diverse image features (paired keys) in the cache models. The negative cache is sensitive to shot numbers due to its role in storing probabilities for various negative pseudo labels, each representing a class to be excluded from the model. Contrary to the intuition that a larger negative cache might be beneficial, a larger negative cache leads to more high-entropy, noisier pseudo labels, as highlighted in self-training studies like [11], thereby lowering the performance. Conversely, the positive cache, which stores single and high-confidence prediction, is less affected by variations in shot capacity, thereby maintaining consistent accuracy across different shot capacities. To facilitate practical applications, the shot capacity settings are fixed and directly applied to new datasets without the need for additional parameter adjustments.\n\n# 5. Conclusion\n\nIn this work, we have presented TDA, a dynamic adapter for efficient and effective test-time adaptation of vision-language models. The proposed method employs a key-value cache, which maintains a dynamic queue with test-sample features as keys and corresponding few-shot pseudo labels as values, allowing for gradual adaptation to test data through progressive pseudo label improvement. Moreover, TDA introduces a negative cache to mitigate the undesirable effects of noisy pseudo labels by assigning negative pseudo labels to certain classes when the model is uncertain about its predictions. The results of extensive experiments over two benchmarks demonstrate that TDA outperforms state-of-the-art test-time adaptation methods while significantly reducing testing time. This work contributes to the research and application values of test-time adaptation and presents a promising solution to the efficiency issue of test-time adaptation of vision-language models.\n\n# Acknowledgement\n\nThis study is supported under the Mohamed bin Zayed University of Artificial Intelligence.",
        "images": [],
        "items": [
          {
            "type": "heading",
            "lvl": 1,
            "value": "Accuracy (%)",
            "md": "# Accuracy (%)",
            "bBox": {
              "x": null,
              "y": 59,
              "w": null,
              "h": 267.64
            }
          },
          {
            "type": "table",
            "rows": [
              [
                "",
                "",
                "CLIP",
                "Positive Cache",
                "Negative Cache",
                "TDA",
                "",
                "",
                "",
                "",
                "",
                ""
              ],
              [
                "",
                "",
                "",
                "",
                "",
                "",
                "1",
                "2",
                "3",
                "4",
                "5",
                "6"
              ],
              [
                "61.5",
                "61.35",
                "61.0",
                "60.82",
                "60.83",
                "60.5",
                "",
                "",
                "",
                "",
                "",
                ""
              ],
              [
                "",
                "60.0",
                "",
                "",
                "",
                "59.5",
                "59.81",
                "59.0",
                "",
                "",
                "",
                ""
              ]
            ],
            "md": "| | |CLIP|Positive Cache|Negative Cache|TDA| | | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| | | | | | |1|2|3|4|5|6|\n|61.5|61.35|61.0|60.82|60.83|60.5| | | | | | |\n| |60.0| | | |59.5|59.81|59.0| | | | |",
            "isPerfectTable": true,
            "csv": "\"\",\"\",\"CLIP\",\"Positive Cache\",\"Negative Cache\",\"TDA\",\"\",\"\",\"\",\"\",\"\",\"\"\n\"\",\"\",\"\",\"\",\"\",\"\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\"\n\"61.5\",\"61.35\",\"61.0\",\"60.82\",\"60.83\",\"60.5\",\"\",\"\",\"\",\"\",\"\",\"\"\n\"\",\"60.0\",\"\",\"\",\"\",\"59.5\",\"59.81\",\"59.0\",\"\",\"\",\"\",\"\"",
            "bBox": {
              "x": 63,
              "y": 87,
              "w": 463.64,
              "h": 170.28
            }
          },
          {
            "type": "text",
            "value": "Figure 3. Ablation studies on two cache designs in TDA: Positive Cache and Negative Cache. All the models are built upon the baseline model CLIP-ResNet-50.\n\nFigure 4. Parameter studies on the Shot Capacity in Positive Cache and Negative Cache.\n\nWe first assess the efficacy of the two cache designs in TDA. As shown in Figure 3, both Positive Cache and Negative Cache significantly surpass the baseline model CLIP, demonstrating that test-time adaptation can be improved by introducing a dynamic adapter with either positive pseudo labeling or negative pseudo labeling. Besides, the two cache designs in TDA can complement each other as TDA (i.e., the combination of the two designs) clearly outperforms either Positive Cache or Negative Cache on the challenging ImageNet dataset. Moreover, we extend our ablation studies to the cross-domain benchmark, and results show that the positive cache achieved 60.38% accuracy, the negative cache 60.11%, while their combination yielded a 61.03% accuracy, thereby highlighting the significance of each type of cache in enhancing TDA’s performance.\n\nWe proceed to perform parameter studies on the shot capacity, which refers to the maximum number of key-value pairs per class, in both Positive Cache and Negative Cache models. These studies aim to find the optimal balance between the diversity and accuracy of the key-value pairs. Figure 4 shows that the performance of both cache models is significantly affected when the shot capacity is either too low or too high. We find that the shot capacity is set as 3 for the Positive Cache and 2 for the Negative Cache yields the best performance. This is because an appropriate shot capacity ensures both high-quality pseudo labels (paired values) and diverse image features (paired keys) in the cache models. The negative cache is sensitive to shot numbers due to its role in storing probabilities for various negative pseudo labels, each representing a class to be excluded from the model. Contrary to the intuition that a larger negative cache might be beneficial, a larger negative cache leads to more high-entropy, noisier pseudo labels, as highlighted in self-training studies like [11], thereby lowering the performance. Conversely, the positive cache, which stores single and high-confidence prediction, is less affected by variations in shot capacity, thereby maintaining consistent accuracy across different shot capacities. To facilitate practical applications, the shot capacity settings are fixed and directly applied to new datasets without the need for additional parameter adjustments.",
            "md": "Figure 3. Ablation studies on two cache designs in TDA: Positive Cache and Negative Cache. All the models are built upon the baseline model CLIP-ResNet-50.\n\nFigure 4. Parameter studies on the Shot Capacity in Positive Cache and Negative Cache.\n\nWe first assess the efficacy of the two cache designs in TDA. As shown in Figure 3, both Positive Cache and Negative Cache significantly surpass the baseline model CLIP, demonstrating that test-time adaptation can be improved by introducing a dynamic adapter with either positive pseudo labeling or negative pseudo labeling. Besides, the two cache designs in TDA can complement each other as TDA (i.e., the combination of the two designs) clearly outperforms either Positive Cache or Negative Cache on the challenging ImageNet dataset. Moreover, we extend our ablation studies to the cross-domain benchmark, and results show that the positive cache achieved 60.38% accuracy, the negative cache 60.11%, while their combination yielded a 61.03% accuracy, thereby highlighting the significance of each type of cache in enhancing TDA’s performance.\n\nWe proceed to perform parameter studies on the shot capacity, which refers to the maximum number of key-value pairs per class, in both Positive Cache and Negative Cache models. These studies aim to find the optimal balance between the diversity and accuracy of the key-value pairs. Figure 4 shows that the performance of both cache models is significantly affected when the shot capacity is either too low or too high. We find that the shot capacity is set as 3 for the Positive Cache and 2 for the Negative Cache yields the best performance. This is because an appropriate shot capacity ensures both high-quality pseudo labels (paired values) and diverse image features (paired keys) in the cache models. The negative cache is sensitive to shot numbers due to its role in storing probabilities for various negative pseudo labels, each representing a class to be excluded from the model. Contrary to the intuition that a larger negative cache might be beneficial, a larger negative cache leads to more high-entropy, noisier pseudo labels, as highlighted in self-training studies like [11], thereby lowering the performance. Conversely, the positive cache, which stores single and high-confidence prediction, is less affected by variations in shot capacity, thereby maintaining consistent accuracy across different shot capacities. To facilitate practical applications, the shot capacity settings are fixed and directly applied to new datasets without the need for additional parameter adjustments.",
            "bBox": {
              "x": 50,
              "y": 82,
              "w": 494.75,
              "h": 638.96
            }
          },
          {
            "type": "heading",
            "lvl": 1,
            "value": "5. Conclusion",
            "md": "# 5. Conclusion",
            "bBox": {
              "x": 308,
              "y": 242,
              "w": 184.64,
              "h": 215.96
            }
          },
          {
            "type": "text",
            "value": "In this work, we have presented TDA, a dynamic adapter for efficient and effective test-time adaptation of vision-language models. The proposed method employs a key-value cache, which maintains a dynamic queue with test-sample features as keys and corresponding few-shot pseudo labels as values, allowing for gradual adaptation to test data through progressive pseudo label improvement. Moreover, TDA introduces a negative cache to mitigate the undesirable effects of noisy pseudo labels by assigning negative pseudo labels to certain classes when the model is uncertain about its predictions. The results of extensive experiments over two benchmarks demonstrate that TDA outperforms state-of-the-art test-time adaptation methods while significantly reducing testing time. This work contributes to the research and application values of test-time adaptation and presents a promising solution to the efficiency issue of test-time adaptation of vision-language models.",
            "md": "In this work, we have presented TDA, a dynamic adapter for efficient and effective test-time adaptation of vision-language models. The proposed method employs a key-value cache, which maintains a dynamic queue with test-sample features as keys and corresponding few-shot pseudo labels as values, allowing for gradual adaptation to test data through progressive pseudo label improvement. Moreover, TDA introduces a negative cache to mitigate the undesirable effects of noisy pseudo labels by assigning negative pseudo labels to certain classes when the model is uncertain about its predictions. The results of extensive experiments over two benchmarks demonstrate that TDA outperforms state-of-the-art test-time adaptation methods while significantly reducing testing time. This work contributes to the research and application values of test-time adaptation and presents a promising solution to the efficiency issue of test-time adaptation of vision-language models.",
            "bBox": {
              "x": 147,
              "y": 226,
              "w": 397.42,
              "h": 439.96
            }
          },
          {
            "type": "heading",
            "lvl": 1,
            "value": "Acknowledgement",
            "md": "# Acknowledgement",
            "bBox": {
              "x": 308,
              "y": 680,
              "w": 94.18,
              "h": 11.96
            }
          },
          {
            "type": "text",
            "value": "This study is supported under the Mohamed bin Zayed University of Artificial Intelligence.",
            "md": "This study is supported under the Mohamed bin Zayed University of Artificial Intelligence.",
            "bBox": {
              "x": 308,
              "y": 711,
              "w": 128.24,
              "h": 9.96
            }
          }
        ],
        "status": "OK",
        "links": [
          {
            "text": "cantly surpass the baseline model CLIP, demonstrating that"
          },
          {
            "text": "significantly affected when the shot capacity is either too"
          },
          {
            "text": ""
          }
        ],
        "width": 612,
        "height": 792,
        "triggeredAutoMode": false,
        "structuredData": null,
        "noStructuredContent": false,
        "noTextContent": false
      },
      {
        "page": 9,
        "text": "References\n [1] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool.\n       Food-101 – mining discriminative components with random\n       forests. In European Conference on Computer Vision, 2014.\n       5, 11\n\n [2] Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca\n       Bertinetto. Parameter-free online test-time adaptation. In\n       Proceedings of the IEEE/CVF Conference on Computer Vi-\n       sion and Pattern Recognition, pages 8344–8353, 2022. 1\n [3] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna\n       Ebrahimi. Contrastive test-time adaptation. In Proceedings\n       of the IEEE/CVF Conference on Computer Vision and Pat-\n       tern Recognition, pages 295–305, 2022. 1\n [4] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A.\n       Vedaldi. Describing textures in the wild. In Proceedings of\n       the IEEE Conf. on Computer Vision and Pattern Recognition\n       (CVPR), 2014. 5, 11\n [5] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.\n       ImageNet: A Large-Scale Hierarchical Image Database. In\n       CVPR09, 2009. 5\n [6] Karan Desai and Justin Johnson. VirTex: Learning Visual\n       Representations from Textual Annotations. In CVPR, 2021.\n       2\n [7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\n       Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\n       Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\n       vain Gelly, et al. An image is worth 16x16 words: Trans-\n       formers for image recognition at scale.                arXiv preprint\n       arXiv:2010.11929, 2020. 6\n [8] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning gener-\n       ative visual models from few training examples: An incre-\n       mental bayesian approach tested on 101 object categories. In\n       CVPR Workshops, 2004. 5, 11\n [9] Chun-Mei Feng, Kai Yu, Yong Liu, Salman Khan, and\n       Wangmeng Zuo. Diverse data augmentation with diffusions\n       for effective test-time prompt tuning. In Proceedings of the\n       IEEE/CVF International Conference on Computer Vision,\n       pages 2704–2714, 2023. 1, 2, 6\n[10] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao\n       Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao.\n       Clip-adapter: Better vision-language models with feature\n       adapters. arXiv preprint arXiv:2110.04544, 2021. 2\n[11] Yves Grandvalet and Yoshua Bengio.                     Semi-supervised\n       learning by entropy minimization. Advances in neural in-\n       formation processing systems, 17, 2004. 2, 8\n[12] Edouard Grave, Moustapha M Cisse, and Armand Joulin.\n       Unbounded cache model for online language modeling with\n       open vocabulary. Advances in neural information processing\n       systems, 30, 2017. 2, 3\n[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\n       Deep residual learning for image recognition. In CVPR,\n       2016. 6\n[14] Patrick Helber, Benjamin Bischke, Andreas Dengel, and\n       Damian Borth. Eurosat: A novel dataset and deep learning\n       benchmark for land use and land cover classification. IEEE\n       J. Sel. Top. Appl. Earth Obs. Remote. Sens., 2019. 5, 11\n[15] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kada-\n       vath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu,\n       Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt,\n       and Justin Gilmer. The many faces of robustness: A criti-\n       cal analysis of out-of-distribution generalization. In ICCV,\n       2021. 5, 11\n[16] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Stein-\n       hardt, and Dawn Song. Natural adversarial examples. In\n       CVPR, pages 15262–15271, 2021. 5, 11\n[17] Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang,\n       Zicheng Liu, Yumao Lu, and Lijuan Wang.                      Scaling up\n       vision-language pre-training for image captioning. In Pro-\n       ceedings of the IEEE/CVF Conference on Computer Vision\n       and Pattern Recognition, pages 17980–17989, 2022. 2\n[18] Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier\n       adjustment module for model-agnostic domain generaliza-\n       tion. Advances in Neural Information Processing Systems,\n       34:2427–2440, 2021. 2\n[19] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,\n       Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom\n       Duerig. Scaling up visual and vision-language representa-\n       tion learning with noisy text supervision. In International\n       Conference on Machine Learning, pages 4904–4916. PMLR,\n       2021. 1, 2\n[20] Jeff Johnson, Matthijs Douze, and Herv´e J´egou. Billion-\n       scale similarity search with gpus. IEEE Transactions on Big\n       Data, 7(3):535–547, 2019. 3\n[21] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettle-\n       moyer, and Mike Lewis. Generalization through memoriza-\n       tion: Nearest neighbor language models. In International\n       Conference on Learning Representations, 2020. 2\n[22] Youngdong Kim, Junho Yim, Juseung Yun, and Junmo Kim.\n       Nlnl: Negative learning for noisy labels. In Proceedings of\n       the IEEE/CVF international conference on computer vision,\n       pages 101–110, 2019. 2, 5\n[23] Youngdong Kim, Juseung Yun, Hyounguk Shon, and Junmo\n       Kim. Joint negative and positive learning for noisy labels.\n       In Proceedings of the IEEE/CVF Conference on Computer\n       Vision and Pattern Recognition, pages 9442–9451, 2021. 2,\n       5\n[24] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.\n       3d object representations for fine-grained categorization. In\n       ICCV Workshops, 2013. 5, 11\n[25] Xin Li, Dongze Lian, Zhihe Lu, Jiawang Bai, Zhibo Chen,\n       and Xinchao Wang. Graphadapter: Tuning vision-language\n       models with dual knowledge graph. Advances in Neural In-\n       formation Processing Systems, 36, 2024. 2\n[26] S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi.\n       Fine-grained visual classification of aircraft. Technical re-\n       port, 2013. 5, 11\n[27] Stephen Merity, Caiming Xiong, James Bradbury, and\n       Richard Socher. Pointer sentinel mixture models. arXiv\n       preprint arXiv:1609.07843, 2016. 3\n[28] Maria-Elena Nilsback and Andrew Zisserman. Automated\n       flower classification over a large number of classes. In In-\n       dian Conference on Computer Vision, Graphics and Image\n       Processing, 2008. 5, 11",
        "md": "# References\n\n1. Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 – mining discriminative components with random forests. In European Conference on Computer Vision, 2014. 5, 11\n2. Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8344–8353, 2022. 1\n3. Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 295–305, 2022. 1\n4. M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, and A. Vedaldi. Describing textures in the wild. In Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2014. 5, 11\n5. J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09, 2009. 5\n6. Karan Desai and Justin Johnson. VirTex: Learning Visual Representations from Textual Annotations. In CVPR, 2021. 2\n7. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 6\n8. Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In CVPR Workshops, 2004. 5, 11\n9. Chun-Mei Feng, Kai Yu, Yong Liu, Salman Khan, and Wangmeng Zuo. Diverse data augmentation with diffusions for effective test-time prompt tuning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2704–2714, 2023. 1, 2, 6\n10. Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. Clip-adapter: Better vision-language models with feature adapters. arXiv preprint arXiv:2110.04544, 2021. 2\n11. Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. Advances in neural information processing systems, 17, 2004. 2, 8\n12. Edouard Grave, Moustapha M Cisse, and Armand Joulin. Unbounded cache model for online language modeling with open vocabulary. Advances in neural information processing systems, 30, 2017. 2, 3\n13. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 6\n14. Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE J. Sel. Top. Appl. Earth Obs. Remote. Sens., 2019. 5, 11\n15. Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. In ICCV, 2021. 5, 11\n16. Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In CVPR, pages 15262–15271, 2021. 5, 11\n17. Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, and Lijuan Wang. Scaling up vision-language pre-training for image captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17980–17989, 2022. 2\n18. Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier adjustment module for model-agnostic domain generalization. Advances in Neural Information Processing Systems, 34:2427–2440, 2021. 2\n19. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pages 4904–4916. PMLR, 2021. 1, 2\n20. Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. IEEE Transactions on Big Data, 7(3):535–547, 2019. 3\n21. Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models. In International Conference on Learning Representations, 2020. 2\n22. Youngdong Kim, Junho Yim, Juseung Yun, and Junmo Kim. Nlnl: Negative learning for noisy labels. In Proceedings of the IEEE/CVF international conference on computer vision, pages 101–110, 2019. 2, 5\n23. Youngdong Kim, Juseung Yun, Hyounguk Shon, and Junmo Kim. Joint negative and positive learning for noisy labels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9442–9451, 2021. 2, 5\n24. Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In ICCV Workshops, 2013. 5, 11\n25. Xin Li, Dongze Lian, Zhihe Lu, Jiawang Bai, Zhibo Chen, and Xinchao Wang. Graphadapter: Tuning vision-language models with dual knowledge graph. Advances in Neural Information Processing Systems, 36, 2024. 2\n26. S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classification of aircraft. Technical report, 2013. 5, 11\n27. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. 3\n28. Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In Indian Conference on Computer Vision, Graphics and Image Processing, 2008. 5, 11",
        "images": [],
        "items": [
          {
            "type": "heading",
            "lvl": 1,
            "value": "References",
            "md": "# References",
            "bBox": {
              "x": 50,
              "y": 82,
              "w": 55.54,
              "h": 11.96
            }
          },
          {
            "type": "text",
            "value": "1. Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 – mining discriminative components with random forests. In European Conference on Computer Vision, 2014. 5, 11\n2. Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8344–8353, 2022. 1\n3. Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 295–305, 2022. 1\n4. M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, and A. Vedaldi. Describing textures in the wild. In Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2014. 5, 11\n5. J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09, 2009. 5\n6. Karan Desai and Justin Johnson. VirTex: Learning Visual Representations from Textual Annotations. In CVPR, 2021. 2\n7. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 6\n8. Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In CVPR Workshops, 2004. 5, 11\n9. Chun-Mei Feng, Kai Yu, Yong Liu, Salman Khan, and Wangmeng Zuo. Diverse data augmentation with diffusions for effective test-time prompt tuning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2704–2714, 2023. 1, 2, 6\n10. Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. Clip-adapter: Better vision-language models with feature adapters. arXiv preprint arXiv:2110.04544, 2021. 2\n11. Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. Advances in neural information processing systems, 17, 2004. 2, 8\n12. Edouard Grave, Moustapha M Cisse, and Armand Joulin. Unbounded cache model for online language modeling with open vocabulary. Advances in neural information processing systems, 30, 2017. 2, 3\n13. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 6\n14. Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE J. Sel. Top. Appl. Earth Obs. Remote. Sens., 2019. 5, 11\n15. Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. In ICCV, 2021. 5, 11\n16. Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In CVPR, pages 15262–15271, 2021. 5, 11\n17. Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, and Lijuan Wang. Scaling up vision-language pre-training for image captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17980–17989, 2022. 2\n18. Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier adjustment module for model-agnostic domain generalization. Advances in Neural Information Processing Systems, 34:2427–2440, 2021. 2\n19. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pages 4904–4916. PMLR, 2021. 1, 2\n20. Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. IEEE Transactions on Big Data, 7(3):535–547, 2019. 3\n21. Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models. In International Conference on Learning Representations, 2020. 2\n22. Youngdong Kim, Junho Yim, Juseung Yun, and Junmo Kim. Nlnl: Negative learning for noisy labels. In Proceedings of the IEEE/CVF international conference on computer vision, pages 101–110, 2019. 2, 5\n23. Youngdong Kim, Juseung Yun, Hyounguk Shon, and Junmo Kim. Joint negative and positive learning for noisy labels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9442–9451, 2021. 2, 5\n24. Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In ICCV Workshops, 2013. 5, 11\n25. Xin Li, Dongze Lian, Zhihe Lu, Jiawang Bai, Zhibo Chen, and Xinchao Wang. Graphadapter: Tuning vision-language models with dual knowledge graph. Advances in Neural Information Processing Systems, 36, 2024. 2\n26. S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classification of aircraft. Technical report, 2013. 5, 11\n27. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. 3\n28. Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In Indian Conference on Computer Vision, Graphics and Image Processing, 2008. 5, 11",
            "md": "1. Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 – mining discriminative components with random forests. In European Conference on Computer Vision, 2014. 5, 11\n2. Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8344–8353, 2022. 1\n3. Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 295–305, 2022. 1\n4. M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, and A. Vedaldi. Describing textures in the wild. In Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2014. 5, 11\n5. J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09, 2009. 5\n6. Karan Desai and Justin Johnson. VirTex: Learning Visual Representations from Textual Annotations. In CVPR, 2021. 2\n7. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 6\n8. Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In CVPR Workshops, 2004. 5, 11\n9. Chun-Mei Feng, Kai Yu, Yong Liu, Salman Khan, and Wangmeng Zuo. Diverse data augmentation with diffusions for effective test-time prompt tuning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2704–2714, 2023. 1, 2, 6\n10. Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. Clip-adapter: Better vision-language models with feature adapters. arXiv preprint arXiv:2110.04544, 2021. 2\n11. Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. Advances in neural information processing systems, 17, 2004. 2, 8\n12. Edouard Grave, Moustapha M Cisse, and Armand Joulin. Unbounded cache model for online language modeling with open vocabulary. Advances in neural information processing systems, 30, 2017. 2, 3\n13. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 6\n14. Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE J. Sel. Top. Appl. Earth Obs. Remote. Sens., 2019. 5, 11\n15. Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. In ICCV, 2021. 5, 11\n16. Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In CVPR, pages 15262–15271, 2021. 5, 11\n17. Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, and Lijuan Wang. Scaling up vision-language pre-training for image captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17980–17989, 2022. 2\n18. Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier adjustment module for model-agnostic domain generalization. Advances in Neural Information Processing Systems, 34:2427–2440, 2021. 2\n19. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pages 4904–4916. PMLR, 2021. 1, 2\n20. Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. IEEE Transactions on Big Data, 7(3):535–547, 2019. 3\n21. Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models. In International Conference on Learning Representations, 2020. 2\n22. Youngdong Kim, Junho Yim, Juseung Yun, and Junmo Kim. Nlnl: Negative learning for noisy labels. In Proceedings of the IEEE/CVF international conference on computer vision, pages 101–110, 2019. 2, 5\n23. Youngdong Kim, Juseung Yun, Hyounguk Shon, and Junmo Kim. Joint negative and positive learning for noisy labels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9442–9451, 2021. 2, 5\n24. Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In ICCV Workshops, 2013. 5, 11\n25. Xin Li, Dongze Lian, Zhihe Lu, Jiawang Bai, Zhibo Chen, and Xinchao Wang. Graphadapter: Tuning vision-language models with dual knowledge graph. Advances in Neural Information Processing Systems, 36, 2024. 2\n26. S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classification of aircraft. Technical report, 2013. 5, 11\n27. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. 3\n28. Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In Indian Conference on Computer Vision, Graphics and Image Processing, 2008. 5, 11",
            "bBox": {
              "x": 70,
              "y": 93,
              "w": 475.11,
              "h": 626.97
            }
          }
        ],
        "status": "OK",
        "links": [
          {
            "text": "5, 11"
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": "2"
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": "5"
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          }
        ],
        "width": 612,
        "height": 792,
        "triggeredAutoMode": false,
        "structuredData": null,
        "noStructuredContent": false,
        "noTextContent": false
      },
      {
        "page": 10,
        "text": "[29] Emin Orhan.          A simple cache model for image recogni-\n       tion. Advances in Neural Information Processing Systems,\n       31, 2018. 2\n[30] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and\n       C. V. Jawahar. Cats and dogs. In CVPR, 2012. 5, 11\n[31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\n       Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\n       Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen\n       Krueger, and Ilya Sutskever. Learning transferable visual\n       models from natural language supervision. In ICML, 2021.\n       1, 2, 3, 6\n[32] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and\n       Vaishaal Shankar. Do imagenet classifiers generalize to im-\n       agenet? In ICML, 2019. 5, 11\n[33] Mamshad Nayeem Rizve, Kevin Duarte, Yogesh S Rawat,\n       and Mubarak Shah.           In defense of pseudo-labeling: An\n       uncertainty-aware pseudo-label selection framework for\n       semi-supervised learning. arXiv preprint arXiv:2101.06329,\n       2021. 2, 5\n[34] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bring-\n       mann, Wieland Brendel, and Matthias Bethge. Improving\n       robustness against common corruptions by covariate shift\n       adaptation. Advances in Neural Information Processing Sys-\n       tems, 33:11539–11551, 2020. 2\n[35] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom\n       Goldstein, Anima Anandkumar, and Chaowei Xiao. Test-\n       time prompt tuning for zero-shot generalization in vision-\n       language models. In Advances in Neural Information Pro-\n       cessing Systems, 2022. 1, 2, 3, 5, 6\n[36] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.\n       UCF101: A dataset of 101 human actions classes from\n       videos in the wild. CoRR, abs/1212.0402, 2012. 5, 11\n[37] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A.\n       Efros, and Moritz Hardt.             Test-time training with self-\n       supervision for generalization under distribution shifts. In\n       ICML, 2020. 2\n[38] Thomas Varsavsky, Mauricio Orbes-Arteaga, Carole H Su-\n       dre, Mark S Graham, Parashkev Nachev, and M Jorge Car-\n       doso. Test-time unsupervised domain adaptation. In Medi-\n       cal Image Computing and Computer Assisted Intervention–\n       MICCAI 2020: 23rd International Conference, Lima, Peru,\n       October 4–8, 2020, Proceedings, Part I 23, pages 428–436.\n       Springer, 2020. 2\n[39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\n       reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\n       Polosukhin. Attention is all you need. Advances in neural\n       information processing systems, 30, 2017. 6\n[40] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno A. Ol-\n       shausen, and Trevor Darrell. Tent: Fully test-time adaptation\n       by entropy minimization. In ICLR, 2021. 1, 2\n[41] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P\n       Xing. Learning robust global representations by penalizing\n       local predictive power. In NeurIPS, 2019. 5, 11\n[42] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba.\n       Sun database: Large-scale scene recognition from abbey to\n       zoo. In 2010 IEEE Computer Society Conference on Com-\n       puter Vision and Pattern Recognition, 2010. 5, 11\n[43] Jinyu Yang, Jiali Duan, Son Tran, Yi Xu, Sampath Chanda,\n       Liqun Chen, Belinda Zeng, Trishul Chilimbi, and Junzhou\n       Huang. Vision-language pre-training with triple contrastive\n       learning. In Proceedings of the IEEE/CVF Conference on\n       Computer Vision and Pattern Recognition, pages 15671–\n       15680, 2022. 1, 2\n[44] Chenyu Yi, SIYUAN YANG, Yufei Wang, Haoliang Li, Yap\n       peng Tan, and Alex Kot. Temporal coherent test time opti-\n       mization for robust video classification. In The Eleventh In-\n       ternational Conference on Learning Representations, 2023.\n       2\n[45] Tao Yu, Zhihe Lu, Xin Jin, Zhibo Chen, and Xinchao Wang.\n       Task residual for tuning vision-language models. In Proceed-\n       ings of the IEEE/CVF Conference on Computer Vision and\n       Pattern Recognition, pages 10899–10909, 2023. 2\n[46] Marvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek\n       Gupta, Sergey Levine, and Chelsea Finn. Adaptive risk min-\n       imization: Learning to adapt to domain shift. Advances in\n       Neural Information Processing Systems, 34:23664–23678,\n       2021. 2\n[47] Marvin Mengxin Zhang, Sergey Levine, and Chelsea Finn.\n       MEMO: Test time robustness via adaptation and augmenta-\n       tion. In Advances in Neural Information Processing Systems,\n       2022. 2\n[48] Renrui Zhang, Wei Zhang, Rongyao Fang, Peng Gao, Kun-\n       chang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip-\n       adapter: Training-free adaption of clip for few-shot classifi-\n       cation. In European Conference on Computer Vision, pages\n       493–510. Springer, 2022. 2, 3, 6\n[49] Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Han-\n       qiu Deng, Yu Qiao, Peng Gao, and Hongsheng Li. Prompt,\n       generate, then cache: Cascade of foundation models makes\n       strong few-shot learners. In Proceedings of the IEEE/CVF\n       Conference on Computer Vision and Pattern Recognition,\n       pages 15211–15222, 2023. 2\n[50] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei\n       Liu. Conditional prompt learning for vision-language mod-\n       els. In Proceedings of the IEEE/CVF Conference on Com-\n       puter Vision and Pattern Recognition, pages 16816–16825,\n       2022. 2, 6\n[51] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei\n       Liu. Learning to prompt for vision-language models. In-\n       ternational Journal of Computer Vision, 130(9):2337–2348,\n       2022. 2, 6",
        "md": "# References\n\n1. Emin Orhan. A simple cache model for image recognition. Advances in Neural Information Processing Systems, 31, 2018. 2\n2. Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In CVPR, 2012. 5, 11\n3. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. 1, 2, 3, 6\n4. Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In ICML, 2019. 5, 11\n5. Mamshad Nayeem Rizve, Kevin Duarte, Yogesh S Rawat, and Mubarak Shah. In defense of pseudo-labeling: An uncertainty-aware pseudo-label selection framework for semi-supervised learning. arXiv preprint arXiv:2101.06329, 2021. 2, 5\n6. Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. Advances in Neural Information Processing Systems, 33:11539–11551, 2020. 2\n7. Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Test-time prompt tuning for zero-shot generalization in vision-language models. In Advances in Neural Information Processing Systems, 2022. 1, 2, 3, 5, 6\n8. Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. UCF101: A dataset of 101 human actions classes from videos in the wild. CoRR, abs/1212.0402, 2012. 5, 11\n9. Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A. Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In ICML, 2020. 2\n10. Thomas Varsavsky, Mauricio Orbes-Arteaga, Carole H Sudre, Mark S Graham, Parashkev Nachev, and M Jorge Cardoso. Test-time unsupervised domain adaptation. In Medical Image Computing and Computer Assisted Intervention–MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part I 23, pages 428–436. Springer, 2020. 2\n11. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 6\n12. Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno A. Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In ICLR, 2021. 1, 2\n13. Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. In NeurIPS, 2019. 5, 11\n14. J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2010. 5, 11\n15. Jinyu Yang, Jiali Duan, Son Tran, Yi Xu, Sampath Chanda, Liqun Chen, Belinda Zeng, Trishul Chilimbi, and Junzhou Huang. Vision-language pre-training with triple contrastive learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15671–15680, 2022. 1, 2\n16. Chenyu Yi, SIYUAN YANG, Yufei Wang, Haoliang Li, Yap peng Tan, and Alex Kot. Temporal coherent test time optimization for robust video classification. In The Eleventh International Conference on Learning Representations, 2023. 2\n17. Tao Yu, Zhihe Lu, Xin Jin, Zhibo Chen, and Xinchao Wang. Task residual for tuning vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10899–10909, 2023. 2\n18. Marvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek Gupta, Sergey Levine, and Chelsea Finn. Adaptive risk minimization: Learning to adapt to domain shift. Advances in Neural Information Processing Systems, 34:23664–23678, 2021. 2\n19. Marvin Mengxin Zhang, Sergey Levine, and Chelsea Finn. MEMO: Test time robustness via adaptation and augmentation. In Advances in Neural Information Processing Systems, 2022. 2\n20. Renrui Zhang, Wei Zhang, Rongyao Fang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip-adapter: Training-free adaption of clip for few-shot classification. In European Conference on Computer Vision, pages 493–510. Springer, 2022. 2, 3, 6\n21. Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Yu Qiao, Peng Gao, and Hongsheng Li. Prompt, generate, then cache: Cascade of foundation models makes strong few-shot learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15211–15222, 2023. 2\n22. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16816–16825, 2022. 2, 6\n23. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. International Journal of Computer Vision, 130(9):2337–2348, 2022. 2, 6",
        "images": [],
        "items": [
          {
            "type": "heading",
            "lvl": 1,
            "value": "References",
            "md": "# References",
            "bBox": {
              "x": 0,
              "y": 0,
              "w": 612,
              "h": 792
            }
          },
          {
            "type": "text",
            "value": "1. Emin Orhan. A simple cache model for image recognition. Advances in Neural Information Processing Systems, 31, 2018. 2\n2. Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In CVPR, 2012. 5, 11\n3. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. 1, 2, 3, 6\n4. Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In ICML, 2019. 5, 11\n5. Mamshad Nayeem Rizve, Kevin Duarte, Yogesh S Rawat, and Mubarak Shah. In defense of pseudo-labeling: An uncertainty-aware pseudo-label selection framework for semi-supervised learning. arXiv preprint arXiv:2101.06329, 2021. 2, 5\n6. Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. Advances in Neural Information Processing Systems, 33:11539–11551, 2020. 2\n7. Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Test-time prompt tuning for zero-shot generalization in vision-language models. In Advances in Neural Information Processing Systems, 2022. 1, 2, 3, 5, 6\n8. Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. UCF101: A dataset of 101 human actions classes from videos in the wild. CoRR, abs/1212.0402, 2012. 5, 11\n9. Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A. Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In ICML, 2020. 2\n10. Thomas Varsavsky, Mauricio Orbes-Arteaga, Carole H Sudre, Mark S Graham, Parashkev Nachev, and M Jorge Cardoso. Test-time unsupervised domain adaptation. In Medical Image Computing and Computer Assisted Intervention–MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part I 23, pages 428–436. Springer, 2020. 2\n11. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 6\n12. Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno A. Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In ICLR, 2021. 1, 2\n13. Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. In NeurIPS, 2019. 5, 11\n14. J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2010. 5, 11\n15. Jinyu Yang, Jiali Duan, Son Tran, Yi Xu, Sampath Chanda, Liqun Chen, Belinda Zeng, Trishul Chilimbi, and Junzhou Huang. Vision-language pre-training with triple contrastive learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15671–15680, 2022. 1, 2\n16. Chenyu Yi, SIYUAN YANG, Yufei Wang, Haoliang Li, Yap peng Tan, and Alex Kot. Temporal coherent test time optimization for robust video classification. In The Eleventh International Conference on Learning Representations, 2023. 2\n17. Tao Yu, Zhihe Lu, Xin Jin, Zhibo Chen, and Xinchao Wang. Task residual for tuning vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10899–10909, 2023. 2\n18. Marvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek Gupta, Sergey Levine, and Chelsea Finn. Adaptive risk minimization: Learning to adapt to domain shift. Advances in Neural Information Processing Systems, 34:23664–23678, 2021. 2\n19. Marvin Mengxin Zhang, Sergey Levine, and Chelsea Finn. MEMO: Test time robustness via adaptation and augmentation. In Advances in Neural Information Processing Systems, 2022. 2\n20. Renrui Zhang, Wei Zhang, Rongyao Fang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip-adapter: Training-free adaption of clip for few-shot classification. In European Conference on Computer Vision, pages 493–510. Springer, 2022. 2, 3, 6\n21. Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Yu Qiao, Peng Gao, and Hongsheng Li. Prompt, generate, then cache: Cascade of foundation models makes strong few-shot learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15211–15222, 2023. 2\n22. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16816–16825, 2022. 2, 6\n23. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. International Journal of Computer Vision, 130(9):2337–2348, 2022. 2, 6",
            "md": "1. Emin Orhan. A simple cache model for image recognition. Advances in Neural Information Processing Systems, 31, 2018. 2\n2. Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In CVPR, 2012. 5, 11\n3. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. 1, 2, 3, 6\n4. Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In ICML, 2019. 5, 11\n5. Mamshad Nayeem Rizve, Kevin Duarte, Yogesh S Rawat, and Mubarak Shah. In defense of pseudo-labeling: An uncertainty-aware pseudo-label selection framework for semi-supervised learning. arXiv preprint arXiv:2101.06329, 2021. 2, 5\n6. Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. Advances in Neural Information Processing Systems, 33:11539–11551, 2020. 2\n7. Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Test-time prompt tuning for zero-shot generalization in vision-language models. In Advances in Neural Information Processing Systems, 2022. 1, 2, 3, 5, 6\n8. Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. UCF101: A dataset of 101 human actions classes from videos in the wild. CoRR, abs/1212.0402, 2012. 5, 11\n9. Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A. Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In ICML, 2020. 2\n10. Thomas Varsavsky, Mauricio Orbes-Arteaga, Carole H Sudre, Mark S Graham, Parashkev Nachev, and M Jorge Cardoso. Test-time unsupervised domain adaptation. In Medical Image Computing and Computer Assisted Intervention–MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part I 23, pages 428–436. Springer, 2020. 2\n11. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 6\n12. Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno A. Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In ICLR, 2021. 1, 2\n13. Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. In NeurIPS, 2019. 5, 11\n14. J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2010. 5, 11\n15. Jinyu Yang, Jiali Duan, Son Tran, Yi Xu, Sampath Chanda, Liqun Chen, Belinda Zeng, Trishul Chilimbi, and Junzhou Huang. Vision-language pre-training with triple contrastive learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15671–15680, 2022. 1, 2\n16. Chenyu Yi, SIYUAN YANG, Yufei Wang, Haoliang Li, Yap peng Tan, and Alex Kot. Temporal coherent test time optimization for robust video classification. In The Eleventh International Conference on Learning Representations, 2023. 2\n17. Tao Yu, Zhihe Lu, Xin Jin, Zhibo Chen, and Xinchao Wang. Task residual for tuning vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10899–10909, 2023. 2\n18. Marvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek Gupta, Sergey Levine, and Chelsea Finn. Adaptive risk minimization: Learning to adapt to domain shift. Advances in Neural Information Processing Systems, 34:23664–23678, 2021. 2\n19. Marvin Mengxin Zhang, Sergey Levine, and Chelsea Finn. MEMO: Test time robustness via adaptation and augmentation. In Advances in Neural Information Processing Systems, 2022. 2\n20. Renrui Zhang, Wei Zhang, Rongyao Fang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip-adapter: Training-free adaption of clip for few-shot classification. In European Conference on Computer Vision, pages 493–510. Springer, 2022. 2, 3, 6\n21. Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Yu Qiao, Peng Gao, and Hongsheng Li. Prompt, generate, then cache: Cascade of foundation models makes strong few-shot learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15211–15222, 2023. 2\n22. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16816–16825, 2022. 2, 6\n23. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. International Journal of Computer Vision, 130(9):2337–2348, 2022. 2, 6",
            "bBox": {
              "x": 70,
              "y": 93,
              "w": 474.78,
              "h": 626.97
            }
          }
        ],
        "status": "OK",
        "links": [
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": "1, 2, 3, 6"
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": "2"
          },
          {
            "text": "[46] Marvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek"
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": "[49] Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Han-"
          },
          {
            "text": "[49] Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Han-"
          },
          {
            "text": "[49] Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Han-"
          },
          {
            "text": "[50] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei"
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          }
        ],
        "width": 612,
        "height": 792,
        "triggeredAutoMode": false,
        "structuredData": null,
        "noStructuredContent": false,
        "noTextContent": false
      },
      {
        "page": 11,
        "text": "                6. Benchmark Details                                                 Dataset              Classes       Test Size\n                This section provides detailed information on the two                ImageNet              1,000         50,000\n                benchmarks used in our work. OOD Benchmark is used                   ImageNet-V2           1,000         10,000\n                to evaluate the model robustness against natural distribu-           ImageNet-S            1,000         50,000\n                tion shifts using the traditional ImageNet and its out-of-           ImageNet-A             200           7,500\n                distribution (OOD) versions containing images with vary-             ImageNet-R             200          30,000\n                ing styles and corruptions. Herein below, we provide a con-          Aircraft               100           3,333\n                cise overview of each of the OOD datasets.                           Caltech101             100           2,465\n                • ImageNet-V2 [32] consists of 10,000 images and 1,000\n                  ImageNet classes, and was collected by applying an up-             Cars                   196           8,041\n                  dated natural data collection pipeline to the original Ima-        DTD                     47           1,692\n                  geNet data.                                                        EuroSAT                 10           8,100\n                • ImageNet-A [16] is a subset of 7,500 visually similar but          Flowers102             102           2,463\n                  naturally perturbed ImageNet images of 200 classes.                Food101                101          30,300\n                • ImageNet-R [15] includes 30,000 images belonging to                Pets                    37           3,669\n                  200 categories of the ImageNet dataset, but with diverse           SUN397                 397          19,850\n                  artistic styles.                                                   UCF101                 101           3,783\n                • ImageNet-S [41] consists of 50,000 sketches of 1000\n                  class objects from the ImageNet dataset, and represents                   Table 4. Datasets statistics.Accuracy (%)a domain shift from natural images to sketches.\n                Cross-Domain Benchmark consists of 10 image classifi-                                      ImageNet\n                cation datasets to evaluate the effectiveness of the method\n                on different domains. This benchmark incorporates the fol-     60.8\n                lowing datasets: Caltech101 [8] for general image classi-\n                fication, OxfordPets (Pets) [30], StanfordCars (Cars) [24],\n                Flowers102 [28], Food101 [1], and FGVCAircraft (Air-\n                craft) [26] for fine-grained image classification, EuroSAT\n                [14] for satellite image classification, UCF101 [36] for ac-   60.5\n                tion classification, DTD [4] for texture classification, and\n                SUN397 [42] for scene classification.\n                The detailed statistics of all the datasets are shown in Ta-\n                ble 4.                                                         60.2\n\n7. Parameter Studies on Thresholds\n\nThis section provides more parameter studies on three\nthresholds defined in our work. Our experiments are con-\nducted on the ImageNet validation set using the default set-\ntings.\n\n\nThe threshold for negative pseudo-labeling.                   In Eq 4 of\nour manuscript, the threshold pl is used to select negative\npseudo labels by applying the negative mask. We conduct\nparameter studies on pl and the results are illustrated in Fig-\nure 5. The best performance is achieved when pl is set to\n0.03 and subsequent increases in pl do not yield notable im-\nprovement or degradation in the results, indicating the sta-\nbility of this parameter. It can be noticed that the perfor-\nmance deteriorates when pl is less than 0.03 because the\nconfident classes with low probabilities should not be in-\ncluded in negative pseudo labels.\n                 0.02          0.04         0.06         0.08          0.10\n                             Negative mask threshold\n\nFigure 5. Parameter studies on the Negative Mask Threshold pl\nfor the negative pseudo-labeling in Negative Cache. The results\nare reported on ImageNet top-1 accuracy using only the Negative\nCache to produce an adapted prediction. The experiments are con-\nducted with CLIP-ResNet50.\n\n\nThe threshold range for testing feature selection in the\nnegative cache.         In Eq 5 of our manuscript, the thresh-\nolds [τ, τh] are used to check whether the testing featurel\nwill be considered to be included in Negative Cache if the\nentropy of the prediction is in the specified interval. Ta-\nble 5 presents the results of an ablation study focusing on\nthe impact of adjusting the threshold range for testing fea-\nture selection in the Negative Cache. This investigation\ndelves into the testing feature selection of uncertain sam-",
        "md": "# 6. Benchmark Details\n\n|Dataset|Classes|Test Size|\n|---|---|---|\n|ImageNet|1,000|50,000|\n|ImageNet-V2|1,000|10,000|\n|ImageNet-S|1,000|50,000|\n|ImageNet-A|200|7,500|\n|ImageNet-R|200|30,000|\n|Aircraft|100|3,333|\n|Caltech101|100|2,465|\n|Cars|196|8,041|\n|DTD|47|1,692|\n|EuroSAT|10|8,100|\n|Flowers102|102|2,463|\n|Food101|101|30,300|\n|Pets|37|3,669|\n|SUN397|397|19,850|\n|UCF101|101|3,783|\n\nCross-Domain Benchmark consists of 10 image classification datasets to evaluate the effectiveness of the method on different domains. This benchmark incorporates the following datasets: Caltech101 [8] for general image classification, OxfordPets (Pets) [30], StanfordCars (Cars) [24], Flowers102 [28], Food101 [1], and FGVCAircraft (Aircraft) [26] for fine-grained image classification, EuroSAT [14] for satellite image classification, UCF101 [36] for action classification, DTD [4] for texture classification, and SUN397 [42] for scene classification.\n\nThe detailed statistics of all the datasets are shown in Table 4.\n\n|Accuracy (%)|Domain Shift from Natural Images to Sketches|\n|---|---|\n|60.8|ImageNet|\n|60.5| |\n|60.2| |\n\n# 7. Parameter Studies on Thresholds\n\nThis section provides more parameter studies on three thresholds defined in our work. Our experiments are conducted on the ImageNet validation set using the default settings.\n\nThe threshold for negative pseudo-labeling. In Eq 4 of our manuscript, the threshold pl is used to select negative pseudo labels by applying the negative mask. We conduct parameter studies on pl and the results are illustrated in Figure 5. The best performance is achieved when pl is set to 0.03 and subsequent increases in pl do not yield notable improvement or degradation in the results, indicating the stability of this parameter. It can be noticed that the performance deteriorates when pl is less than 0.03 because the confident classes with low probabilities should not be included in negative pseudo labels.\n\n0.02 0.04 0.06 0.08 0.10\n\nNegative mask threshold\n\nFigure 5. Parameter studies on the Negative Mask Threshold pl for the negative pseudo-labeling in Negative Cache. The results are reported on ImageNet top-1 accuracy using only the Negative Cache to produce an adapted prediction. The experiments are conducted with CLIP-ResNet50.\n\nThe threshold range for testing feature selection in the negative cache. In Eq 5 of our manuscript, the thresholds [τ, τh] are used to check whether the testing feature will be considered to be included in Negative Cache if the entropy of the prediction is in the specified interval. Table 5 presents the results of an ablation study focusing on the impact of adjusting the threshold range for testing feature selection in the Negative Cache. This investigation delves into the testing feature selection of uncertain samples.",
        "images": [],
        "items": [
          {
            "type": "heading",
            "lvl": 1,
            "value": "6. Benchmark Details",
            "md": "# 6. Benchmark Details",
            "bBox": {
              "x": 50,
              "y": 82,
              "w": 288.52,
              "h": 565.97
            }
          },
          {
            "type": "table",
            "rows": [
              [
                "Dataset",
                "Classes",
                "Test Size"
              ],
              [
                "ImageNet",
                "1,000",
                "50,000"
              ],
              [
                "ImageNet-V2",
                "1,000",
                "10,000"
              ],
              [
                "ImageNet-S",
                "1,000",
                "50,000"
              ],
              [
                "ImageNet-A",
                "200",
                "7,500"
              ],
              [
                "ImageNet-R",
                "200",
                "30,000"
              ],
              [
                "Aircraft",
                "100",
                "3,333"
              ],
              [
                "Caltech101",
                "100",
                "2,465"
              ],
              [
                "Cars",
                "196",
                "8,041"
              ],
              [
                "DTD",
                "47",
                "1,692"
              ],
              [
                "EuroSAT",
                "10",
                "8,100"
              ],
              [
                "Flowers102",
                "102",
                "2,463"
              ],
              [
                "Food101",
                "101",
                "30,300"
              ],
              [
                "Pets",
                "37",
                "3,669"
              ],
              [
                "SUN397",
                "397",
                "19,850"
              ],
              [
                "UCF101",
                "101",
                "3,783"
              ]
            ],
            "md": "|Dataset|Classes|Test Size|\n|---|---|---|\n|ImageNet|1,000|50,000|\n|ImageNet-V2|1,000|10,000|\n|ImageNet-S|1,000|50,000|\n|ImageNet-A|200|7,500|\n|ImageNet-R|200|30,000|\n|Aircraft|100|3,333|\n|Caltech101|100|2,465|\n|Cars|196|8,041|\n|DTD|47|1,692|\n|EuroSAT|10|8,100|\n|Flowers102|102|2,463|\n|Food101|101|30,300|\n|Pets|37|3,669|\n|SUN397|397|19,850|\n|UCF101|101|3,783|",
            "isPerfectTable": true,
            "csv": "\"Dataset\",\"Classes\",\"Test Size\"\n\"ImageNet\",\"1,000\",\"50,000\"\n\"ImageNet-V2\",\"1,000\",\"10,000\"\n\"ImageNet-S\",\"1,000\",\"50,000\"\n\"ImageNet-A\",\"200\",\"7,500\"\n\"ImageNet-R\",\"200\",\"30,000\"\n\"Aircraft\",\"100\",\"3,333\"\n\"Caltech101\",\"100\",\"2,465\"\n\"Cars\",\"196\",\"8,041\"\n\"DTD\",\"47\",\"1,692\"\n\"EuroSAT\",\"10\",\"8,100\"\n\"Flowers102\",\"102\",\"2,463\"\n\"Food101\",\"101\",\"30,300\"\n\"Pets\",\"37\",\"3,669\"\n\"SUN397\",\"397\",\"19,850\"\n\"UCF101\",\"101\",\"3,783\"",
            "bBox": {
              "x": 336,
              "y": 85,
              "w": 163.1,
              "h": 562.97
            }
          },
          {
            "type": "text",
            "value": "Cross-Domain Benchmark consists of 10 image classification datasets to evaluate the effectiveness of the method on different domains. This benchmark incorporates the following datasets: Caltech101 [8] for general image classification, OxfordPets (Pets) [30], StanfordCars (Cars) [24], Flowers102 [28], Food101 [1], and FGVCAircraft (Aircraft) [26] for fine-grained image classification, EuroSAT [14] for satellite image classification, UCF101 [36] for action classification, DTD [4] for texture classification, and SUN397 [42] for scene classification.\n\nThe detailed statistics of all the datasets are shown in Table 4.",
            "md": "Cross-Domain Benchmark consists of 10 image classification datasets to evaluate the effectiveness of the method on different domains. This benchmark incorporates the following datasets: Caltech101 [8] for general image classification, OxfordPets (Pets) [30], StanfordCars (Cars) [24], Flowers102 [28], Food101 [1], and FGVCAircraft (Aircraft) [26] for fine-grained image classification, EuroSAT [14] for satellite image classification, UCF101 [36] for action classification, DTD [4] for texture classification, and SUN397 [42] for scene classification.\n\nThe detailed statistics of all the datasets are shown in Table 4.",
            "bBox": {
              "x": 50,
              "y": 85,
              "w": 392.94,
              "h": 562.97
            }
          },
          {
            "type": "table",
            "rows": [
              [
                "Accuracy (%)",
                "Domain Shift from Natural Images to Sketches"
              ],
              [
                "60.8",
                "ImageNet"
              ],
              [
                "60.5",
                ""
              ],
              [
                "60.2",
                ""
              ]
            ],
            "md": "|Accuracy (%)|Domain Shift from Natural Images to Sketches|\n|---|---|\n|60.8|ImageNet|\n|60.5| |\n|60.2| |",
            "isPerfectTable": true,
            "csv": "\"Accuracy (%)\",\"Domain Shift from Natural Images to Sketches\"\n\"60.8\",\"ImageNet\"\n\"60.5\",\"\"\n\"60.2\",\"\"",
            "bBox": {
              "x": null,
              "y": 103,
              "w": null,
              "h": 544.97
            }
          },
          {
            "type": "heading",
            "lvl": 1,
            "value": "7. Parameter Studies on Thresholds",
            "md": "# 7. Parameter Studies on Thresholds",
            "bBox": {
              "x": 50,
              "y": 504,
              "w": 288.52,
              "h": 143.97
            }
          },
          {
            "type": "text",
            "value": "This section provides more parameter studies on three thresholds defined in our work. Our experiments are conducted on the ImageNet validation set using the default settings.\n\nThe threshold for negative pseudo-labeling. In Eq 4 of our manuscript, the threshold pl is used to select negative pseudo labels by applying the negative mask. We conduct parameter studies on pl and the results are illustrated in Figure 5. The best performance is achieved when pl is set to 0.03 and subsequent increases in pl do not yield notable improvement or degradation in the results, indicating the stability of this parameter. It can be noticed that the performance deteriorates when pl is less than 0.03 because the confident classes with low probabilities should not be included in negative pseudo labels.\n\n0.02 0.04 0.06 0.08 0.10\n\nNegative mask threshold\n\nFigure 5. Parameter studies on the Negative Mask Threshold pl for the negative pseudo-labeling in Negative Cache. The results are reported on ImageNet top-1 accuracy using only the Negative Cache to produce an adapted prediction. The experiments are conducted with CLIP-ResNet50.\n\nThe threshold range for testing feature selection in the negative cache. In Eq 5 of our manuscript, the thresholds [τ, τh] are used to check whether the testing feature will be considered to be included in Negative Cache if the entropy of the prediction is in the specified interval. Table 5 presents the results of an ablation study focusing on the impact of adjusting the threshold range for testing feature selection in the Negative Cache. This investigation delves into the testing feature selection of uncertain samples.",
            "md": "This section provides more parameter studies on three thresholds defined in our work. Our experiments are conducted on the ImageNet validation set using the default settings.\n\nThe threshold for negative pseudo-labeling. In Eq 4 of our manuscript, the threshold pl is used to select negative pseudo labels by applying the negative mask. We conduct parameter studies on pl and the results are illustrated in Figure 5. The best performance is achieved when pl is set to 0.03 and subsequent increases in pl do not yield notable improvement or degradation in the results, indicating the stability of this parameter. It can be noticed that the performance deteriorates when pl is less than 0.03 because the confident classes with low probabilities should not be included in negative pseudo labels.\n\n0.02 0.04 0.06 0.08 0.10\n\nNegative mask threshold\n\nFigure 5. Parameter studies on the Negative Mask Threshold pl for the negative pseudo-labeling in Negative Cache. The results are reported on ImageNet top-1 accuracy using only the Negative Cache to produce an adapted prediction. The experiments are conducted with CLIP-ResNet50.\n\nThe threshold range for testing feature selection in the negative cache. In Eq 5 of our manuscript, the thresholds [τ, τh] are used to check whether the testing feature will be considered to be included in Negative Cache if the entropy of the prediction is in the specified interval. Table 5 presents the results of an ablation study focusing on the impact of adjusting the threshold range for testing feature selection in the Negative Cache. This investigation delves into the testing feature selection of uncertain samples.",
            "bBox": {
              "x": 50,
              "y": 85,
              "w": 495,
              "h": 636.96
            }
          }
        ],
        "status": "OK",
        "links": [
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": ""
          },
          {
            "text": "0.03 and subsequent increases in"
          },
          {
            "text": "the impact of adjusting the threshold range for testing fea-"
          }
        ],
        "width": 612,
        "height": 792,
        "triggeredAutoMode": false,
        "structuredData": null,
        "noStructuredContent": false,
        "noTextContent": false
      },
      {
        "page": 12,
        "text": "  Minimum entropy features               Maximum entropy features           Residual Ratio       0.5      1.0      2.0      3.0       4.0      5.0\n  τl      τh         Accuracy            τl      τh         Accuracy        TDA                61.07     61.20    61.35    61.29    60.90     60.63\n  0.0     1.0          60.69             0.2     0.4           60.51        Sharpness Ratio      0.5      1.0      3.0      5.0       7.0      9.0\n  0.0     0.2          60.67             0.2     0.5           60.53        TDA                60.98     61.20    61.29    61.35    61.20     61.19\n  0.0     0.3          60.69             0.2     0.6           60.51      Table 6. Analysis on the residual and sharpness ratios of TDA.\n  0.1     0.3          60.76             0.3     0.5           60.30\n  0.1     0.4          60.77             0.3     0.7           60.30    8. More Experimental Analysis\n  0.2     0.4          60.81             0.4     0.6           60.16    Caches built for inference.              The caches are built on the\n  0.2     0.5          60.83             0.4     0.7           60.16    fly during inference, starting empty and progressively accu-\n  0.2     0.6          60.81             0.5     0.7           60.34    mulating samples. At the start of the testing phase on Im-\n  0.3     0.5          60.34             0.5     0.8           60.34    ageNet, where only 1% of the data was used, we observed\n  0.3     0.6          60.35             0.6     0.8           60.35    a slight accuracy drop of 0.06%. We also noted that by-\n                                                                        passing cache usage at the early testing phase leads to a\nTable 5. Ablation study of the impact of varying Threshold Range        marginal accuracy improvement of 0.1%. We didn’t adopt\n[τ, τh] for testing feature selection in the Negative Cache. The\n  l                                                                     this approach as it increases complexity by introducing an\nstudy investigates the testing feature selection of the uncertain       extra hyperparameter for determining when to use caches.\nsamples in two ways: choosing the minimum and maximum en-\ntropy features in the given range. The results are reported on Im-      Class imbalance under high shot capacity.                    Our analysis\nageNet top-1 accuracy using only the Negative Cache to produce          with a 6-shot positive cache reveals minimal class imbal-\nan adapted prediction. The experiments are conducted with CLIP-         ance (only 4 out of 1000 ImageNet classes have less than\nResNet50.                                                               6 samples) but identifies a significant cache accuracy drop\n                                                                        from 90.3% to 86.6% when shot capacity increases from 3\n                                                                        to 6. Such accuracy drop happens because larger cache ca-\nples using two distinct approaches: one involving values                pacities tend to accumulate noise, thereby reducing the reli-\ncloser to the minimum range threshold (τ) and the otherl                ability of cached labels and negatively affecting the adapted\nto the maximum range threshold (τh), achieved by revers-                predictions. Hence, the performance decline with larger\ning the second condition: H(ftestWT) < H(˜entWcT )c           q         caches is mainly due to noise accumulation rather than class\nor H(ftestWT) > H(˜entWc c    q         T). By collecting the low-      imbalance.\nest entropy features within the range [0.2, 0.5], the high-\nest result is attained 60.83%. Opting values below 0.2 in-              9. Broader Impact\ndicates the selection of confident samples for the Negative\nCache, resulting in a reduction in the confidence of CLIP’s             The broader impact of test-time adaptation of vision-\nprediction. Furthermore, a shift by 0.1 from [0.2, 0.5] to              language models lies in its potential to enhance real-world\n[0.3, 0.6] in the thresholds leads to an inclusion of more              applicability, improve accessibility and inclusivity, address\nnoisy samples during the early collection phase of the neg-             bias and fairness concerns, and advance research and de-\native cache, resulting in a 0.48% decrease in performance.              velopment. By allowing models to adapt to new, unseen\nSelecting maximum entropy features with the same thresh-                data during inference, these models can be more versatile\nold range displays a slight decline in performance compared             and adaptable, benefiting various domains such as health-\nto the minimum entropy feature selection within the speci-              care and assistive technologies. Test-time adaptation also\nfied range. Hence, the most valuable uncertain samples fall             offers opportunities to mitigate biases, personalize user ex-\nwithin the [0.2, 0.5] range, whose entropy is closer to 0.2.            periences, and push the boundaries of what vision-language\nThe reported results exclusively utilize the Negative Cache             models can achieve. However, ethical considerations must\nto generate an adapted prediction.                                      be taken into account to ensure responsible development\n                                                                        and deployment, ensuring transparency, fairness, and ac-\n                                                                        countability in the adaptation process.\nThe residual and sharpness ratios.                  The experiments in\nTable 6 show that the optimal residual ratio is 2.0 for TDA\n(instead of 1.0 in Tip-Adapter), indicating a higher signifi-\ncance of adapted features compared with CLIP features in\ntest-time adaptation. The optimal sharpness ratio for TDA\nis 5.0, which is close to the 5.5 in Tip-Adapter.",
        "md": "| |Minimum entropy features| | |Maximum entropy features| |Residual Ratio|0.5|1.0|2.0|3.0|4.0|5.0| | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|τl|τh|Accuracy| | | | |61.07|61.20|61.35|61.29|60.90|60.63| | | | | |\n|0.0|1.0|60.69|0.2|0.4|60.51|Sharpness Ratio|0.5|1.0|3.0|5.0|7.0|9.0| | | | | |\n|0.0|0.2|60.67|0.2|0.5|60.53|TDA|60.98|61.20|61.29|61.35|61.20|61.19| | | | | |\n|0.0|0.3|60.69|0.2|0.6|60.51| | | | | | | | | | | | |\n|0.1|0.3|60.76|0.3|0.5|60.30| | | | | | | | | | | | |\n|0.1|0.4|60.77|0.3|0.7|60.30| | | | | | | | | | | | |\n|0.2|0.4|60.81|0.4|0.6|60.16| | | | | | | | | | | | |\n|0.2|0.5|60.83|0.4|0.7|60.16| | | | | | | | | | | | |\n|0.2|0.6|60.81|0.5|0.7|60.34| | | | | | | | | | | | |\n| | | | | |0.3|0.5|60.34|0.5|0.8|60.34| | | | | | | |\n|0.3|0.6|60.35|0.6|0.8|60.35| | | | | | | | | | | | |\n\nTable 5. Ablation study of the impact of varying Threshold Range [τ, τh] for testing feature selection in the Negative Cache. The study investigates the testing feature selection of the uncertain samples in two ways: choosing the minimum and maximum entropy features in the given range. The results are reported on ImageNet top-1 accuracy using only the Negative Cache to produce an adapted prediction. The experiments are conducted with CLIP-ResNet50.\n\n# More Experimental Analysis</h8>\nCaches built for inference. The caches are built on the fly during inference, starting empty and progressively accumulating samples. At the start of the testing phase on ImageNet, where only 1% of the data was used, we observed a slight accuracy drop of 0.06%. We also noted that bypassing cache usage at the early testing phase leads to a marginal accuracy improvement of 0.1%. We didn’t adopt this approach as it increases complexity by introducing an extra hyperparameter for determining when to use caches.\n\nClass imbalance under high shot capacity. Our analysis with a 6-shot positive cache reveals minimal class imbalance (only 4 out of 1000 ImageNet classes have less than 6 samples) but identifies a significant cache accuracy drop from 90.3% to 86.6% when shot capacity increases from 3 to 6. Such accuracy drop happens because larger cache capacities tend to accumulate noise, thereby reducing the reliability of cached labels and negatively affecting the adapted predictions. Hence, the performance decline with larger caches is mainly due to noise accumulation rather than class imbalance.\n\n# Broader Impact</h9>\nThe broader impact of test-time adaptation of vision-language models lies in its potential to enhance real-world applicability, improve accessibility and inclusivity, address bias and fairness concerns, and advance research and development. By allowing models to adapt to new, unseen data during inference, these models can be more versatile and adaptable, benefiting various domains such as healthcare and assistive technologies. Test-time adaptation also offers opportunities to mitigate biases, personalize user experiences, and push the boundaries of what vision-language models can achieve. However, ethical considerations must be taken into account to ensure responsible development and deployment, ensuring transparency, fairness, and accountability in the adaptation process.\n\nThe residual and sharpness ratios. The experiments in Table 6 show that the optimal residual ratio is 2.0 for TDA (instead of 1.0 in Tip-Adapter), indicating a higher significance of adapted features compared with CLIP features in test-time adaptation. The optimal sharpness ratio for TDA is 5.0, which is close to the 5.5 in Tip-Adapter.",
        "images": [],
        "items": [
          {
            "type": "table",
            "rows": [
              [
                "",
                "Minimum entropy features",
                "",
                "",
                "Maximum entropy features",
                "",
                "Residual Ratio",
                "0.5",
                "1.0",
                "2.0",
                "3.0",
                "4.0",
                "5.0",
                "",
                "",
                "",
                "",
                ""
              ],
              [
                "τl",
                "τh",
                "Accuracy",
                "",
                "",
                "",
                "",
                "61.07",
                "61.20",
                "61.35",
                "61.29",
                "60.90",
                "60.63",
                "",
                "",
                "",
                "",
                ""
              ],
              [
                "0.0",
                "1.0",
                "60.69",
                "0.2",
                "0.4",
                "60.51",
                "Sharpness Ratio",
                "0.5",
                "1.0",
                "3.0",
                "5.0",
                "7.0",
                "9.0",
                "",
                "",
                "",
                "",
                ""
              ],
              [
                "0.0",
                "0.2",
                "60.67",
                "0.2",
                "0.5",
                "60.53",
                "TDA",
                "60.98",
                "61.20",
                "61.29",
                "61.35",
                "61.20",
                "61.19",
                "",
                "",
                "",
                "",
                ""
              ],
              [
                "0.0",
                "0.3",
                "60.69",
                "0.2",
                "0.6",
                "60.51",
                "",
                "",
                "",
                "",
                "",
                "",
                "",
                "",
                "",
                "",
                "",
                ""
              ],
              [
                "0.1",
                "0.3",
                "60.76",
                "0.3",
                "0.5",
                "60.30",
                "",
                "",
                "",
                "",
                "",
                "",
                "",
                "",
                "",
                "",
                "",
                ""
              ],
              [
                "0.1",
                "0.4",
                "60.77",
                "0.3",
                "0.7",
                "60.30",
                "",
                "",
                "",
                "",
                "",
                "",
                "",
                "",
                "",
                "",
                "",
                ""
              ],
              [
                "0.2",
                "0.4",
                "60.81",
                "0.4",
                "0.6",
                "60.16",
                "",
                "",
                "",
                "",
                "",
                "",
                "",
                "",
                "",
                "",
                "",
                ""
              ],
              [
                "0.2",
                "0.5",
                "60.83",
                "0.4",
                "0.7",
                "60.16",
                "",
                "",
                "",
                "",
                "",
                "",
                "",
                "",
                "",
                "",
                "",
                ""
              ],
              [
                "0.2",
                "0.6",
                "60.81",
                "0.5",
                "0.7",
                "60.34",
                "",
                "",
                "",
                "",
                "",
                "",
                "",
                "",
                "",
                "",
                "",
                ""
              ],
              [
                "",
                "",
                "",
                "",
                "",
                "0.3",
                "0.5",
                "60.34",
                "0.5",
                "0.8",
                "60.34",
                "",
                "",
                "",
                "",
                "",
                "",
                ""
              ],
              [
                "0.3",
                "0.6",
                "60.35",
                "0.6",
                "0.8",
                "60.35",
                "",
                "",
                "",
                "",
                "",
                "",
                "",
                "",
                "",
                "",
                "",
                ""
              ]
            ],
            "md": "| |Minimum entropy features| | |Maximum entropy features| |Residual Ratio|0.5|1.0|2.0|3.0|4.0|5.0| | | | | |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|τl|τh|Accuracy| | | | |61.07|61.20|61.35|61.29|60.90|60.63| | | | | |\n|0.0|1.0|60.69|0.2|0.4|60.51|Sharpness Ratio|0.5|1.0|3.0|5.0|7.0|9.0| | | | | |\n|0.0|0.2|60.67|0.2|0.5|60.53|TDA|60.98|61.20|61.29|61.35|61.20|61.19| | | | | |\n|0.0|0.3|60.69|0.2|0.6|60.51| | | | | | | | | | | | |\n|0.1|0.3|60.76|0.3|0.5|60.30| | | | | | | | | | | | |\n|0.1|0.4|60.77|0.3|0.7|60.30| | | | | | | | | | | | |\n|0.2|0.4|60.81|0.4|0.6|60.16| | | | | | | | | | | | |\n|0.2|0.5|60.83|0.4|0.7|60.16| | | | | | | | | | | | |\n|0.2|0.6|60.81|0.5|0.7|60.34| | | | | | | | | | | | |\n| | | | | |0.3|0.5|60.34|0.5|0.8|60.34| | | | | | | |\n|0.3|0.6|60.35|0.6|0.8|60.35| | | | | | | | | | | | |",
            "isPerfectTable": true,
            "csv": "\"\",\"Minimum entropy features\",\"\",\"\",\"Maximum entropy features\",\"\",\"Residual Ratio\",\"0.5\",\"1.0\",\"2.0\",\"3.0\",\"4.0\",\"5.0\",\"\",\"\",\"\",\"\",\"\"\n\"τl\",\"τh\",\"Accuracy\",\"\",\"\",\"\",\"\",\"61.07\",\"61.20\",\"61.35\",\"61.29\",\"60.90\",\"60.63\",\"\",\"\",\"\",\"\",\"\"\n\"0.0\",\"1.0\",\"60.69\",\"0.2\",\"0.4\",\"60.51\",\"Sharpness Ratio\",\"0.5\",\"1.0\",\"3.0\",\"5.0\",\"7.0\",\"9.0\",\"\",\"\",\"\",\"\",\"\"\n\"0.0\",\"0.2\",\"60.67\",\"0.2\",\"0.5\",\"60.53\",\"TDA\",\"60.98\",\"61.20\",\"61.29\",\"61.35\",\"61.20\",\"61.19\",\"\",\"\",\"\",\"\",\"\"\n\"0.0\",\"0.3\",\"60.69\",\"0.2\",\"0.6\",\"60.51\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\n\"0.1\",\"0.3\",\"60.76\",\"0.3\",\"0.5\",\"60.30\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\n\"0.1\",\"0.4\",\"60.77\",\"0.3\",\"0.7\",\"60.30\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\n\"0.2\",\"0.4\",\"60.81\",\"0.4\",\"0.6\",\"60.16\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\n\"0.2\",\"0.5\",\"60.83\",\"0.4\",\"0.7\",\"60.16\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\n\"0.2\",\"0.6\",\"60.81\",\"0.5\",\"0.7\",\"60.34\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\n\"\",\"\",\"\",\"\",\"\",\"0.3\",\"0.5\",\"60.34\",\"0.5\",\"0.8\",\"60.34\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\n\"0.3\",\"0.6\",\"60.35\",\"0.6\",\"0.8\",\"60.35\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"",
            "bBox": {
              "x": 56,
              "y": 82,
              "w": 480.96,
              "h": 323.97
            }
          },
          {
            "type": "text",
            "value": "Table 5. Ablation study of the impact of varying Threshold Range [τ, τh] for testing feature selection in the Negative Cache. The study investigates the testing feature selection of the uncertain samples in two ways: choosing the minimum and maximum entropy features in the given range. The results are reported on ImageNet top-1 accuracy using only the Negative Cache to produce an adapted prediction. The experiments are conducted with CLIP-ResNet50.",
            "md": "Table 5. Ablation study of the impact of varying Threshold Range [τ, τh] for testing feature selection in the Negative Cache. The study investigates the testing feature selection of the uncertain samples in two ways: choosing the minimum and maximum entropy features in the given range. The results are reported on ImageNet top-1 accuracy using only the Negative Cache to produce an adapted prediction. The experiments are conducted with CLIP-ResNet50.",
            "bBox": {
              "x": 50,
              "y": 83,
              "w": 236.34,
              "h": 322.97
            }
          },
          {
            "type": "heading",
            "lvl": 1,
            "value": "More Experimental Analysis</h8>",
            "md": "# More Experimental Analysis</h8>",
            "bBox": {
              "x": 57,
              "y": 283,
              "w": 166.52,
              "h": 122.97
            }
          },
          {
            "type": "text",
            "value": "Caches built for inference. The caches are built on the fly during inference, starting empty and progressively accumulating samples. At the start of the testing phase on ImageNet, where only 1% of the data was used, we observed a slight accuracy drop of 0.06%. We also noted that bypassing cache usage at the early testing phase leads to a marginal accuracy improvement of 0.1%. We didn’t adopt this approach as it increases complexity by introducing an extra hyperparameter for determining when to use caches.\n\nClass imbalance under high shot capacity. Our analysis with a 6-shot positive cache reveals minimal class imbalance (only 4 out of 1000 ImageNet classes have less than 6 samples) but identifies a significant cache accuracy drop from 90.3% to 86.6% when shot capacity increases from 3 to 6. Such accuracy drop happens because larger cache capacities tend to accumulate noise, thereby reducing the reliability of cached labels and negatively affecting the adapted predictions. Hence, the performance decline with larger caches is mainly due to noise accumulation rather than class imbalance.",
            "md": "Caches built for inference. The caches are built on the fly during inference, starting empty and progressively accumulating samples. At the start of the testing phase on ImageNet, where only 1% of the data was used, we observed a slight accuracy drop of 0.06%. We also noted that bypassing cache usage at the early testing phase leads to a marginal accuracy improvement of 0.1%. We didn’t adopt this approach as it increases complexity by introducing an extra hyperparameter for determining when to use caches.\n\nClass imbalance under high shot capacity. Our analysis with a 6-shot positive cache reveals minimal class imbalance (only 4 out of 1000 ImageNet classes have less than 6 samples) but identifies a significant cache accuracy drop from 90.3% to 86.6% when shot capacity increases from 3 to 6. Such accuracy drop happens because larger cache capacities tend to accumulate noise, thereby reducing the reliability of cached labels and negatively affecting the adapted predictions. Hence, the performance decline with larger caches is mainly due to noise accumulation rather than class imbalance.",
            "bBox": {
              "x": 56,
              "y": 99,
              "w": 488.76,
              "h": 348.96
            }
          },
          {
            "type": "heading",
            "lvl": 1,
            "value": "Broader Impact</h9>",
            "md": "# Broader Impact</h9>",
            "bBox": {
              "x": 0,
              "y": 0,
              "w": 612,
              "h": 792
            }
          },
          {
            "type": "text",
            "value": "The broader impact of test-time adaptation of vision-language models lies in its potential to enhance real-world applicability, improve accessibility and inclusivity, address bias and fairness concerns, and advance research and development. By allowing models to adapt to new, unseen data during inference, these models can be more versatile and adaptable, benefiting various domains such as healthcare and assistive technologies. Test-time adaptation also offers opportunities to mitigate biases, personalize user experiences, and push the boundaries of what vision-language models can achieve. However, ethical considerations must be taken into account to ensure responsible development and deployment, ensuring transparency, fairness, and accountability in the adaptation process.\n\nThe residual and sharpness ratios. The experiments in Table 6 show that the optimal residual ratio is 2.0 for TDA (instead of 1.0 in Tip-Adapter), indicating a higher significance of adapted features compared with CLIP features in test-time adaptation. The optimal sharpness ratio for TDA is 5.0, which is close to the 5.5 in Tip-Adapter.",
            "md": "The broader impact of test-time adaptation of vision-language models lies in its potential to enhance real-world applicability, improve accessibility and inclusivity, address bias and fairness concerns, and advance research and development. By allowing models to adapt to new, unseen data during inference, these models can be more versatile and adaptable, benefiting various domains such as healthcare and assistive technologies. Test-time adaptation also offers opportunities to mitigate biases, personalize user experiences, and push the boundaries of what vision-language models can achieve. However, ethical considerations must be taken into account to ensure responsible development and deployment, ensuring transparency, fairness, and accountability in the adaptation process.\n\nThe residual and sharpness ratios. The experiments in Table 6 show that the optimal residual ratio is 2.0 for TDA (instead of 1.0 in Tip-Adapter), indicating a higher significance of adapted features compared with CLIP features in test-time adaptation. The optimal sharpness ratio for TDA is 5.0, which is close to the 5.5 in Tip-Adapter.",
            "bBox": {
              "x": 50,
              "y": 82,
              "w": 494.86,
              "h": 638.96
            }
          }
        ],
        "status": "OK",
        "links": [
          {
            "text": "(instead of 1.0 in Tip-Adapter), indicating a higher signifi-"
          }
        ],
        "width": 612,
        "height": 792,
        "triggeredAutoMode": false,
        "structuredData": null,
        "noStructuredContent": false,
        "noTextContent": false
      }
    ],
    "job_metadata": {
      "credits_used": 24.0,
      "job_credits_usage": 12,
      "job_pages": 12,
      "job_auto_mode_triggered_pages": 0,
      "job_is_cache_hit": false,
      "credits_max": 1000
    },
    "job_id": "51aeb82f-86b1-47bc-a704-342e794f724b",
    "file_path": "2403.18293v1.pdf"
  }
]